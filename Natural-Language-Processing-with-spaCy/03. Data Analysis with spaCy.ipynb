{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. spaCy pipelines\n",
    "-------------------\n",
    "\n",
    "00:00 - 00:07\n",
    "\n",
    "Welcome! We previously learned about spaCy pipelines, let's explore them further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. spaCy pipelines\n",
    "-------------------\n",
    "\n",
    "00:07 - 00:22\n",
    "\n",
    "Recall that when we call nlp on a text, spaCy first tokenizes the text to produce a Doc container. The Doc object is then processed in several different steps, known as the processing pipeline.\n",
    "\n",
    "• spaCy first tokenizes the text to produce a Doc object\n",
    "\n",
    "• The Doc is processed in several different steps of processing pipeline\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(example_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. spaCy pipelines\n",
    "-------------------\n",
    "\n",
    "00:22 - 01:36\n",
    "\n",
    "To continue our learnings on spaCy pipelines, in this video, we will explore how to create pipeline components and add them to an existing or blank spaCy pipeline. A pipeline is a sequence of pipes (pipeline components), or actors on data, that make alterations to the data or extract information from it. In some cases, later pipes require the output from earlier components, while in other cases, a pipe can exist entirely on its own. As an example, for a named entity recognition pipeline, three pipes can be used: a Tokenizer pipe, which is the first processing step in spaCy pipelines; a rule-based named entity recognizer known as the EntityRuler, which finds entities; and an EntityLinker pipe that identifies the type of each entity. Through this processing pipeline, an input text is converted to a Doc container with its corresponding annotated entities. We can use the doc-dot-ents feature to find the entities in the input text.\n",
    "\n",
    "• A pipeline is a sequence of pipes, or actors on data\n",
    "\n",
    "• A spaCy NER pipeline:\n",
    "  * Tokenization \n",
    "  * Named entity identification\n",
    "  * Named entity classification\n",
    "\n",
    "```\n",
    "[Input text] --> [Tokenizer] --> [EntityRuler] --> [EntityLinker] --> [Doc with annotated entities]\n",
    "```\n",
    "\n",
    "```python\n",
    "print([ent.text for ent in doc.ents])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Adding pipes\n",
    "----------------\n",
    "\n",
    "01:36 - 02:48\n",
    "\n",
    "We often use an existing spaCy model. However, in some cases, an off-the-shelf model will not satisfy our requirements. An example of this is the sentence segmentation for a long document with 10,000 sentences. To recall, sentence segmentation is breaking a text into its given sentences. Sentencizer is the name of the spaCy pipeline component that performs sentence segmentation. Given a document that has 10,000 sentences, even if we use the smallest English model, the most efficient spaCy model, en_core_web_sm, the model can take a long time to process 10,000 sentences and separate them. The reason is that when calling an existing spaCy model on a text, the whole NLP pipeline will be activated and that means that each pipe from named entity recognition to dependency parsing will run on the text. This increases the use of computational time by 100 times.\n",
    "\n",
    "• sentencizer: spaCy pipeline component for sentence segmentation.\n",
    "\n",
    "```python\n",
    "text = \" \".join([\"This is a test sentence.\"]*10000)\n",
    "en_core_sm_nlp = spacy.load(\"en_core_web_sm\")\n",
    "start_time = time.time()\n",
    "doc = en_core_sm_nlp(text)\n",
    "print(f\"Finished processing with en_core_web_sm model in {round((time.time() - start_time)/60.0 , 5)} minutes\")\n",
    "```\n",
    "\n",
    "```\n",
    ">>> Finished processing with en_core_web_sm model in 0.09332 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Adding pipes\n",
    "----------------\n",
    "\n",
    "02:48 - 03:28\n",
    "\n",
    "In this instance, we would want to make a blank spaCy English model by using spacy-dot-blank(\"en\") and add the sentencizer component to the pipeline by using -dot-add_pipe method of the nlp model. By creating a blank model and simply adding a sentencizer pipe, we can considerably reduce computational time. The reason is that for this version of the spaCy model, only intended pipeline component (sentence segmentation) will run on the given documents.\n",
    "\n",
    "• Create a blank model and add a sentencizer pipe:\n",
    "\n",
    "```python\n",
    "blank_nlp = spacy.blank(\"en\")\n",
    "blank_nlp.add_pipe(\"sentencizer\")\n",
    "start_time = time.time()\n",
    "doc = blank_nlp(text)\n",
    "print(f\"Finished processing with blank model in {round((time.time() - start_time)/60.0 , 5)} minutes\")\n",
    "```\n",
    "\n",
    "```\n",
    ">>> Finished processing with blank model in 0.00091 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Analyzing pipeline components\n",
    "---------------------------------\n",
    "\n",
    "03:28 - 04:25\n",
    "\n",
    "spaCy allows us to analyze a spaCy pipeline to check whether any required attributes are not set. The nlp-dot-analyze_pipes method analyzes the components in a pipeline and outputs structured information about them, like the attributes they set on the Doc and Token, whether they retokenize the Doc and which scores they produce during training. It also shows warnings if components require values that are not set by the previous components. For example, when the entity linker is used but no component before EntityLinker sets named entities. While calling nlp-dot-analyze_pipes() method we can also set the pretty argument to True, which will print a nicely organized table as the result of analyzing the pipeline components.\n",
    "\n",
    "• nlp.analyze_pipes() analyzes a spaCy pipeline to determine:\n",
    "  * Attributes that pipeline components set\n",
    "  * Scores a component produces during training\n",
    "  * Presence of all required attributes\n",
    "\n",
    "• Setting pretty to True will print a table instead of only returning the structured data.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "analysis = nlp.analyze_pipes(pretty=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Analyzing pipeline components\n",
    "---------------------------------\n",
    "\n",
    "04:25 - 04:47\n",
    "\n",
    "The snapshot shows the results of the analyze_pipes method. While we don't go into technical details of all the fields, we are familiar with some of the components and attributes provided in this snapshot. In this case, the result of analysis is \"No problems found\".\n",
    "\n",
    "```\n",
    "=============================== Pipeline Overview ===============================\n",
    "\n",
    "#   Component         Assigns              Requires        Scores             Retokenizes\n",
    "-   ---------------   -------------------  -------------   ---------------    -----------\n",
    "0   tok2vec          doc.tensor                                             False\n",
    "\n",
    "1   tagger           token.tag                            tag_acc           False\n",
    "\n",
    "2   parser           token.dep                            dep_uas           False\n",
    "                     token.head                           dep_las\n",
    "                     token.is_sent_start                  dep_las_per_type\n",
    "                     doc.sents                            sents_p\n",
    "                                                         sents_r\n",
    "                                                         sents_f\n",
    "\n",
    "3   attribute_ruler                                                         False\n",
    "\n",
    "4   lemmatizer       token.lemma                         lemma_acc          False\n",
    "\n",
    "5   ner              doc.ents                            ents_f             False\n",
    "                     token.ent_iob                       ents_p\n",
    "                     token.ent_type                      ents_r\n",
    "                                                        ents_per_type\n",
    "\n",
    "6   entity_linker    token.ent_kb_id      doc.ents       nel_micro_f        False\n",
    "                                         doc.sents       nel_micro_r\n",
    "                                         token.ent_iob   nel_micro_p\n",
    "                                         token.ent_type\n",
    "\n",
    "✓ No problems found.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:47 - 04:50\n",
    "\n",
    "Let's practice our learnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding pipes in spaCy\n",
    "=====================\n",
    "\n",
    "You often use an existing spaCy model for different NLP tasks. However, in some cases, an off-the-shelf pipeline component such as sentence segmentation will take long times to produce expected results. In this exercise, you'll practice adding a pipeline component to a spaCy model (text processing pipeline). \n",
    "\n",
    "You will use the first five reviews from the Amazon Fine Food Reviews dataset for this exercise. You can access these reviews by using the `texts` string. \n",
    "\n",
    "The `spaCy` package is already imported for you to use.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Load a blank `spaCy` English model and add a `sentencizer` component to the model.\n",
    "-   Create a `Doc` container for the `texts`, create a list to store `sentences` of the given document and print its number of sentences.\n",
    "-   Print the list of tokens in the second sentence from the `sentences` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a blank spaCy English model and add a sentencizer component\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create Doc containers, store sentences and print its number of sentences\n",
    "doc = nlp(texts)\n",
    "sentences = [sent for sent in doc.sents]\n",
    "print(\"Number of sentences: \", len(sentences), \"\\n\")\n",
    "\n",
    "# Print the list of tokens in the second sentence\n",
    "print(\"Second sentence tokens: \", [token for token in sentences[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing pipelines in spaCy\n",
    "============================\n",
    "\n",
    "`spaCy` allows you to analyze a spaCy pipeline to check whether any required attributes are not set. In this exercise, you'll practice analyzing a `spaCy` pipeline. Earlier in the video, an existing `en_core_web_sm` pipeline was analyzed and the result was `No problems found.`, in this instance, you will analyze a blank `spaCy` English model with few added components and observe results of the analysis.\n",
    "\n",
    "The `spaCy` package is already imported for you to use.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Load a blank `spaCy` English model as `nlp`.\n",
    "-   Add `tagger` and `entity_linker` pipeline components to the blank model.\n",
    "-   Analyze the `nlp` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a blank spaCy English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add tagger and entity_linker pipeline components\n",
    "nlp.add_pipe(\"tagger\")\n",
    "nlp.add_pipe(\"entity_linker\")\n",
    "\n",
    "# Analyze the pipeline\n",
    "analysis = nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "Question\n",
    "--------\n",
    "\n",
    "The output of `analyze_pipes()` method showed that `entity_linker requirements not met: doc.ents, doc.sents, token.ent_iob, token.ent_type`.\n",
    "\n",
    "Which NLP components should be added before adding `entity_linker` component to ensure the created `spaCy` pipeline have all the required attributes for entity linking?\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "[/] ner, sentencizer\n",
    "\n",
    "[] ner, lemmatizer\n",
    "\n",
    "[] lemmatizer, sentencizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
