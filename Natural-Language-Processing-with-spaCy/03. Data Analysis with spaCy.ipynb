{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. spaCy pipelines\n",
    "-------------------\n",
    "\n",
    "00:00 - 00:07\n",
    "\n",
    "Welcome! We previously learned about spaCy pipelines, let's explore them further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. spaCy pipelines\n",
    "-------------------\n",
    "\n",
    "00:07 - 00:22\n",
    "\n",
    "Recall that when we call nlp on a text, spaCy first tokenizes the text to produce a Doc container. The Doc object is then processed in several different steps, known as the processing pipeline.\n",
    "\n",
    "• spaCy first tokenizes the text to produce a Doc object\n",
    "\n",
    "• The Doc is processed in several different steps of processing pipeline\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(example_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. spaCy pipelines\n",
    "-------------------\n",
    "\n",
    "00:22 - 01:36\n",
    "\n",
    "To continue our learnings on spaCy pipelines, in this video, we will explore how to create pipeline components and add them to an existing or blank spaCy pipeline. A pipeline is a sequence of pipes (pipeline components), or actors on data, that make alterations to the data or extract information from it. In some cases, later pipes require the output from earlier components, while in other cases, a pipe can exist entirely on its own. As an example, for a named entity recognition pipeline, three pipes can be used: a Tokenizer pipe, which is the first processing step in spaCy pipelines; a rule-based named entity recognizer known as the EntityRuler, which finds entities; and an EntityLinker pipe that identifies the type of each entity. Through this processing pipeline, an input text is converted to a Doc container with its corresponding annotated entities. We can use the doc-dot-ents feature to find the entities in the input text.\n",
    "\n",
    "• A pipeline is a sequence of pipes, or actors on data\n",
    "\n",
    "• A spaCy NER pipeline:\n",
    "  * Tokenization \n",
    "  * Named entity identification\n",
    "  * Named entity classification\n",
    "\n",
    "```\n",
    "[Input text] --> [Tokenizer] --> [EntityRuler] --> [EntityLinker] --> [Doc with annotated entities]\n",
    "```\n",
    "\n",
    "```python\n",
    "print([ent.text for ent in doc.ents])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Adding pipes\n",
    "----------------\n",
    "\n",
    "01:36 - 02:48\n",
    "\n",
    "We often use an existing spaCy model. However, in some cases, an off-the-shelf model will not satisfy our requirements. An example of this is the sentence segmentation for a long document with 10,000 sentences. To recall, sentence segmentation is breaking a text into its given sentences. Sentencizer is the name of the spaCy pipeline component that performs sentence segmentation. Given a document that has 10,000 sentences, even if we use the smallest English model, the most efficient spaCy model, en_core_web_sm, the model can take a long time to process 10,000 sentences and separate them. The reason is that when calling an existing spaCy model on a text, the whole NLP pipeline will be activated and that means that each pipe from named entity recognition to dependency parsing will run on the text. This increases the use of computational time by 100 times.\n",
    "\n",
    "• sentencizer: spaCy pipeline component for sentence segmentation.\n",
    "\n",
    "```python\n",
    "text = \" \".join([\"This is a test sentence.\"]*10000)\n",
    "en_core_sm_nlp = spacy.load(\"en_core_web_sm\")\n",
    "start_time = time.time()\n",
    "doc = en_core_sm_nlp(text)\n",
    "print(f\"Finished processing with en_core_web_sm model in {round((time.time() - start_time)/60.0 , 5)} minutes\")\n",
    "```\n",
    "\n",
    "```\n",
    ">>> Finished processing with en_core_web_sm model in 0.09332 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Adding pipes\n",
    "----------------\n",
    "\n",
    "02:48 - 03:28\n",
    "\n",
    "In this instance, we would want to make a blank spaCy English model by using spacy-dot-blank(\"en\") and add the sentencizer component to the pipeline by using -dot-add_pipe method of the nlp model. By creating a blank model and simply adding a sentencizer pipe, we can considerably reduce computational time. The reason is that for this version of the spaCy model, only intended pipeline component (sentence segmentation) will run on the given documents.\n",
    "\n",
    "• Create a blank model and add a sentencizer pipe:\n",
    "\n",
    "```python\n",
    "blank_nlp = spacy.blank(\"en\")\n",
    "blank_nlp.add_pipe(\"sentencizer\")\n",
    "start_time = time.time()\n",
    "doc = blank_nlp(text)\n",
    "print(f\"Finished processing with blank model in {round((time.time() - start_time)/60.0 , 5)} minutes\")\n",
    "```\n",
    "\n",
    "```\n",
    ">>> Finished processing with blank model in 0.00091 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Analyzing pipeline components\n",
    "---------------------------------\n",
    "\n",
    "03:28 - 04:25\n",
    "\n",
    "spaCy allows us to analyze a spaCy pipeline to check whether any required attributes are not set. The nlp-dot-analyze_pipes method analyzes the components in a pipeline and outputs structured information about them, like the attributes they set on the Doc and Token, whether they retokenize the Doc and which scores they produce during training. It also shows warnings if components require values that are not set by the previous components. For example, when the entity linker is used but no component before EntityLinker sets named entities. While calling nlp-dot-analyze_pipes() method we can also set the pretty argument to True, which will print a nicely organized table as the result of analyzing the pipeline components.\n",
    "\n",
    "• nlp.analyze_pipes() analyzes a spaCy pipeline to determine:\n",
    "  * Attributes that pipeline components set\n",
    "  * Scores a component produces during training\n",
    "  * Presence of all required attributes\n",
    "\n",
    "• Setting pretty to True will print a table instead of only returning the structured data.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "analysis = nlp.analyze_pipes(pretty=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Analyzing pipeline components\n",
    "---------------------------------\n",
    "\n",
    "04:25 - 04:47\n",
    "\n",
    "The snapshot shows the results of the analyze_pipes method. While we don't go into technical details of all the fields, we are familiar with some of the components and attributes provided in this snapshot. In this case, the result of analysis is \"No problems found\".\n",
    "\n",
    "```\n",
    "=============================== Pipeline Overview ===============================\n",
    "\n",
    "#   Component         Assigns              Requires        Scores             Retokenizes\n",
    "-   ---------------   -------------------  -------------   ---------------    -----------\n",
    "0   tok2vec          doc.tensor                                             False\n",
    "\n",
    "1   tagger           token.tag                            tag_acc           False\n",
    "\n",
    "2   parser           token.dep                            dep_uas           False\n",
    "                     token.head                           dep_las\n",
    "                     token.is_sent_start                  dep_las_per_type\n",
    "                     doc.sents                            sents_p\n",
    "                                                         sents_r\n",
    "                                                         sents_f\n",
    "\n",
    "3   attribute_ruler                                                         False\n",
    "\n",
    "4   lemmatizer       token.lemma                         lemma_acc          False\n",
    "\n",
    "5   ner              doc.ents                            ents_f             False\n",
    "                     token.ent_iob                       ents_p\n",
    "                     token.ent_type                      ents_r\n",
    "                                                        ents_per_type\n",
    "\n",
    "6   entity_linker    token.ent_kb_id      doc.ents       nel_micro_f        False\n",
    "                                         doc.sents       nel_micro_r\n",
    "                                         token.ent_iob   nel_micro_p\n",
    "                                         token.ent_type\n",
    "\n",
    "✓ No problems found.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:47 - 04:50\n",
    "\n",
    "Let's practice our learnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding pipes in spaCy\n",
    "=====================\n",
    "\n",
    "You often use an existing spaCy model for different NLP tasks. However, in some cases, an off-the-shelf pipeline component such as sentence segmentation will take long times to produce expected results. In this exercise, you'll practice adding a pipeline component to a spaCy model (text processing pipeline). \n",
    "\n",
    "You will use the first five reviews from the Amazon Fine Food Reviews dataset for this exercise. You can access these reviews by using the `texts` string. \n",
    "\n",
    "The `spaCy` package is already imported for you to use.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Load a blank `spaCy` English model and add a `sentencizer` component to the model.\n",
    "-   Create a `Doc` container for the `texts`, create a list to store `sentences` of the given document and print its number of sentences.\n",
    "-   Print the list of tokens in the second sentence from the `sentences` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a blank spaCy English model and add a sentencizer component\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create Doc containers, store sentences and print its number of sentences\n",
    "doc = nlp(texts)\n",
    "sentences = [sent for sent in doc.sents]\n",
    "print(\"Number of sentences: \", len(sentences), \"\\n\")\n",
    "\n",
    "# Print the list of tokens in the second sentence\n",
    "print(\"Second sentence tokens: \", [token for token in sentences[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing pipelines in spaCy\n",
    "============================\n",
    "\n",
    "`spaCy` allows you to analyze a spaCy pipeline to check whether any required attributes are not set. In this exercise, you'll practice analyzing a `spaCy` pipeline. Earlier in the video, an existing `en_core_web_sm` pipeline was analyzed and the result was `No problems found.`, in this instance, you will analyze a blank `spaCy` English model with few added components and observe results of the analysis.\n",
    "\n",
    "The `spaCy` package is already imported for you to use.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Load a blank `spaCy` English model as `nlp`.\n",
    "-   Add `tagger` and `entity_linker` pipeline components to the blank model.\n",
    "-   Analyze the `nlp` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a blank spaCy English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add tagger and entity_linker pipeline components\n",
    "nlp.add_pipe(\"tagger\")\n",
    "nlp.add_pipe(\"entity_linker\")\n",
    "\n",
    "# Analyze the pipeline\n",
    "analysis = nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "Question\n",
    "--------\n",
    "\n",
    "The output of `analyze_pipes()` method showed that `entity_linker requirements not met: doc.ents, doc.sents, token.ent_iob, token.ent_type`.\n",
    "\n",
    "Which NLP components should be added before adding `entity_linker` component to ensure the created `spaCy` pipeline have all the required attributes for entity linking?\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "[/] ner, sentencizer\n",
    "\n",
    "[] ner, lemmatizer\n",
    "\n",
    "[] lemmatizer, sentencizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. spaCy EntityRuler\n",
    "---------------------\n",
    "\n",
    "00:00 - 00:11\n",
    "\n",
    "Welcome! Let's learn about EntityRuler, a component in spaCy that allows us to include or modify named entities using pattern matching rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. spaCy EntityRuler\n",
    "---------------------\n",
    "\n",
    "00:11 - 01:52\n",
    "\n",
    "EntityRuler lets us add entities to Doc-dot-ents. It can be combined with EntityRecognizer, a spaCy pipeline component for named-entity recognition, to boost accuracy, or used on its own to implement a purely rule-based entity recognition system. We can add named-entities to a Doc container using an entity pattern. Entity patterns are dictionaries with two keys. One key is the \"label\" which is specifying the label to assign to the entity if the pattern is matched, and the second key is the \"pattern\", which is the matched string. The entity ruler accepts two types of patterns: phrase entity and token entity patterns. A phrase entity pattern is used for exact string matches, for example to exactly match Microsoft as a named entity with a label of ORG, we can use an entity pattern dictionary with a \"label\" equal to ORG and the \"pattern\" to be set as \"Microsoft\". A token entity pattern uses one dictionary to describe one token. For example, to match lower cases san francisco to an entity type of GPE (a location type), we can use an entity pattern dictionary with a \"label\" equal to GPE and the \"pattern\" to be set to a list of two key value pairs where the key is set to \"LOWER\" and the value is set to \"san\" for one and \"francisco\" for the other pair.\n",
    "\n",
    "• EntityRuler adds named-entities to a Doc container\n",
    "\n",
    "• It can be used on its own or combined with EntityRecognizer\n",
    "\n",
    "• Phrase entity patterns for exact string matches (string):\n",
    "```python\n",
    "{\"label\": \"ORG\", \"pattern\": \"Microsoft\"}\n",
    "```\n",
    "\n",
    "• Token entity patterns with one dictionary describing one token (list):\n",
    "```python\n",
    "{\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Adding EntityRuler to spaCy pipeline\n",
    "----------------------------------------\n",
    "\n",
    "01:52 - 02:49\n",
    "\n",
    "The EntityRuler can be added to a spaCy model using dot-add_pipe() method by passing \"entity_ruler\" name. When the nlp model is called on a text, it will find matches in the doc container and add them as entities in the doc-dot-ents, using the specified pattern label as the entity label. As an example, we load a blank spaCy model and use -dot-add_pipe(\"entity_ruler\") method to add EntityRuler component. Next, we define a list of patterns. Patterns can be a combination of phrase entity and token entity patterns. These patterns can be added to the EntityRuler component using -dot-add_patterns() method.\n",
    "\n",
    "• Using .add_pipe() method \n",
    "\n",
    "• List of patterns can be added using .add_patterns() method\n",
    "\n",
    "```python\n",
    "nlp = spacy.blank(\"en\")\n",
    "entity_ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Microsoft\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
    "entity_ruler.add_patterns(patterns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Adding EntityRuler to spaCy pipeline\n",
    "----------------------------------------\n",
    "\n",
    "02:49 - 03:15\n",
    "\n",
    "Next, we run the model on a given text to generate a Doc container. The nlp model uses the EntityRuler component to populate the dot-ents attribute of the Doc container. In this instance, Microsoft and San Francisco are extracted as entities with ORG and GPE entity labels respectively.\n",
    "\n",
    "• .ents store the results of an EntityLinker component\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Microsoft is hiring software developer in San Francisco.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "```\n",
    "\n",
    "```\n",
    "[('Microsoft', 'ORG'), ('San Francisco', 'GPE')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. EntityRuler in action\n",
    "-------------------------\n",
    "\n",
    "03:15 - 03:38\n",
    "\n",
    "The entity ruler is designed to integrate with spaCy's existing components and enhance the named entity recognizer performance. Let us look at an example of \"Manhattan associates is a company in the US\". In this case, the model is unable to accurately classify Manhattan associates as an ORG.\n",
    "\n",
    "• Integrates with spaCy pipeline components\n",
    "\n",
    "• Enhances the named-entity recognizer\n",
    "\n",
    "• spaCy model without EntityRuler:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Manhattan associates is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "```\n",
    "\n",
    "```\n",
    "[('Manhattan', 'GPE'), ('U.S.', 'GPE')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. EntityRuler in action\n",
    "-------------------------\n",
    "\n",
    "03:38 - 04:21\n",
    "\n",
    "We can add an EntityRuler component to the current nlp pipeline. If we add the ruler after an existing ner component by setting the \"after\" argument of the -dot-add_pipe() method to \"ner\", the entity ruler will only add entities to the doc-dot-ents if they don't overlap with existing entities predicted by the model. In this case, the model tags Manhattan with an incorrect GPE type, because the ruler component is called after existing ner (EntityRecognizer) component of the model.\n",
    "\n",
    "• EntityRuler added after existing ner component:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", after='ner')\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": \"manhattan\"}, {\"lower\": \"associates\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Manhattan associates is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "```\n",
    "\n",
    "```\n",
    "[('Manhattan', 'GPE'), ('U.S.', 'GPE')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. EntityRuler in action\n",
    "-------------------------\n",
    "\n",
    "04:21 - 04:50\n",
    "\n",
    "However, if we add an EntityRuler before the ner component by setting the \"before\" argument of -dot-add_pipe() method to \"ner\", to recognize Manhattan associate as an ORG, the entity recognizer will respect the existing entity spans and adjust its predictions based on patterns added to the EntityRuler. This can improve model accuracy in our case.\n",
    "\n",
    "• EntityRuler added before existing ner component:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before='ner')\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": \"manhattan\"}, {\"lower\": \"associates\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Manhattan associates is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "```\n",
    "\n",
    "```\n",
    "[('Manhattan associates', 'ORG'), ('U.S.', 'GPE')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:50 - 04:54\n",
    "\n",
    "Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EntityRuler with blank spaCy model\n",
    "==================================\n",
    "\n",
    "`EntityRuler` lets you to add entities to `doc.ents`. It can be combined with `EntityRecognizer`, a spaCy pipeline component for named-entity recognition, to boost accuracy, or used on its own to implement a purely rule-based entity recognition system. In this exercise, you will practice adding an `EntityRuler` component to a blank `spaCy` English model and classify named entities of the given `text` using purely rule-based named-entity recognition.\n",
    "\n",
    "The `spaCy` package is already imported and a blank `spaCy` English model is ready for your use as `nlp`. A list of `patterns` to classify lower cased `OpenAI` and `Microsoft` as `ORG`is already created for your use.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create and add an `EntityRuler`component to the pipeline.\n",
    "-   Add given patterns to the `EntityRuler`component. \n",
    "-   Run the model on the given `text` and create its corresponding `Doc` container. \n",
    "-   Print a tuple of (entities text and types) for all entities in the `Doc` container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"openai\"}]},\n",
    "            {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"microsoft\"}]}]\n",
    "text = \"OpenAI has joined forces with Microsoft.\"\n",
    "\n",
    "# Add EntityRuler component to the model\n",
    "entity_ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add given patterns to the EntityRuler component\n",
    "entity_ruler.add_patterns(patterns)\n",
    "\n",
    "# Run the model on a given text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print entities text and type for all entities in the Doc container\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EntityRuler for NER\n",
    "===================\n",
    "\n",
    "`EntityRuler` can be combined with `EntityRecognizer` of an existing model to boost its accuracy. In this exercise, you will practice combining an `EntityRuler`component and an existing `NER` component of the `en_core_web_sm` model. The model is already loaded as `nlp`. \n",
    "\n",
    "When `EntityRuler` is added before `NER`component, the entity recognizer will respect the existing entity spans and adjust its predictions based on patterns added to the `EntityRuler` to improve accuracy of named entity recognition task.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Add an `EntityRuler` to the `nlp` before `ner` component.\n",
    "-   Define a token entity pattern to classify lower cased `new york group` as `ORG`.\n",
    "-   Add the `patterns` to the `EntityRuler`component.\n",
    "-   Run the model and print the tuple of entities text and type for the `Doc` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"New York Group was built in 1987.\"\n",
    "\n",
    "# Add an EntityRuler to the nlp before NER component\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# Define a pattern to classify lower cased new york group as ORG\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": \"new york group\"}]}]\n",
    "\n",
    "# Add the patterns to the EntityRuler component\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Run the model and print entities text and type for all the entities\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EntityRuler with multi-patterns in spaCy\n",
    "========================================\n",
    "\n",
    "`EntityRuler` lets you to add entities to `doc.ents` and boost its named entity recognition performance. In this exercise, you will practice adding an `EntityRuler`component to an existing `nlp` pipeline to ensure multiple entities are correctly being classified.\n",
    "\n",
    "The `en_core_web_sm` model is already loaded and is available for your use as `nlp`. You can access an example text in `example_text` and use `nlp` and `doc` to access an `spaCy` model and `Doc` container of `example_text`respectively.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Print a list of tuples of entities text and types in the `example_text` with the `nlp` model.\n",
    "-   Define multiple patterns to match lower cased `brother` and `sisters` to `PERSON`label.\n",
    "-   Add an `EntityRuler` component to the `nlp` pipeline and add the `patterns` to the `EntityRuler`.\n",
    "-   Print a tuple of text and type of entities for the `example_text` with the `nlp` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Print a list of tuples of entities text and types in the example_text\n",
    "print(\"Before EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents], \"\\n\")\n",
    "\n",
    "# Define pattern to add a label PERSON for lower cased sisters and brother entities\n",
    "patterns = [{\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"brother\"}]},\n",
    "            {\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"sisters\"}]}]\n",
    "\n",
    "# Add an EntityRuler component and add the patterns to the ruler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Print a list of tuples of entities text and types\n",
    "print(\"After EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. RegEx with spaCy\n",
    "--------------------\n",
    "\n",
    "00:00 - 00:05\n",
    "\n",
    "Welcome, let us learn about rule-based information extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. What is RegEx?\n",
    "------------------\n",
    "\n",
    "00:05 - 00:45\n",
    "\n",
    "Rule-based information extraction is useful for many NLP tasks. Certain types of entities, such as dates or phone numbers have distinct formats that can be recognized by a set of rules without needing to train any model. Regular expressions, or RegEx, are used for rule-based information extraction with complex string matching patterns. RegEx can be used to retrieve patterns or replace matching patterns in a string with some other patterns. For example, given a text, we can use regular expressions to find any reference to links or phone numbers.\n",
    "\n",
    "• Rule-based information extraction (IR) is useful for many NLP tasks\n",
    "\n",
    "• Regular expression (RegEx) is used with complex string matching patterns\n",
    "\n",
    "• RegEx finds and retrieves patterns or replace matching patterns\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam quis purus a odio dapibus volutpat. Donec sed enim consequat, dapibus nisl at, fermentum tellus. Suspendisse id hendrerit felis. Sed sit amet hendrerit metus. https://www.att.com. Aliquam erat volutpat. In lobortis fermentum nulla non ullamcorper.\n",
    "\n",
    "www.tellus.com. Donec elementum nibh ut tellus hendrerit consectetur. 555-555-5555 Aliquam eget imperdiet diam. Phasellus molestie rhoncus massa nec bibendum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. RegEx strengths and weaknesses\n",
    "----------------------------------\n",
    "\n",
    "00:45 - 01:30\n",
    "\n",
    "Nearly all data scientists and engineers use RegEx at some stage in their workflow from cleaning data to implementing machine learning models. There are several advantages in using RegEx. Due to its complex syntax, it allows programmers to write robust rules. It allows finding all types of variances in strings, performs quickly and it is supported by different programming languages. Despite these advantages, RegEx has a few weaknesses. Its syntax is quite difficult for beginners. Writing good RegEx patterns requires a knowledge of all the ways a pattern may vary in texts.\n",
    "\n",
    "Pros:\n",
    "• Enables writing robust rules to retrieve information\n",
    "• Can allow us to find many types of variance in strings  \n",
    "• Runs fast\n",
    "• Supported by programming languages\n",
    "\n",
    "Cons:\n",
    "• Syntax is challenging for beginners\n",
    "• Requires knowledge of all the ways a pattern may be mentioned in texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. RegEx in Python\n",
    "-------------------\n",
    "\n",
    "01:30 - 02:26\n",
    "\n",
    "Python comes prepackaged with a RegEx library, called re. Let's assume we want to find the phone numbers in a text. The first step is to define a pattern. Assuming a phone number is always written as something like 3 digits-3 digits-4 digits, a pattern to find such phone numbers is shown. In this pattern, backslash-d is representative of a metacharacter that matches any digit from 0 to 9. A number within curly brackets shows how many occurrences of the pattern are expected. Hence parenthesis backslash-d curly brackets 3 is looking for three digits. We also use dash in between digits to match the shape of the phone number.\n",
    "\n",
    "• Python comes prepackaged with a RegEx library, re.\n",
    "\n",
    "• The first step in using re package is to define a pattern.\n",
    "\n",
    "• The resulting pattern is used to find matching content.\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "pattern = r\"((\\d){3}-(\\d){3}-(\\d){4})\"\n",
    "text = \"Our phone number is 832-123-5555 and their phone number is 425-123-4567.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. RegEx in Python\n",
    "-------------------\n",
    "\n",
    "02:26 - 03:10\n",
    "\n",
    "To find any matching patterns in a given text, we can use re-dot-finditer() method from the re package. We can iterate through found matches. Every match contains of start and end characters of the matching section of the text, they are accessible using match-dot-start() and match-dot-end() methods. We can see that two phone numbers that are matching the given pattern, 832-123-5555 and 425-123-4567, are found with their corresponding start and end characters from the input text.\n",
    "\n",
    "• We use .finditer() method from re package\n",
    "\n",
    "```python\n",
    "iter_matches = re.finditer(pattern, text)\n",
    "for match in iter_matches:\n",
    "    start_char = match.start()\n",
    "    end_char = match.end()\n",
    "    print(\"Start character: \", start_char, \"| End character: \", end_char,\n",
    "          \"| Matching text: \", text[start_char:end_char])\n",
    "```\n",
    "\n",
    "```\n",
    "Start character:  20 | End character:  32 | Matching text:  832-123-5555\n",
    "Start character:  59 | End character:  71 | Matching text:  425-123-4567\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. RegEx in spaCy\n",
    "------------------\n",
    "\n",
    "03:10 - 04:48\n",
    "\n",
    "spaCy has quick ways to implement RegEx in three pipes: Matcher, PhraseMatcher, and EntityRuler. Matcher and PhraseMatcher do not align the matched patterns as entities in the doc-dot-ents. For this reason, we utilize EntityRuler to implement regular expressions. We have already learned to use EntityRuler to improve entity recognition accuracy in spaCy. We will learn more about Marcher and PhraseMatcher later on. Let's look at an example of using EntityRuler to find phone numbers. The pattern consists of a list of dictionaries with two keys of label and pattern. In this instance, the label is set to PHONE_NUMBER. To match a pattern such as 3 digits-3 digits-4 digits, we use a pattern that consists of 5 smaller dictionaries, where each dictionary is representing a part of the matching pattern. The first, third and fifth dictionaries with the key of SHAPE, are representing patterns with a shape of three or four digits by using three or four d's. The second and fourth dictionaries with a key of ORTH, are representing the exact match of a string, which is set to a dash in this pattern. Writing patterns in spaCy requires practice, spaCy documentation provides more information about different pattern attributes.\n",
    "\n",
    "• RegEx in three pipeline components: Matcher, PhraseMatcher and EntityRuler.\n",
    "\n",
    "```python\n",
    "text = \"Our phone number is 832-123-5555 and their phone number is 425-123-4567.\"\n",
    "nlp = spacy.blank(\"en\")\n",
    "patterns = [{\"label\": \"PHONE_NUMBER\", \"pattern\": [{\"SHAPE\": \"ddd\"},\n",
    "           {\"ORTH\": \"-\"}, {\"SHAPE\": \"ddd\"},\n",
    "           {\"ORTH\": \"-\"}, {\"SHAPE\": \"dddd\"}]}]\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "```\n",
    "\n",
    "```\n",
    "[('832-123-5555', 'PHONE_NUMBER'), ('425-123-4567', 'PHONE_NUMBER')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:48 - 04:51\n",
    "\n",
    "Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegEx in Python\n",
    "===============\n",
    "\n",
    "Rule-based information extraction is useful for many NLP tasks. Certain types of entities, such as dates or phone numbers have distinct formats that can be recognized by a set of rules without needing to train any model. In this exercise, you will practice using `re`package for RegEx. The goal is to find phone numbers in a given `text`.\n",
    "\n",
    "`re` package is already imported for your use. You can use `\\d` to match string patterns representative of a metacharacter that matches any digit from 0 to 9.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Define a pattern to match phone numbers of the form (111)-111-1111.\n",
    "-   Find all the matching patterns using `re.finditer()` method.\n",
    "-   For each match, print start and end characters and matching section of the given `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Our phone number is (425)-123-4567.\"\n",
    "\n",
    "# Define a pattern to match phone numbers\n",
    "pattern = r\"\\((\\d){3}\\)-(\\d){3}-(\\d){4}\"\n",
    "\n",
    "# Find all the matching patterns in the text\n",
    "phones = re.finditer(pattern, text)\n",
    "\n",
    "# Print start and end characters and matching section of the text\n",
    "for match in phones:\n",
    "    start_char = match.start()\n",
    "    end_char = match.end()\n",
    "    print(\"Start character: \", start_char, \"| End character: \", end_char, \"| Matching text: \", text[start_char:end_char])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegEx with EntityRuler in spaCy\n",
    "===============================\n",
    "\n",
    "Regular expressions, or RegEx, are used for rule-based information extraction with complex string matching patterns. RegEx can be used to retrieve patterns or replace matching patterns in a string with some other patterns. In this exercise, you will practice using `EntityRuler`in `spaCy` to find email addresses in a given `text`.\n",
    "\n",
    "`spaCy` package is already imported for your use. You can use `\\d` to match string patterns representative of a metacharacter that matches any digit from 0 to 9.\n",
    "\n",
    "A `spaCy` pattern can use `REGEX` as an attribute. In this case, a pattern will be of shape `[{\"TEXT\": {\"REGEX\": \"<a given pattern>\"}}]`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Define a pattern to match phone numbers of the form `8888888888` to be used by the `EntityRuler`.\n",
    "-   Load a blank `spaCy` English model and add an `EntityRuler` component to the pipeline.\n",
    "-   Add the compiled pattern to the `EntityRuler` component.\n",
    "-   Run the model and print the tuple of text and type of entities for the given `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Our phone number is 4251234567.\"\n",
    "\n",
    "# Define a pattern to match phone numbers\n",
    "patterns = [{\"label\": \"PHONE_NUMBERS\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"(\\d){10}\"}}]}]\n",
    "\n",
    "# Load a blank model and add an EntityRuler\n",
    "nlp = spacy.blank(\"en\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add the compiled patterns to the EntityRuler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Print the tuple of entities texts and types for the given text\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. spaCy Matcher and PhraseMatcher\n",
    "-----------------------------------\n",
    "\n",
    "00:00 - 00:07\n",
    "\n",
    "Welcome! Let us learn more about rule-based information extraction using spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Matcher in spaCy\n",
    "--------------------\n",
    "\n",
    "00:07 - 00:51\n",
    "\n",
    "RegEx patterns are not trivial to read and debug. For these reasons, spaCy provides a readable, production-level, and maintainable alternative, the Matcher class. The Matcher class can match predefined rules to a sequence of tokens in Doc containers. Let's look at an example. We first import spaCy and the Matcher class. We then load the en_core_web_sm model and run the model on the given text to generate a Doc container. Next, a Matcher object is initialized with the given model's vocabulary by using Matcher(nlp-dot-vocab).\n",
    "\n",
    "• RegEx patterns can be complex, difficult to read and debug.\n",
    "\n",
    "• spaCy provides a readable and production-level alternative, the Matcher class.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Good morning, this is our first day on campus.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Matcher in spaCy\n",
    "--------------------\n",
    "\n",
    "00:51 - 01:33\n",
    "\n",
    "Next, we define a pattern to match lower cased good and morning by defining a list with two key value pairs. The first one, has a key of \"LOWER\" and value of \"good\" and the second one, has a key of \"LOWER\" and value of \"morning\". Then we add this pattern with a custom name, such as morning_greeting, to a list of patterns in the Matcher object and run the matcher on the Doc container. The output of a Matcher object is matched patterns which include tuples of a match id, start and end token indices of the matched pattern.\n",
    "\n",
    "• Matching output include start and end token indices of the matched pattern.\n",
    "\n",
    "```python\n",
    "pattern = [{\"LOWER\": \"good\"}, {\"LOWER\": \"morning\"}]\n",
    "matcher.add(\"morning_greeting\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end,\n",
    "          \"| Matched text: \", doc[start:end].text)\n",
    "```\n",
    "\n",
    "```\n",
    "Start token: 0 | End token: 2 | Matched text: Good morning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Matcher extended syntax support\n",
    "-----------------------------------\n",
    "\n",
    "01:33 - 01:57\n",
    "\n",
    "The Matcher class allows patterns to be more expressive by allowing some operators inside the curly brackets. These operators are for extended comparison and look similar to Python's in, not in and comparison operators. The table shows a list of supported operators in the Matcher class.\n",
    "\n",
    "• Allows operators in defining the matching patterns.\n",
    "\n",
    "• Similar operators to Python's in, not in and comparison operators\n",
    "\n",
    "| Attribute | Value type | Description |\n",
    "|-----------|------------|-------------|\n",
    "| IN | any type | Attribute value is a member of a list |\n",
    "| NOT_IN | any type | Attribute value is not a member of a list |\n",
    "| ==, >=, <=, >, < | int, float | Comparison operators for equality or inequality checks |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Matcher extended syntax support\n",
    "-----------------------------------\n",
    "\n",
    "01:57 - 02:33\n",
    "\n",
    "For instance, if we want to match both lowercase good morning and good evening patterns in a text, we can use a single matching pattern and the IN operator. In this case, the pattern will be a list of two key value pairs. The first one is {\"LOWER\": \"good\"} and the second one is {\"LOWER\": {\"IN\": [\"morning\", \"evening\"]}}.\n",
    "\n",
    "• Using IN operator to match both good morning and good evening\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Good morning and good evening.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"good\"}, {\"LOWER\": {\"IN\": [\"morning\", \"evening\"]}}]\n",
    "matcher.add(\"morning_greeting\", [pattern])\n",
    "matches = matcher(doc)\n",
    "```\n",
    "\n",
    "• The output of matching using IN operator\n",
    "\n",
    "```python\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end,\n",
    "          \"| Matched text: \", doc[start:end].text)\n",
    "```\n",
    "\n",
    "```\n",
    "Start token: 0 | End token: 2 | Matched text: Good morning\n",
    "Start token: 3 | End token: 5 | Matched text: good evening\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. PhraseMatcher in spaCy\n",
    "--------------------------\n",
    "\n",
    "02:33 - 03:26\n",
    "\n",
    "While processing unstructured text, we often have long lists and dictionaries that we want to scan and match in given texts. The Matcher patterns are handcrafted and each token needs to be coded individually. If we have a long list of phrases, Matcher is no longer the best option. In this instance, PhraseMatcher class helps us match long dictionaries. As an example for PhraseMatcher, let's assume that we want to match two terms in a given text, Bill Gates and John Smith. First, we import spaCy and PhraseMatcher class. Then, we load the en_core_web_sm model and initialize the PhraseMatcher object using PhraseMatcher(nlp-dot-vocab).\n",
    "\n",
    "- PhraseMatcher class matches a long list of phrases in a given text.\n",
    "\n",
    "```python\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = [\"Bill Gates\", \"John Smith\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. PhraseMatcher in spaCy\n",
    "--------------------------\n",
    "\n",
    "03:26 - 04:00\n",
    "\n",
    "Next, we create patterns for the PhraseMatcher object, by calling the nlp-dot-make_doc() method on each term. This method converts given terms into pattern entities, that are usable by the PhraseMatcher class. Then, we follow similar steps as the Matcher class, and run the PhraseMatcher object on the given Doc container of a text and iterate through matches to extract start and end token IDs of the matched patterns.\n",
    "\n",
    "- PhraseMatcher outputs include start and end token indices of the matched pattern\n",
    "\n",
    "```python\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "matcher.add(\"PeopleOfInterest\", patterns)\n",
    "doc = nlp(\"Bill Gates met John Smith for an important discussion regarding importance of AI.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end,\n",
    "          \"| Matched text: \", doc[start:end].text)\n",
    "```\n",
    "\n",
    "```\n",
    "Start token: 0 | End token: 2 | Matched text: Bill Gates\n",
    "Start token: 3 | End token: 5 | Matched text: John Smith\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. PhraseMatcher in spaCy\n",
    "--------------------------\n",
    "\n",
    "04:00 - 04:54\n",
    "\n",
    "The previous example shows how we can match patterns by their exact values. If we want to match lower cased patterns or utilize shape of a pattern for matching, we can use the attr (attribute) argument in the PhraseMatcher class. In one example, we set the attr argument to LOWER and allow PhraseMatcher to find lower cased matching patterns. In the second example, by setting the attr argument to SHAPE, we are asking PhraseMatcher to match patterns to a given shape. In this instance, we are looking to retrieve IP addresses in a text and provide multiple examples of them, such as 110-dot-0-dot-0-dot-0 to the PhraseMatcher class.\n",
    "\n",
    "- We can use `attr` argument of the `PhraseMatcher` class\n",
    "\n",
    "```python\n",
    "matcher = PhraseMatcher(nlp.vocab, attr = \"LOWER\")\n",
    "terms = [\"Government\", \"Investment\"]\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "matcher.add(\"InvestmentTerms\", patterns)\n",
    "doc = nlp(\"It was interesting to the investment division of the government.\")\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab, attr = \"SHAPE\")\n",
    "terms = [\"110.0.0.0\", \"101.243.0.0\"]\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "matcher.add(\"IPAddresses\", patterns)\n",
    "doc = nlp(\"The tracked IP address was 234.135.0.0.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:54 - 04:57\n",
    "\n",
    "Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching a single term in spaCy\n",
    "===============================\n",
    "\n",
    "RegEx patterns are not trivial to read, write and debug. But you are not at a loss, spaCy provides a readable and production-level alternative, the Matcher class. The Matcher class can match predefined rules to a sequence of tokens in a given Doc container. In this exercise, you will practice using `Matcher`to find a single word.\n",
    "\n",
    "You can access the corresponding text in `example_text` and use `nlp` and `doc` to access an `spaCy` model and `Doc` container of `example_text` respectively.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Initialize a `Matcher` class.\n",
    "-   Define a pattern to match lower cased `witch` in the `example_text`.\n",
    "-   Add the patterns to the `Matcher` class and find matches.\n",
    "-   Iterate through matches and print start and end token indices and span of the matched text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Initialize a Matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define a pattern to match lower cased word witch\n",
    "pattern = [{\"lower\": \"witch\"}]\n",
    "\n",
    "# Add the pattern to matcher object and find matches\n",
    "matcher.add(\"CustomMatcher\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Print start and end token indices and span of the matched text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PhraseMatcher in spaCy\n",
    "======================\n",
    "\n",
    "While processing unstructured text, you often have long lists and dictionaries that you want to scan and match in given texts. The Matcher patterns are handcrafted and each token needs to be coded individually. If you have a long list of phrases, `Matcher` is no longer the best option. In this instance, `PhraseMatcher`class helps us match long dictionaries. In this exercise, you will practice to retrieve patterns with matching shapes to multiple terms using `PhraseMatcher` class.\n",
    "\n",
    "`en_core_web_sm` model is already loaded and ready for you to use as `nlp`. `PhraseMatcher`class is imported. A `text` string and a list of `terms` are available for your use.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Initialize a `PhraseMatcher` class with an `attr` to match to shape of given `terms`.\n",
    "-   Create `patterns` to add to the `PhraseMatcher` object.\n",
    "-   Find matches to the given patterns and print start and end token indices and matching section of the given `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"There are only a few acceptable IP addresse: (1) 127.100.0.1, (2) 123.4.1.0.\"\n",
    "terms = [\"110.0.0.0\", \"101.243.0.0\"]\n",
    "\n",
    "# Initialize a PhraseMatcher class to match to shapes of given terms\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
    "\n",
    "# Create patterns to add to the PhraseMatcher object\n",
    "patterns = [nlp(term) for term in terms]\n",
    "matcher.add(\"IPAddresses\", patterns)\n",
    "\n",
    "# Find matches to the given patterns and print start and end characters and matches texts\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching with extended syntax in spaCy\n",
    "======================================\n",
    "\n",
    "Rule-based information extraction is essential for any NLP pipeline. The Matcher class allows patterns to be more expressive by allowing some operators inside the curly brackets. These operators are for extended comparison and look similar to Python's in, not in and comparison operators. In this exercise, you will practice with `spaCy`'s matching functionality, `Matcher`, to find matches for given terms from an example text.\n",
    "\n",
    "`Matcher` class is already imported from `spacy.matcher` library. You will use a `Doc`container of an example text in this exercise by calling `doc`. A pre-loaded `spaCy` model is also accessible at `nlp`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Define a matcher object using `Matcher` and `nlp`.\n",
    "-   Use the `IN` operator to define a pattern to match `tiny squares` and `tiny mouthful`.\n",
    "-   Use this pattern to find matches for `doc`.\n",
    "-   Print start and end token indices and text span of the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Define a matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define a pattern to match tiny squares and tiny mouthful\n",
    "pattern = [{\"lower\": \"tiny\"}, {\"lower\": {\"IN\": [\"squares\", \"mouthful\"]}}]\n",
    "\n",
    "# Add the pattern to matcher object and find matches\n",
    "matcher.add(\"CustomMatcher\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Print out start and end token indices and the matched text span per match\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
