{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. spaCy pipelines\n",
    "-------------------\n",
    "\n",
    "00:00 - 00:07\n",
    "\n",
    "Welcome! We previously learned about spaCy pipelines, let's explore them further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. spaCy pipelines\n",
    "-------------------\n",
    "\n",
    "00:07 - 00:22\n",
    "\n",
    "Recall that when we call nlp on a text, spaCy first tokenizes the text to produce a Doc container. The Doc object is then processed in several different steps, known as the processing pipeline.\n",
    "\n",
    "• spaCy first tokenizes the text to produce a Doc object\n",
    "\n",
    "• The Doc is processed in several different steps of processing pipeline\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(example_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. spaCy pipelines\n",
    "-------------------\n",
    "\n",
    "00:22 - 01:36\n",
    "\n",
    "To continue our learnings on spaCy pipelines, in this video, we will explore how to create pipeline components and add them to an existing or blank spaCy pipeline. A pipeline is a sequence of pipes (pipeline components), or actors on data, that make alterations to the data or extract information from it. In some cases, later pipes require the output from earlier components, while in other cases, a pipe can exist entirely on its own. As an example, for a named entity recognition pipeline, three pipes can be used: a Tokenizer pipe, which is the first processing step in spaCy pipelines; a rule-based named entity recognizer known as the EntityRuler, which finds entities; and an EntityLinker pipe that identifies the type of each entity. Through this processing pipeline, an input text is converted to a Doc container with its corresponding annotated entities. We can use the doc-dot-ents feature to find the entities in the input text.\n",
    "\n",
    "• A pipeline is a sequence of pipes, or actors on data\n",
    "\n",
    "• A spaCy NER pipeline:\n",
    "  * Tokenization \n",
    "  * Named entity identification\n",
    "  * Named entity classification\n",
    "\n",
    "```\n",
    "[Input text] --> [Tokenizer] --> [EntityRuler] --> [EntityLinker] --> [Doc with annotated entities]\n",
    "```\n",
    "\n",
    "```python\n",
    "print([ent.text for ent in doc.ents])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Adding pipes\n",
    "----------------\n",
    "\n",
    "01:36 - 02:48\n",
    "\n",
    "We often use an existing spaCy model. However, in some cases, an off-the-shelf model will not satisfy our requirements. An example of this is the sentence segmentation for a long document with 10,000 sentences. To recall, sentence segmentation is breaking a text into its given sentences. Sentencizer is the name of the spaCy pipeline component that performs sentence segmentation. Given a document that has 10,000 sentences, even if we use the smallest English model, the most efficient spaCy model, en_core_web_sm, the model can take a long time to process 10,000 sentences and separate them. The reason is that when calling an existing spaCy model on a text, the whole NLP pipeline will be activated and that means that each pipe from named entity recognition to dependency parsing will run on the text. This increases the use of computational time by 100 times.\n",
    "\n",
    "• sentencizer: spaCy pipeline component for sentence segmentation.\n",
    "\n",
    "```python\n",
    "text = \" \".join([\"This is a test sentence.\"]*10000)\n",
    "en_core_sm_nlp = spacy.load(\"en_core_web_sm\")\n",
    "start_time = time.time()\n",
    "doc = en_core_sm_nlp(text)\n",
    "print(f\"Finished processing with en_core_web_sm model in {round((time.time() - start_time)/60.0 , 5)} minutes\")\n",
    "```\n",
    "\n",
    "```\n",
    ">>> Finished processing with en_core_web_sm model in 0.09332 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Adding pipes\n",
    "----------------\n",
    "\n",
    "02:48 - 03:28\n",
    "\n",
    "In this instance, we would want to make a blank spaCy English model by using spacy-dot-blank(\"en\") and add the sentencizer component to the pipeline by using -dot-add_pipe method of the nlp model. By creating a blank model and simply adding a sentencizer pipe, we can considerably reduce computational time. The reason is that for this version of the spaCy model, only intended pipeline component (sentence segmentation) will run on the given documents.\n",
    "\n",
    "• Create a blank model and add a sentencizer pipe:\n",
    "\n",
    "```python\n",
    "blank_nlp = spacy.blank(\"en\")\n",
    "blank_nlp.add_pipe(\"sentencizer\")\n",
    "start_time = time.time()\n",
    "doc = blank_nlp(text)\n",
    "print(f\"Finished processing with blank model in {round((time.time() - start_time)/60.0 , 5)} minutes\")\n",
    "```\n",
    "\n",
    "```\n",
    ">>> Finished processing with blank model in 0.00091 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Analyzing pipeline components\n",
    "---------------------------------\n",
    "\n",
    "03:28 - 04:25\n",
    "\n",
    "spaCy allows us to analyze a spaCy pipeline to check whether any required attributes are not set. The nlp-dot-analyze_pipes method analyzes the components in a pipeline and outputs structured information about them, like the attributes they set on the Doc and Token, whether they retokenize the Doc and which scores they produce during training. It also shows warnings if components require values that are not set by the previous components. For example, when the entity linker is used but no component before EntityLinker sets named entities. While calling nlp-dot-analyze_pipes() method we can also set the pretty argument to True, which will print a nicely organized table as the result of analyzing the pipeline components.\n",
    "\n",
    "• nlp.analyze_pipes() analyzes a spaCy pipeline to determine:\n",
    "  * Attributes that pipeline components set\n",
    "  * Scores a component produces during training\n",
    "  * Presence of all required attributes\n",
    "\n",
    "• Setting pretty to True will print a table instead of only returning the structured data.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "analysis = nlp.analyze_pipes(pretty=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Analyzing pipeline components\n",
    "---------------------------------\n",
    "\n",
    "04:25 - 04:47\n",
    "\n",
    "The snapshot shows the results of the analyze_pipes method. While we don't go into technical details of all the fields, we are familiar with some of the components and attributes provided in this snapshot. In this case, the result of analysis is \"No problems found\".\n",
    "\n",
    "```\n",
    "=============================== Pipeline Overview ===============================\n",
    "\n",
    "#   Component         Assigns              Requires        Scores             Retokenizes\n",
    "-   ---------------   -------------------  -------------   ---------------    -----------\n",
    "0   tok2vec          doc.tensor                                             False\n",
    "\n",
    "1   tagger           token.tag                            tag_acc           False\n",
    "\n",
    "2   parser           token.dep                            dep_uas           False\n",
    "                     token.head                           dep_las\n",
    "                     token.is_sent_start                  dep_las_per_type\n",
    "                     doc.sents                            sents_p\n",
    "                                                         sents_r\n",
    "                                                         sents_f\n",
    "\n",
    "3   attribute_ruler                                                         False\n",
    "\n",
    "4   lemmatizer       token.lemma                         lemma_acc          False\n",
    "\n",
    "5   ner              doc.ents                            ents_f             False\n",
    "                     token.ent_iob                       ents_p\n",
    "                     token.ent_type                      ents_r\n",
    "                                                        ents_per_type\n",
    "\n",
    "6   entity_linker    token.ent_kb_id      doc.ents       nel_micro_f        False\n",
    "                                         doc.sents       nel_micro_r\n",
    "                                         token.ent_iob   nel_micro_p\n",
    "                                         token.ent_type\n",
    "\n",
    "✓ No problems found.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:47 - 04:50\n",
    "\n",
    "Let's practice our learnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding pipes in spaCy\n",
    "=====================\n",
    "\n",
    "You often use an existing spaCy model for different NLP tasks. However, in some cases, an off-the-shelf pipeline component such as sentence segmentation will take long times to produce expected results. In this exercise, you'll practice adding a pipeline component to a spaCy model (text processing pipeline). \n",
    "\n",
    "You will use the first five reviews from the Amazon Fine Food Reviews dataset for this exercise. You can access these reviews by using the `texts` string. \n",
    "\n",
    "The `spaCy` package is already imported for you to use.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Load a blank `spaCy` English model and add a `sentencizer` component to the model.\n",
    "-   Create a `Doc` container for the `texts`, create a list to store `sentences` of the given document and print its number of sentences.\n",
    "-   Print the list of tokens in the second sentence from the `sentences` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a blank spaCy English model and add a sentencizer component\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create Doc containers, store sentences and print its number of sentences\n",
    "doc = nlp(texts)\n",
    "sentences = [sent for sent in doc.sents]\n",
    "print(\"Number of sentences: \", len(sentences), \"\\n\")\n",
    "\n",
    "# Print the list of tokens in the second sentence\n",
    "print(\"Second sentence tokens: \", [token for token in sentences[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing pipelines in spaCy\n",
    "============================\n",
    "\n",
    "`spaCy` allows you to analyze a spaCy pipeline to check whether any required attributes are not set. In this exercise, you'll practice analyzing a `spaCy` pipeline. Earlier in the video, an existing `en_core_web_sm` pipeline was analyzed and the result was `No problems found.`, in this instance, you will analyze a blank `spaCy` English model with few added components and observe results of the analysis.\n",
    "\n",
    "The `spaCy` package is already imported for you to use.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Load a blank `spaCy` English model as `nlp`.\n",
    "-   Add `tagger` and `entity_linker` pipeline components to the blank model.\n",
    "-   Analyze the `nlp` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a blank spaCy English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add tagger and entity_linker pipeline components\n",
    "nlp.add_pipe(\"tagger\")\n",
    "nlp.add_pipe(\"entity_linker\")\n",
    "\n",
    "# Analyze the pipeline\n",
    "analysis = nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "Question\n",
    "--------\n",
    "\n",
    "The output of `analyze_pipes()` method showed that `entity_linker requirements not met: doc.ents, doc.sents, token.ent_iob, token.ent_type`.\n",
    "\n",
    "Which NLP components should be added before adding `entity_linker` component to ensure the created `spaCy` pipeline have all the required attributes for entity linking?\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "[/] ner, sentencizer\n",
    "\n",
    "[] ner, lemmatizer\n",
    "\n",
    "[] lemmatizer, sentencizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. spaCy EntityRuler\n",
    "---------------------\n",
    "\n",
    "00:00 - 00:11\n",
    "\n",
    "Welcome! Let's learn about EntityRuler, a component in spaCy that allows us to include or modify named entities using pattern matching rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. spaCy EntityRuler\n",
    "---------------------\n",
    "\n",
    "00:11 - 01:52\n",
    "\n",
    "EntityRuler lets us add entities to Doc-dot-ents. It can be combined with EntityRecognizer, a spaCy pipeline component for named-entity recognition, to boost accuracy, or used on its own to implement a purely rule-based entity recognition system. We can add named-entities to a Doc container using an entity pattern. Entity patterns are dictionaries with two keys. One key is the \"label\" which is specifying the label to assign to the entity if the pattern is matched, and the second key is the \"pattern\", which is the matched string. The entity ruler accepts two types of patterns: phrase entity and token entity patterns. A phrase entity pattern is used for exact string matches, for example to exactly match Microsoft as a named entity with a label of ORG, we can use an entity pattern dictionary with a \"label\" equal to ORG and the \"pattern\" to be set as \"Microsoft\". A token entity pattern uses one dictionary to describe one token. For example, to match lower cases san francisco to an entity type of GPE (a location type), we can use an entity pattern dictionary with a \"label\" equal to GPE and the \"pattern\" to be set to a list of two key value pairs where the key is set to \"LOWER\" and the value is set to \"san\" for one and \"francisco\" for the other pair.\n",
    "\n",
    "• EntityRuler adds named-entities to a Doc container\n",
    "\n",
    "• It can be used on its own or combined with EntityRecognizer\n",
    "\n",
    "• Phrase entity patterns for exact string matches (string):\n",
    "```python\n",
    "{\"label\": \"ORG\", \"pattern\": \"Microsoft\"}\n",
    "```\n",
    "\n",
    "• Token entity patterns with one dictionary describing one token (list):\n",
    "```python\n",
    "{\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Adding EntityRuler to spaCy pipeline\n",
    "----------------------------------------\n",
    "\n",
    "01:52 - 02:49\n",
    "\n",
    "The EntityRuler can be added to a spaCy model using dot-add_pipe() method by passing \"entity_ruler\" name. When the nlp model is called on a text, it will find matches in the doc container and add them as entities in the doc-dot-ents, using the specified pattern label as the entity label. As an example, we load a blank spaCy model and use -dot-add_pipe(\"entity_ruler\") method to add EntityRuler component. Next, we define a list of patterns. Patterns can be a combination of phrase entity and token entity patterns. These patterns can be added to the EntityRuler component using -dot-add_patterns() method.\n",
    "\n",
    "• Using .add_pipe() method \n",
    "\n",
    "• List of patterns can be added using .add_patterns() method\n",
    "\n",
    "```python\n",
    "nlp = spacy.blank(\"en\")\n",
    "entity_ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Microsoft\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
    "entity_ruler.add_patterns(patterns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Adding EntityRuler to spaCy pipeline\n",
    "----------------------------------------\n",
    "\n",
    "02:49 - 03:15\n",
    "\n",
    "Next, we run the model on a given text to generate a Doc container. The nlp model uses the EntityRuler component to populate the dot-ents attribute of the Doc container. In this instance, Microsoft and San Francisco are extracted as entities with ORG and GPE entity labels respectively.\n",
    "\n",
    "• .ents store the results of an EntityLinker component\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Microsoft is hiring software developer in San Francisco.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "```\n",
    "\n",
    "```\n",
    "[('Microsoft', 'ORG'), ('San Francisco', 'GPE')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. EntityRuler in action\n",
    "-------------------------\n",
    "\n",
    "03:15 - 03:38\n",
    "\n",
    "The entity ruler is designed to integrate with spaCy's existing components and enhance the named entity recognizer performance. Let us look at an example of \"Manhattan associates is a company in the US\". In this case, the model is unable to accurately classify Manhattan associates as an ORG.\n",
    "\n",
    "• Integrates with spaCy pipeline components\n",
    "\n",
    "• Enhances the named-entity recognizer\n",
    "\n",
    "• spaCy model without EntityRuler:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Manhattan associates is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "```\n",
    "\n",
    "```\n",
    "[('Manhattan', 'GPE'), ('U.S.', 'GPE')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. EntityRuler in action\n",
    "-------------------------\n",
    "\n",
    "03:38 - 04:21\n",
    "\n",
    "We can add an EntityRuler component to the current nlp pipeline. If we add the ruler after an existing ner component by setting the \"after\" argument of the -dot-add_pipe() method to \"ner\", the entity ruler will only add entities to the doc-dot-ents if they don't overlap with existing entities predicted by the model. In this case, the model tags Manhattan with an incorrect GPE type, because the ruler component is called after existing ner (EntityRecognizer) component of the model.\n",
    "\n",
    "• EntityRuler added after existing ner component:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", after='ner')\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": \"manhattan\"}, {\"lower\": \"associates\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Manhattan associates is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "```\n",
    "\n",
    "```\n",
    "[('Manhattan', 'GPE'), ('U.S.', 'GPE')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. EntityRuler in action\n",
    "-------------------------\n",
    "\n",
    "04:21 - 04:50\n",
    "\n",
    "However, if we add an EntityRuler before the ner component by setting the \"before\" argument of -dot-add_pipe() method to \"ner\", to recognize Manhattan associate as an ORG, the entity recognizer will respect the existing entity spans and adjust its predictions based on patterns added to the EntityRuler. This can improve model accuracy in our case.\n",
    "\n",
    "• EntityRuler added before existing ner component:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before='ner')\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": \"manhattan\"}, {\"lower\": \"associates\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Manhattan associates is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "```\n",
    "\n",
    "```\n",
    "[('Manhattan associates', 'ORG'), ('U.S.', 'GPE')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:50 - 04:54\n",
    "\n",
    "Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EntityRuler with blank spaCy model\n",
    "==================================\n",
    "\n",
    "`EntityRuler` lets you to add entities to `doc.ents`. It can be combined with `EntityRecognizer`, a spaCy pipeline component for named-entity recognition, to boost accuracy, or used on its own to implement a purely rule-based entity recognition system. In this exercise, you will practice adding an `EntityRuler` component to a blank `spaCy` English model and classify named entities of the given `text` using purely rule-based named-entity recognition.\n",
    "\n",
    "The `spaCy` package is already imported and a blank `spaCy` English model is ready for your use as `nlp`. A list of `patterns` to classify lower cased `OpenAI` and `Microsoft` as `ORG`is already created for your use.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create and add an `EntityRuler`component to the pipeline.\n",
    "-   Add given patterns to the `EntityRuler`component. \n",
    "-   Run the model on the given `text` and create its corresponding `Doc` container. \n",
    "-   Print a tuple of (entities text and types) for all entities in the `Doc` container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"openai\"}]},\n",
    "            {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"microsoft\"}]}]\n",
    "text = \"OpenAI has joined forces with Microsoft.\"\n",
    "\n",
    "# Add EntityRuler component to the model\n",
    "entity_ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add given patterns to the EntityRuler component\n",
    "entity_ruler.add_patterns(patterns)\n",
    "\n",
    "# Run the model on a given text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print entities text and type for all entities in the Doc container\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EntityRuler for NER\n",
    "===================\n",
    "\n",
    "`EntityRuler` can be combined with `EntityRecognizer` of an existing model to boost its accuracy. In this exercise, you will practice combining an `EntityRuler`component and an existing `NER` component of the `en_core_web_sm` model. The model is already loaded as `nlp`. \n",
    "\n",
    "When `EntityRuler` is added before `NER`component, the entity recognizer will respect the existing entity spans and adjust its predictions based on patterns added to the `EntityRuler` to improve accuracy of named entity recognition task.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Add an `EntityRuler` to the `nlp` before `ner` component.\n",
    "-   Define a token entity pattern to classify lower cased `new york group` as `ORG`.\n",
    "-   Add the `patterns` to the `EntityRuler`component.\n",
    "-   Run the model and print the tuple of entities text and type for the `Doc` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"New York Group was built in 1987.\"\n",
    "\n",
    "# Add an EntityRuler to the nlp before NER component\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# Define a pattern to classify lower cased new york group as ORG\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": \"new york group\"}]}]\n",
    "\n",
    "# Add the patterns to the EntityRuler component\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Run the model and print entities text and type for all the entities\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EntityRuler with multi-patterns in spaCy\n",
    "========================================\n",
    "\n",
    "`EntityRuler` lets you to add entities to `doc.ents` and boost its named entity recognition performance. In this exercise, you will practice adding an `EntityRuler`component to an existing `nlp` pipeline to ensure multiple entities are correctly being classified.\n",
    "\n",
    "The `en_core_web_sm` model is already loaded and is available for your use as `nlp`. You can access an example text in `example_text` and use `nlp` and `doc` to access an `spaCy` model and `Doc` container of `example_text`respectively.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Print a list of tuples of entities text and types in the `example_text` with the `nlp` model.\n",
    "-   Define multiple patterns to match lower cased `brother` and `sisters` to `PERSON`label.\n",
    "-   Add an `EntityRuler` component to the `nlp` pipeline and add the `patterns` to the `EntityRuler`.\n",
    "-   Print a tuple of text and type of entities for the `example_text` with the `nlp` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Print a list of tuples of entities text and types in the example_text\n",
    "print(\"Before EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents], \"\\n\")\n",
    "\n",
    "# Define pattern to add a label PERSON for lower cased sisters and brother entities\n",
    "patterns = [{\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"brother\"}]},\n",
    "            {\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"sisters\"}]}]\n",
    "\n",
    "# Add an EntityRuler component and add the patterns to the ruler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Print a list of tuples of entities text and types\n",
    "print(\"After EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
