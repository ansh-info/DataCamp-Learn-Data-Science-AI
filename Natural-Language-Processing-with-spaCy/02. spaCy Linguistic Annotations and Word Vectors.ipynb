{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Linguistic features\n",
    "-----------------------\n",
    "\n",
    "00:00 - 00:09\n",
    "\n",
    "Welcome! In this video, we will cover more details on POS tagging and introduce dependency parsing.\n",
    "\n",
    "2\\. POS tagging\n",
    "---------------\n",
    "\n",
    "00:09 - 00:44\n",
    "\n",
    "We have learned how we can use spaCy to extract part-of-speech tags. Each word is tagged by a POS tag depending on its context and the other surrounding words, and their POS tags. For example, given a tricky sentence such as \"My cat will fish for a fish tomorrow in a fishy way.\", the spaCy tagger makes correct POS tag predictions for the fish and fishy words by tagging the first word fish to VERB, the second word fish to NOUN and the word fishy to ADJ.\n",
    "\n",
    "3\\. What is the importance of POS?\n",
    "----------------------------------\n",
    "\n",
    "00:44 - 01:05\n",
    "\n",
    "Now, the question we might ask is what is the importance of POS tags? Many applications need to know the word type for better accuracy. For example, in translation systems, the word fish as a verb and as a noun will map to different words in Spanish.\n",
    "\n",
    "4\\. What is the importance of POS?\n",
    "----------------------------------\n",
    "\n",
    "01:05 - 01:46\n",
    "\n",
    "Syntactic information such as POS tags can help many tasks further down the pipeline such as word-sense disambiguation (WSD). WSD is a classical problem of deciding in which sense a word is used in a sentence. Determining the sense of the word can be crucial in search engines, machine translation, and question-answering systems. For example, for the word \"Play\", the POS tagger can help with WSD when the tagger labels the senses of word with a NOUN or VERB depending on its context.\n",
    "\n",
    "5\\. Word-sense disambiguation\n",
    "-----------------------------\n",
    "\n",
    "01:46 - 02:27\n",
    "\n",
    "Let's use POS tagging for WSD. We create a tuple of the token and the dot-pos_ tag by looping over each token in the Doc container and check if \"fish\" is in the tokenized text. The word fish, in \"I will fish tomorrow\", has a -dot-pos_ tag of a VERB, which identifies its sense correctly as \"to catch fish\". In the sentence \"I ate fish\", the word fish has a -dot-pos_ tag of NOUN and it identifies the sense as \"an animal\".\n",
    "\n",
    "6\\. Dependency parsing\n",
    "----------------------\n",
    "\n",
    "02:27 - 03:31\n",
    "\n",
    "We have learned POS tags, which are grammatical categories of words. POS tags do not reveal any relation between distant words in a given sentence. This is where dependency parsing comes in. This process provides a structured way of exploring the sentence syntax. It is analyzing sentence structure via dependencies between tokens. A dependency or a dependency relation is a directed link between two tokens. The result of this procedure is always a tree. For example, for the sentence \"We understand the differences.\", spaCy assigns a dependency label for each token such as \"nsubj\", \"dobj\" and \"det\". For example, the first arc with nsubj label shows the subject and verb relationship between \"we\" and \"understand\".\n",
    "\n",
    "7\\. Dependency parsing and spaCy\n",
    "--------------------------------\n",
    "\n",
    "03:31 - 03:55\n",
    "\n",
    "A dependency label describes the type of syntactic relation between two tokens. A few of the most common dependency labels is provided in the table such as nsubj (Nominal subject), root, det (determiner), dobj (direct object) and aux (auxiliary).\n",
    "\n",
    "8\\. Dependency parsing and displaCy\n",
    "-----------------------------------\n",
    "\n",
    "03:55 - 04:40\n",
    "\n",
    "Let's draw our first dependency tree using displaCy. We can use spacy-dot-displacy-dot-serve by passing two arguments, a Doc container of a given text and a word of \"dep\" (dependency) to display a dependency tree. In a dependency relation, one of the tokens is the parent, and the other is its dependent. For example, for the dependency relation between the words \"the\" and \"differences\", \"the\" is the dependent, and the dependency label is \"det\", which stands for determiner.\n",
    "\n",
    "9\\. Dependency parsing and spaCy\n",
    "--------------------------------\n",
    "\n",
    "04:40 - 04:58\n",
    "\n",
    "We use -dot-text and -dot-dep_ attributes of a token to access the dependency label of each of the tokens. We can also use spacy-dot-explain() method to view definition of each dependency label.\n",
    "\n",
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:58 - 05:02\n",
    "\n",
    "Great job! Let's practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linguistic annotations in spaCy\n",
    "\n",
    "Linguistic annotations that are available in spaCy, such as POS tagging, named-entity recognition, and dependency parsing, can be used to better understand writing quality and learn about many aspects of human language, including words, sentences, and meaning. In this exercise, you'll practice your learnings on different linguistic features.\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "-   Drag and drop a definition or spaCy token attribute into the correct bucket of the corresponding linguistic feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Linguistic-annotations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-sense disambiguation with spaCy\n",
    "====================================\n",
    "\n",
    "WSD is a classical problem of deciding in which sense a word is used in a sentence. Determining the sense of the word can be crucial in search engines, machine translation, and question-answering systems. In this exercise, you will practice using POS tagging for word-sense disambiguation. \n",
    "\n",
    "There are two sentences containing the word **jam**, with two different senses and you are tasked to identify the POS tags to help you determine the corresponding sense of the word in a given sentence. \n",
    "\n",
    "The two sentences are available in the `texts` list. The `en_core_web_sm` model is already loaded and available for your use as `nlp`.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Create a `documents` list containing the `Doc` containers of each element in the `texts` list.\n",
    "-   Print a tuple of the token's text and POS tags per each `Doc` container only if the word **jam** is in the token text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"This device is used to jam the signal.\",\n",
    "         \"I am stuck in a traffic jam\"]\n",
    "\n",
    "# Create a list of Doc containers in the texts list\n",
    "documents = [nlp(t) for t in texts]\n",
    "\n",
    "# Print a token's text and POS tag if the word jam is in the token's text\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Sentence {i+1}: \", [(token.text, token.pos_) for token in doc if \"jam\" in token.text], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "Question\n",
    "--------\n",
    "\n",
    "The word **jam** has multiple senses. In the sentence **\"This device is used to jam the signal.\"**, what is the sense of the word **jam**?\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "VERB: become or make unable to move or work due to a part seizing or becoming stuck.\n",
    "\n",
    "[/] NOUN: an instance of a machine or thing seizing or becoming stuck.\n",
    "\n",
    "NOUN: an awkward situation or predicament."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency parsing with spaCy\n",
    "=============================\n",
    "\n",
    "Dependency parsing analyzes the grammatical structure in a sentence and finds out related words as well as the type of relationship between them. An application of dependency parsing is to identify a sentence object and subject. In this exercise, you will practice extracting dependency labels for given texts. \n",
    "\n",
    "Three comments from the Airline Travel Information System (ATIS) dataset have been provided for you in a list called `texts`. `en_core_web_sm` model is already loaded and available for your use as `nlp`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create a `documents` list containing the `doc` containers of each element in the `texts` list.\n",
    "-   Print a tuple of (the token's text, dependency label, and label's explanation) per each `doc` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of Doc containers of texts list\n",
    "documents = [nlp(t) for t in texts]\n",
    "\n",
    "# Print each token's text, dependency label and its explanation\n",
    "for doc in documents:\n",
    "    print([(token.text, token.dep_, spacy.explain(token.dep_)) for token in doc], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Introduction to word vectors\n",
    "--------------------------------\n",
    "\n",
    "00:00 - 00:04\n",
    "\n",
    "Welcome! Let's learn about word vectors.\n",
    "\n",
    "2\\. Word vectors (embeddings)\n",
    "-----------------------------\n",
    "\n",
    "00:04 - 01:41\n",
    "\n",
    "Word vectors, or word embeddings, are numerical representations of words that allow computers to perform complex tasks using text data. The purpose of word vectors is to allow a computer to understand words. Computers cannot understand text as is, but they can process numbers efficiently. For this reason, we'll convert words into numbers. Traditional methods, such as the \"bag-of-words\" method, take all words in a corpus and convert them into a unique number, when creating word vectors. These words are then stored in a dictionary where \"I\" can be mapped to one, \"got\" can be mapped to two and so on. The older methods allow a computer to understand words numerically, however, they do not enable understanding the meaning of the words. Consider an example with two sentences: \"I got covid\" and \"I got coronavirus\". These sentences represented as numerical arrays of [1, 2, 3] and [1, 2, 4] respectively with a bag of words model. The two sentences are identical, but they have different word embeddings. The computer does not have a certain way of knowing that the words \"covid\" and \"coronavirus\" refer to the same thing. The model just sees these as two different words represented by two different numbers. Hence, the model is oblivious to context and semantics.\n",
    "\n",
    "3\\. Word vectors\n",
    "----------------\n",
    "\n",
    "01:41 - 02:33\n",
    "\n",
    "But all hope is not lost. We can use recent methodologies to find word vectors that can be used to teach a computer if two words have similar meanings. Word vectors have a pre-defined number of dimensions. Statistical and machine learning models take into account word frequencies in a corpus and the presence of other words in similar contexts. A computer can then use this information to understand the similarity of words numerically by using vectors. For example, the table shows 7-dimensional word vectors that can help distinguish animals from houses or cats from dogs by capturing different aspects of these words from their surrounding context in a large corpus of text.\n",
    "\n",
    "4\\. Word vectors\n",
    "----------------\n",
    "\n",
    "02:33 - 03:12\n",
    "\n",
    "There are multiple approaches to produce word vectors. Some of the most well-known algorithms are word2vec, Glove, fastText, and transformer-based models. To process and train n-dimensional word vectors, Word2vec and fastText use neural network architectures, while Glove uses the word co-occurrences matrix and transformer-based models use more complex architectures to train and predict word vectors. spaCy uses some of these methodologies to enable access to word vectors.\n",
    "\n",
    "5\\. spaCy vocabulary\n",
    "--------------------\n",
    "\n",
    "03:12 - 03:49\n",
    "\n",
    "Word vectors are a part of many spaCy models, however, a few of the models do not have word vectors. For instance, en_core_web_sm model, the small spaCy model, does not have any word vectors, while the medium-sized model, en_core_web_md, has 20,000-word vectors. We can learn about the size of vocabulary and word vector dimensions by checking the value of the nlp-dot-meta for the keyword \"vectors\".\n",
    "\n",
    "6\\. Word vectors in spaCy\n",
    "-------------------------\n",
    "\n",
    "03:49 - 04:51\n",
    "\n",
    "When using spaCy, we can only extract vectors of words that exist in a model's vocabulary. We use the nlp-dot-vocab method of a spaCy model to access a vocabulary object. Then the nlp-dot-vocab-dot-strings attribute of a Vocab object can be used to access word IDs in the vocabulary. Later, the vocab-dot-vectors can be used to access word vectors of a word using its ID. For example, given a word \"like\", we first access the mapping of the word to its ID in the vocabulary using nlp-dot-vocab-strings[\"like\"], then use this ID to access the corresponding word vector using nlp-dot-vocab-dot-vectors[extracted_word-id].\n",
    "\n",
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:51 - 04:55\n",
    "\n",
    "Great, let's exercise our learnings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy vocabulary\n",
    "================\n",
    "\n",
    "Word vectors, or word embeddings, are numerical representations of words that allow computers to perform complex tasks using text data. Word vectors are a part of many spaCy models, however, a few of the models do not have word vectors. \n",
    "\n",
    "In this exercise, you will practice accessing `spaCy` vocabulary information. Some meta information about word vectors are stored in each `spaCy` model. You can access this information to learn more about the vocabulary size, word vectors dimensions, etc.\n",
    "\n",
    "The `spaCy` package is already imported for your use. In a `spaCy` model's metadata, the number of words is stored as an element with the \"**vectors**\" key and the dimension of word vectors is stored as an element with the \"**width**\" key.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Load the `en_core_web_md` model.\n",
    "-   Print the number of words in the `en_core_web_md` model's vocabulary.\n",
    "-   Print the dimensions of word vectors in the `en_core_web_md` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_md model\n",
    "md_nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Print the number of words in the model's vocabulary\n",
    "print(\"Number of words: \", md_nlp.meta[\"vectors\"][\"vectors\"], \"\\n\")\n",
    "\n",
    "# Print the dimensions of word vectors in en_core_web_md model\n",
    "print(\"Dimension of word vectors: \", md_nlp.meta[\"vectors\"][\"width\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors in spaCy vocabulary\n",
    "================================\n",
    "\n",
    "The purpose of word vectors is to allow a computer to understand words. In this exercise, you will practice extracting word vectors for a given list of words. \n",
    "\n",
    "A list of words is compiled as `words`. The `en_core_web_md` model is already imported and available as `nlp`. \n",
    "\n",
    "The vocabulary of `en_core_web_md` model contains 20,000 words. If a word does not exist in the vocabulary, you will not be able to extract its corresponding word vector. In this exercise, for simplicity, it is ensured that all the given words exist in this model's vocabulary.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Extract the IDs of all the given `words` and store them in an `ids` list.\n",
    "-   For each ID from `ids`, store the first ten elements of the word vector in the `word_vectors` list.\n",
    "-   Print the first ten elements of the first word vector from `word_vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"like\", \"love\"]\n",
    "\n",
    "# IDs of all the given words\n",
    "ids = [nlp.vocab.strings[w] for w in words]\n",
    "\n",
    "# Store the first ten elements of the word vectors for each word\n",
    "word_vectors = [nlp.vocab.vectors[i][:10] for i in ids]\n",
    "\n",
    "# Print the first ten elements of the first word vector\n",
    "print(word_vectors[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
