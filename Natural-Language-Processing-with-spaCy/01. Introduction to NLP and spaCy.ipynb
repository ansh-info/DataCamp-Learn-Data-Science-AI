{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Natural Language Processing (NLP) basics\n",
    "--------------------------------------------\n",
    "\n",
    "00:00 - 00:11\n",
    "\n",
    "Welcome to this course! I'm Azadeh, a principal data scientist. In this course, we'll explore Natural Language Processing (NLP) using spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Natural Language Processing (NLP)\n",
    "-------------------------------------\n",
    "\n",
    "00:11 - 00:47\n",
    "\n",
    "NLP is a subfield of artificial intelligence that combines computer science and linguistics to help computers understand, analyze, and generate human language. NLP helps extract insights from unstructured data. Unstructured data, such as textual data, is information that is not organized in a pre-defined manner. NLP incorporates statistics, machine learning, and deep learning models to understand human language, intent, and sentiment.\n",
    "\n",
    "- A subfield of Artificial Intelligence (AI)\n",
    "- Helps computers to understand human language  \n",
    "- Helps extract insights from unstructured data\n",
    "- Incorporates statistics, machine learning models and deep learning models\n",
    "\n",
    "Artificial Intelligence contains:\n",
    "- Machine Learning\n",
    "- Natural Language Processing\n",
    "- Deep Learning (intersection between Machine Learning and NLP)\n",
    "\n",
    "[Note: The original shows a Venn diagram with Artificial Intelligence as the outer circle, Machine Learning and Natural Language Processing as overlapping circles within it, and Deep Learning in the intersection of ML and NLP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. NLP use cases\n",
    "-----------------\n",
    "\n",
    "00:47 - 01:26\n",
    "\n",
    "NLP has many applications. We will introduce three well-known use cases: sentiment analysis, named-entity recognition, and chatbots. Sentiment analysis is the use of computers to interpret the underlying subjective tone of a piece of text, and categorize it into positive, neutral, or negative classes. For example, a review about great service and affordable price is classified with a positive sentiment, while a review of a horrible experience is categorized with a negative sentiment.\n",
    "\n",
    "# Sentiment analysis\n",
    "\n",
    "- Use of computers to determine the underlying subjective tone of a piece of writing\n",
    "\n",
    "| Sentiment | Example Text |\n",
    "|-----------|--------------|\n",
    "| Positive | \"Great service and affordable price. I will buy it again.\" |\n",
    "| Negative | \"This was a horrible experience. Not worth the money\" |\n",
    "\n",
    "[Note: The original image shows emoticons/smileys - a happy face for Positive and sad face for Negative sentiment, in green and orange boxes respectively]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. NLP use cases\n",
    "-----------------\n",
    "\n",
    "01:26 - 02:06\n",
    "\n",
    "The next NLP use case is named entity recognition (NER). NER is used in information extraction to locate and classify named entities in unstructured text into predefined categories. Entities are objects such as a person or location. For example, with the phrase \"John McCarthy was born on September 4, 1927.\" NER would classify John McCarthy as the name, highlighted in blue here, and September 4, 1927 as the date, highlighted in red.\n",
    "\n",
    "# Named entity recognition (NER)\n",
    "\n",
    "- Locating and classifying named entities mentioned in unstructured text into pre-defined categories\n",
    "- Named entities are real-world objects such as a person or location\n",
    "\n",
    "Example:\n",
    "`[John McCarthy][Name] was born on [September 4, 1927][Date]`\n",
    "\n",
    "[Note: The original image shows text with colored boxes highlighting and labeling the Name and Date entities]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. NLP use cases\n",
    "-----------------\n",
    "\n",
    "02:06 - 02:21\n",
    "\n",
    "Another NLP use case is text generation in chatbots. ChatGPT is an example, which is based on a transformer-based language model trained on a vast amount of unstructured text data.\n",
    "\n",
    "- Generate human-like responses to text input, such as ChatGPT\n",
    "\n",
    "[Note: The original image shows an illustration of a computer screen with a robot/AI assistant icon and chat bubble, displayed in blue and white colors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Introduction to spaCy\n",
    "-------------------------\n",
    "\n",
    "02:21 - 02:56\n",
    "\n",
    "Now that we have learned about NLP, let's learn more about spaCy and how we can utilize it in our NLP projects. spaCy is a free and open-source library for NLP in Python, which is designed to simplify building systems for information extraction. spaCy provides production-ready code widely used for NLP use cases. It supports 64+ languages. It is robust, fast and has built-in visualizers for various NLP functionalities.\n",
    "\n",
    "## spaCy is a free, open-source library for NLP in Python which:\n",
    "\n",
    "- Is designed to build systems for information extraction\n",
    "- Provides production-ready code for NLP use cases\n",
    "- Supports 64+ languages\n",
    "- Is robust and fast and has visualization libraries\n",
    "\n",
    "[Note: The original image includes the spaCy logo in white against a blue patterned background]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Install and import spaCy\n",
    "----------------------------\n",
    "\n",
    "02:56 - 03:40\n",
    "\n",
    "As the first step, we install spaCy using pip, a Python package manager. We can then download any spaCy model using a specific Python command, -m spacy, with a given model name. Here we choose \"en_core_web_sm\", the smallest English model. After downloading the model, we import spacy and create a nlp object by passing the model name in quotation marks to the spacy-dot-load function. spaCy has multiple trained models for the English language that are available for download from spacy-dot-io website.\n",
    "\n",
    "- As the first step, `spaCy` can be installed using the Python package manager pip\n",
    "- `spaCy` trained models can be downloaded \n",
    "- Multiple trained models are available for English language at spacy.io\n",
    "\n",
    "```bash\n",
    "$ python3 pip install spacy\n",
    "\n",
    "python3 -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Read and process text with spaCy\n",
    "------------------------------------\n",
    "\n",
    "03:40 - 04:14\n",
    "\n",
    "Now that our NLP object is ready, we can move on to reading and processing text. The loaded spaCy model (nlp object) can process text and convert it into a Doc object, which is a container to store the processed text. The Doc object contains information like tokens, linguistic annotations, and relationships about the text. We'll learn about each of these later in the chapter.\n",
    "\n",
    "- Loaded `spaCy` model `en_core_web_sm` = `nlp` object\n",
    "- `nlp` object converts text into a `Doc` object (container) to store processed text\n",
    "\n",
    "```\n",
    "nlp(Text)     -->     Doc object     -->     Components:\n",
    "object                                       - Tokens\n",
    "                                            - Linguistic annotations\n",
    "                                            - Relationships\n",
    "```\n",
    "\n",
    "[Note: The original image shows this as a flow diagram with boxes and arrows. The Doc object is represented as a blue cylinder/database icon.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. spaCy in action\n",
    "-------------------\n",
    "\n",
    "04:14 - 05:04\n",
    "\n",
    "Let's look at an example of processing text with spaCy. This example will use a preprocessing step known as tokenization. The first step is to read text, in this case the string \"A spaCy pipeline object is created.\". We convert this text into a Doc object by running a loaded spaCy model, nlp, on the text. Now, we can utilize list comprehension to print all tokens of the input text by using token-dot-text for token in doc. A token is the smallest meaningful part of a text. The process of dividing a text into a list of meaningful tokens is called tokenization.\n",
    "\n",
    "- Processing a string using `spaCy`\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"A spaCy pipeline object is created.\"\n",
    "doc = nlp(text)\n",
    "```\n",
    "\n",
    "- Tokenization\n",
    "  - A `Token` is defined as the smallest meaningful part of the text.\n",
    "  - Tokenization: The process of dividing a text into a list of meaningful tokens\n",
    "\n",
    "```python\n",
    "print([token.text for token in doc])\n",
    "```\n",
    "\n",
    "```\n",
    "['A', 'spaCy', 'pipeline', 'object', 'is', 'created', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "05:04 - 05:09\n",
    "\n",
    "Let's practice our learnings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc container in spaCy\n",
    "======================\n",
    "\n",
    "The first step of a spaCy text processing pipeline is to convert a given text string into a `Doc` container, which stores the processed text. In this exercise, you'll practice loading a `spaCy`model, creating an `nlp()` object, creating a `Doc` container and processing a `text` string that is available for you.\n",
    "\n",
    "`en_core_web_sm` model is already downloaded.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Load `en_core_web_sm` and create an `nlp`object.\n",
    "-   Create a `doc` container of the `text` string.\n",
    "-   Create a list containing the text of each tokens in the `doc` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load en_core_web_sm and create an nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a Doc container for the text object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Create a list containing the text of each token in the Doc container\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER use case\n",
    "============\n",
    "\n",
    "NLP has many applications across different industries such as sentiment analysis, named entity recognition and chatbots. \n",
    "\n",
    "Is the following a correct definition for named entity recognition? \n",
    "\n",
    "*\"Given a string of text, named entity recognition is identifying and categorizing entities in text.\"*\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "50XP\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   False\n",
    "\n",
    "-   True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization with spaCy\n",
    "=======================\n",
    "\n",
    "In this exercise, you'll practice tokenizing text. You'll use the first review from the Amazon Fine Food Reviews dataset for this exercise. You can access this review by using the `text` object provided. \n",
    "\n",
    "The `en_core_web_sm` model is already loaded for you. You can access it by calling `nlp()`. You can use list comprehension to compile output lists.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Store Doc container for the pre-loaded review in a `document` object.\n",
    "-   Store and review texts of all the tokens of the `document` in the variable `first_text_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Doc container of the given text\n",
    "document = nlp(text)\n",
    "\n",
    "# Store and review the token text values of tokens for the Doc container\n",
    "first_text_tokens = [token.text for token in document]\n",
    "print(\"First text tokens:\\n\", first_text_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. spaCy basics\n",
    "----------------\n",
    "\n",
    "00:00 - 00:05\n",
    "\n",
    "Let's learn more about spaCy and some of its core functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. spaCy NLP pipeline\n",
    "----------------------\n",
    "\n",
    "00:05 - 00:36\n",
    "\n",
    "We previously learned that a spaCy NLP pipeline is created when we load a spaCy model. We started by importing spaCy, then call spacy-dot-load() to return a nlp object, a spaCy Language class. The Language class is the text processing pipeline and applies all necessary preprocessing steps to our input text behind the scenes. After that, we can apply nlp() on any given text to return a Doc container.\n",
    "\n",
    "Here's the markdown conversion of the image content:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Here's my spaCy pipeline.\")\n",
    "```\n",
    "\n",
    "- Import `spaCy`\n",
    "- Use `spacy.load()` to return `nlp`, a Language class\n",
    "  - The `Language` object is the text processing pipeline\n",
    "- Apply `nlp()` on any text to get a `Doc` container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. spaCy NLP pipeline\n",
    "----------------------\n",
    "\n",
    "00:36 - 01:06\n",
    "\n",
    "Let's learn more about the spaCy NLP pipeline. Every NLP application consists of several steps of text processing. spaCy applies a series of preprocessing steps to the text when we call nlp(), the spaCy Language class. Some of the processing steps are tokenization, tagging, parsing, Named Entity Recognition and many others which result in a Doc container.\n",
    "\n",
    "spaCy applies some processing steps using its Language class:\n",
    "\n",
    "Text -> [nlp] -> Doc\n",
    "     \n",
    "where [nlp] contains:\n",
    "Tokenizer -> Tagger -> Parser -> NER -> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Container objects in spaCy\n",
    "------------------------------\n",
    "\n",
    "01:06 - 01:42\n",
    "\n",
    "Doc object is only one of the container classes that spaCy supports. spaCy uses multiple data structures to represent text data. Container classes such as Doc hold information about sentences, words and the text. Another container class is the Span object, which represents a slice from a Doc object; and spaCy also has a Token class, which represents an individual token, like a word, punctuation symbol, etc.\n",
    "\n",
    "There are multiple data structures to represent text data in spaCy:\n",
    "\n",
    "| Name | Description |\n",
    "|------|-------------|\n",
    "| Doc | A container for accessing linguistic annotations of text |\n",
    "| Span | A slice from a `Doc` object |\n",
    "| Token | An individual token, i.e. a word, punctuation, whitespace, etc. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Pipeline components\n",
    "-----------------------\n",
    "\n",
    "01:42 - 02:21\n",
    "\n",
    "All the container classes are generated during the spaCy NLP processing steps. Each of the processing steps we saw in the spaCy pipeline has a well-defined task. In this course, we mostly focus on tokenizer, tagger, lemmatizer, and ner components. As shown, the tokenizer creates Doc object and segment text into tokens. Then the tagger and other components add more attributes such as part-of-speech tags, and label named entities.\n",
    "\n",
    "The spaCy language processing pipeline always depends on the loaded model and its capabilities.\n",
    "\n",
    "| Component | Name | Description |\n",
    "|-----------|------|-------------|\n",
    "| Tokenizer | Tokenizer | Segment text into tokens and create `Doc` object |\n",
    "| Tagger | Tagger | Assign part-of-speech tags |\n",
    "| Lemmatizer | Lemmatizer | Reduce the words to their root forms |\n",
    "| EntityRecognizer | NER | Detect and label named entities |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Pipeline components\n",
    "-----------------------\n",
    "\n",
    "02:21 - 02:49\n",
    "\n",
    "There are many more text processing components available in spaCy and it is important to highlight some of the other important text processing components of an nlp instance and their duties, such as Language, DependencyParser, and Sentencizer. Each component has unique features to help us process our text better. We will see more examples of each component throughout the course.\n",
    "\n",
    "Each component has unique features to process text\n",
    "- Language\n",
    "- DependencyParser \n",
    "- Sentencizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Tokenization\n",
    "----------------\n",
    "\n",
    "02:49 - 03:41\n",
    "\n",
    "We introduced tokenization earlier, but let's explore it further. Tokenization is always the first processing step in a spaCy NLP pipeline as all other processing steps require tokens in a given text. Recall that tokenization splits a sentence into its tokens, or the smallest meaningful piece of text. Tokens can be words, numbers and punctuation. The code segment shows the tokenization process we've seen before using a small English spaCy model. Once we apply the nlp object to the input sentence and create a Doc object, we can access each Token by using list comprehension and print a token's text by using -dot-text attribute.\n",
    "\n",
    "- Always the first operation\n",
    "- All the other operations require tokens\n",
    "- Tokens can be words, numbers and punctuation\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Tokenization splits a sentence into its tokens.\")\n",
    "print([token.text for token in doc])\n",
    "```\n",
    "\n",
    "```\n",
    "['Tokenization', 'splits', 'a', 'sentence', 'into', 'its', 'tokens', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Sentence segmentation\n",
    "-------------------------\n",
    "\n",
    "03:41 - 04:23\n",
    "\n",
    "Sentence segmentation or breaking a text into its given sentences, is a more complex task compared to tokenization due to difficulties of handling punctuation and abbreviations. Sentence segmentation happens as part of the DependencyParser pipeline component. We utilize a for loop to iterate over the sentences of \"We are learning NLP. This course introduces spaCy.\" using the dot-sents property of a Doc container. Then, we can use the dot-text attribute to access the sentence text.\n",
    "\n",
    "- More complex than tokenization\n",
    "- Is a part of `DependencyParser` component\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"We are learning NLP. This course introduces spaCy.\"\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "```\n",
    "\n",
    "```\n",
    "We are learning NLP.\n",
    "This course introduces spaCy.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Lemmatization\n",
    "-----------------\n",
    "\n",
    "04:23 - 04:58\n",
    "\n",
    "Lemmatization, one of the spaCy processing steps, reduces the word forms to their lemmas. A lemma is the base form of a token in which the token appears in a dictionary. For instance, the lemma of the words \"eats\" and \"ate\" is \"eat\". Lemmatization improves the accuracy of many language modeling tasks. We iterate over tokens to get their text and lemmas using token-dot-text and token-dot-lemma_.\n",
    "\n",
    "- A lemma is the base form of a token\n",
    "- The lemma of eats and ate is eat\n",
    "- Improves accuracy of language models\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"We are seeing her after one year.\")\n",
    "print([(token.text, token.lemma_) for token in doc])\n",
    "```\n",
    "\n",
    "```\n",
    "[('We', 'we'), ('are', 'be'), ('seeing', 'see'), ('her', 'she'), \n",
    "('after', 'after'), ('one', 'one'), ('year', 'year'), ('.', '.')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:58 - 05:00\n",
    "\n",
    "Let's exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a spaCy pipeline\n",
    "========================\n",
    "\n",
    "You've already run a spaCy NLP pipeline on a single piece of text and also extracted tokens of a given list of Doc containers. In this exercise, you'll practice the initial steps of running a `spaCy` pipeline on `texts`, which is a list of text strings. \n",
    "\n",
    "You will use the `en_core_web_sm` model for this purpose. The `spaCy` package has already been imported for you.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Load the `en_core_web_sm` model as `nlp`.\n",
    "-   Run an `nlp()` model on each item of `texts`, and append each corresponding `Doc`container to a `documents` list.\n",
    "-   Print the token texts for each `Doc` container of the `documents` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load en_core_web_sm model as nlp\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Run an nlp model on each item of texts and append the Doc container to documents\n",
    "documents = []\n",
    "for text in texts:\n",
    "  documents.append(nlp(text))\n",
    "\n",
    "# Print the token texts for each Doc container\n",
    "for doc in documents:\n",
    "  print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization with spaCy\n",
    "========================\n",
    "\n",
    "In this exercise, you will practice lemmatization. Lemmatization can be helpful to generate the root form of derived words. This means that given any sentence, we expect the number of lemmas to be less than or equal to the number of tokens.\n",
    "\n",
    "The first Amazon food review is provided for you in a string called `text`. `en_core_web_sm` is loaded as `nlp`, and has been run on the `text`to compile `document`, a `Doc` container for the text string.\n",
    "\n",
    "`tokens`, a list containing tokens for the `text`is also already loaded for your use.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Append the lemma for all tokens in the `document`, then print the list of `lemmas`.\n",
    "-   Print `tokens` list and observe the differences between `tokens` and `lemmas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp(text)\n",
    "tokens = [token.text for token in document]\n",
    "\n",
    "# Append the lemma for all tokens in the document\n",
    "lemmas = [token.lemma_ for token in document]\n",
    "print(\"Lemmas:\\n\", lemmas, \"\\n\")\n",
    "\n",
    "# Print tokens and compare with lemmas list\n",
    "print(\"Tokens:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation with spaCy\n",
    "================================\n",
    "\n",
    "In this exercise, you will practice sentence segmentation. In NLP, segmenting a document into its sentences is a useful basic operation. It is one of the first steps in many NLP tasks that are more elaborate, such as detecting named entities. Additionally, capturing the number of sentences may provide some insight into the amount of information provided by the text.\n",
    "\n",
    "You can access ten food reviews in the list called `texts`. \n",
    "\n",
    "The `en_core_web_sm` model has already been loaded for you as `nlp` and .\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Run the `spaCy` model on each item in the `texts` list to compile `documents`, a list of all `Doc` containers.\n",
    "-   Extract sentences of each `doc` container by iterating through `documents` list and append them to a list called `sentences`.\n",
    "-   Count the number of sentences in each `doc`container using the `sentences` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a documents list of all Doc containers\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Iterate through documents and append sentences in each doc to the sentences list\n",
    "sentences = []\n",
    "for doc in documents:\n",
    "  sentences.append([s for s in doc.sents])\n",
    "\n",
    "# Find number of sentences per each doc container\n",
    "print([len(s) for s in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Linguistic features in spaCy\n",
    "--------------------------------\n",
    "\n",
    "00:00 - 00:11\n",
    "\n",
    "Welcome back! Let's learn about some of spaCy's linguistic features such as the part-of-speech-tagger and named entity recognizer to extract information from text.\n",
    "\n",
    "2\\. POS tagging\n",
    "---------------\n",
    "\n",
    "00:11 - 00:36\n",
    "\n",
    "POS stands for part-of-speech. A part of speech is a grammatical term that categorizes words based on their function and context within a sentence. For example, the English language has nine main POS categories, some of them are: verb, noun, adjective, adverb and conjunction.\n",
    "\n",
    "3\\. POS tagging with spaCy\n",
    "--------------------------\n",
    "\n",
    "00:36 - 01:02\n",
    "\n",
    "One use case for POS tagging is to confirm the meaning of a word. For example, some words such as \"watch\" can be both noun and verb. spaCy captures POS tags in the pos_ feature of the nlp pipeline. spacy-dot-explain() can be used on a given tag to include explanations of the tags.\n",
    "\n",
    "4\\. POS tagging with spaCy\n",
    "--------------------------\n",
    "\n",
    "01:02 - 01:36\n",
    "\n",
    "Let's look at an example for extracting part-of-speech tags for two sentences: \"I watch TV\" and \"I left without my watch\". We use list comprehension to identify the token and the POS tag using token-dot-pos-underscore, and the explanation using spacy-dot-explain and passing it token-dot-pos-underscore. The word \"watch\" is correctly tagged as a verb in the first sentence, and tagged as a noun in the second example.\n",
    "\n",
    "5\\. Named entity recognition\n",
    "----------------------------\n",
    "\n",
    "01:36 - 02:21\n",
    "\n",
    "On to named entity recognition! A named entity is a word or phrase that refers to a specific entity with a name, such as a organization. Named-entity recognition (NER) is a NLP task that classifies named entities found in an unstructured text into pre-defined categories such as person names. spaCy supports a wide range of entity types such as: PERSON to represent a named person, ORG to represent a company, GPE for a geo-political entity like a country, LOC for other locations such as mountain ranges, DATE and TIME.\n",
    "\n",
    "6\\. NER and spaCy\n",
    "-----------------\n",
    "\n",
    "02:21 - 02:51\n",
    "\n",
    "spaCy models can predict named entities and their corresponding labels as part of the NER component. Named entities are available via the doc-dot-ents property of a Doc container. spaCy will also tag each entity with its corresponding label, which represents an entity type. The label of an entity is available via the -dot-label_ property.\n",
    "\n",
    "7\\. NER and spaCy\n",
    "-----------------\n",
    "\n",
    "02:51 - 03:34\n",
    "\n",
    "The code snippet illustrates how we extract named entities from \"Albert Einstein was genius\". We can iterate through entities by using doc-dot-ents attribute, and access entity text, the start and end characters of each entity, and entity labels by using -dot-text, -dot-start_char, -dot-end_char and -dot-label_ respectively. In this instance, Albert Einstein is detected as a PERSON which starts from the first and ends at the 15th character of the given text.\n",
    "\n",
    "8\\. NER and spaCy\n",
    "-----------------\n",
    "\n",
    "03:34 - 04:11\n",
    "\n",
    "An alternative approach to extract entities and their types is to directly use Token class instead of accessing doc-dot-ents to only check extracted named entities. spaCy tags each token in a given Doc container with its entity type if it is categorized as an entity. We can access a Token's -dot-text and -dot-entity_type. If a token is not classified as an entity such as the words was and genius, we will see an empty string as the entity type.\n",
    "\n",
    "9\\. displaCy\n",
    "------------\n",
    "\n",
    "04:11 - 04:50\n",
    "\n",
    "We can also visualize these entities using displaCy. displaCy has different visualization options, such as the entity visualizer, which highlights named entities and their labels in a text. For example we can use displacy-dot-serve function to visualize named entities of a previous example, \"Albert Einstein was genius\". The displacy-dot-serve function takes two arguments, a Doc container, and the type of displaCy visualization which is \"ent\" (entities) in this instance.\n",
    "\n",
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:50 - 04:54\n",
    "\n",
    "Let's exercise our learnings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging with spaCy\n",
    "======================\n",
    "\n",
    "In this exercise, you will practice POS tagging. POS tagging is a useful tool in NLP as it allows algorithms to understand the grammatical structure of a sentence and to confirm words that have multiple meanings such as `watch`and `play`.\n",
    "\n",
    "For this exercise, `en_core_web_sm` has been loaded for you as `nlp`. Three comments from the Airline Travel Information System (ATIS) dataset have been provided for you in a list called `texts`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Compile `documents`, a list of all `doc`containers for each text in `texts` list using list comprehension.\n",
    "-   For each `doc` container, print each token's text and its corresponding POS tag by iterating through `documents` and tokens of each `doc` container using a nested for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a list of all Doc containers of texts\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Print token texts and POS tags for each Doc container\n",
    "for doc in documents:\n",
    "    for token in doc:\n",
    "        print(\"Text: \", token.text, \"| POS tag: \", token.pos_)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER with spaCy\n",
    "==============\n",
    "\n",
    "Named entity recognition (NER) helps you to easily identify key elements of a given document, like names of people and places. It helps sort unstructured data and detect important information, which is crucial if you are dealing with large datasets. In this exercise, you will practice Named Entity Recognition.\n",
    "\n",
    "`en_core_web_sm` has been loaded for you as `nlp`. Three comments from the Airline Travel Information System (ATIS) dataset have been provided for you in a list called `texts`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Compile `documents`, a list of all `Doc`containers for each text in the `texts` using list comprehension.\n",
    "-   For each `doc` container, print each entity's text and corresponding label by iterating through `doc.ents`.\n",
    "-   Print the sixth token's text, and the entity type of the second `Doc` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a list of all Doc containers of texts\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Print the entity text and label for the entities in each document\n",
    "for doc in documents:\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "# Print the 6th token's text and entity type of the second document\n",
    "print(\"\\nText:\", documents[1][5].text, \"| Entity type: \", documents[1][5].ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text processing with spaCy\n",
    "==========================\n",
    "\n",
    "Every NLP application consists of several text processing steps. You have already learned some of these steps, including tokenization, lemmatization, sentence segmentation and named entity recognition.\n",
    "\n",
    "In this exercise, you'll continue to practice with text processing steps in spaCy, such as breaking the text into sentences and extracting named entities. You will use the first five reviews from the Amazon Fine Food Reviews dataset for this exercise. You can access these reviews by using the `texts` object. \n",
    "\n",
    "The `en_core_web_sm` model has already been loaded for you to use, and you can access it by using `nlp`. The list of `Doc` containers for each item in `texts` is also pre-loaded and accessible at `documents`.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "50 XP\n",
    "\n",
    "-   Create `sentences`, a list of list of all sentences in each `doc` container in `documents` using list comprehension.\n",
    "-   Print `num_sentences`, a list containing the number of sentences for each `doc` container by using the `len()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store sentences of each Doc container in documents\n",
    "sentences = [[sent for sent in doc.sents] for doc in documents]\n",
    "\n",
    "# Print number of sentences in each Doc container in documents\n",
    "num_sentences = [len(s) for s in sentences]\n",
    "print(\"Number of sentences in documents:\\n\", num_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Create a list of tuples of format (entity text, entity label) for the third `doc` container in `third_text_entities`.\n",
    "-   Create a list of tuples of format (token text, POS tag) of first ten tokens of third `doc`container at `third_text_10_pos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store sentences of each Doc container in documents\n",
    "sentences = [[sent for sent in doc.sents] for doc in documents]\n",
    "\n",
    "# Create a list to track number of sentences per Doc container in documents\n",
    "num_sentences = [len([sent for sent in doc.sents]) for doc in documents]\n",
    "print(\"Number of sentences in documents:\\n\", num_sentences, \"\\n\")\n",
    "\n",
    "# Record entities text and corresponding label of the third Doc container\n",
    "third_text_entities = [(ent.text, ent.label_) for ent in documents[2].ents]\n",
    "print(\"Third text entities:\\n\", third_text_entities, \"\\n\")\n",
    "\n",
    "# Record first ten tokens and corresponding POS tag for the third Doc container\n",
    "third_text_10_pos = [(token.text, token.pos_) for token in documents[2]][:10]\n",
    "print(\"First ten tokens of third text:\\n\", third_text_10_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
