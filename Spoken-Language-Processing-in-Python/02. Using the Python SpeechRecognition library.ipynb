{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. SpeechRecognition Python library\n",
    "------------------------------------\n",
    "\n",
    "00:00 - 00:15\n",
    "\n",
    "To get started with spoken language recognition, let's check out the SpeechRecognition Python Library. We'll start with why the SpeechRecognition Library. And then we'll get into seeing how we can use Google's web speech API to transcribe speech to text.\n",
    "\n",
    "2\\. Why the SpeechRecognition library?\n",
    "--------------------------------------\n",
    "\n",
    "00:15 - 00:55\n",
    "\n",
    "Automatic speech recognition is a tough challenge. And there's no shortage of companies and research institutions working on libraries to help solve it. There's the Sphinx library by Carnegie Mellon University, Kaldi, SpeechRecognition, and more. Some have more robust features than others but they all have the same goal of transcribing audio files to text. We're going to be focused on the SpeechRecognition library because of its low barrier to entry and its compatibility with many available speech recognition APIs we'll see shortly.\n",
    "\n",
    "#### Some existing python libraries\n",
    "\n",
    "• CMU Sphinx\n",
    "\n",
    "• Kaldi\n",
    "\n",
    "• SpeechRecognition \n",
    "\n",
    "• Wav2letter++ by Facebook\n",
    "\n",
    "3\\. Getting started with SpeechRecognition\n",
    "------------------------------------------\n",
    "\n",
    "00:55 - 01:15\n",
    "\n",
    "We can get started with the SpeechRecognition library by installing it from PyPi using pip and running the pip install SpeechRecognition command in a terminal or shell. It's compatible with Python 2 and 3 but we'll be using Python 3.\n",
    "\n",
    "#### Install from PyPi:\n",
    "```\n",
    "python$ pip install SpeechRecognition\n",
    "```\n",
    "• Compatible with Python 2 and 3\n",
    "• We'll use Python 3\n",
    "\n",
    "\n",
    "4\\. Using the Recognizer class\n",
    "------------------------------\n",
    "\n",
    "01:15 - 02:23\n",
    "\n",
    "Now we have SpeechRecognition installed, let's check out where all the magic happens, the recognizer class. So how do we use it? To access the Recognizer class, we'll first import the SpeechRecognition module as the abbreviation sr. Then we'll create an instance of the recognizer class by calling it from sr and assigning to a variable, recognizer. Finally, we'll set the recognizers energy threshold to 300. The energy threshold can be thought of as the loudness of audio which is considered speech. Values below the threshold are considered silence, values above are considered speech. A silent room is typically between 0 and 100. SpeechRecognition's documentation recommends 300 as a starting value which covers most speech files. The energy threshold value will adjust automatically as the recognizer listens to an audio file.\n",
    "\n",
    "```python\n",
    "# Import the SpeechRecognition library\n",
    "import speech_recognition as sr\n",
    "# Create an instance of Recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "# Set the energy threshold\n",
    "recognizer.energy_threshold = 300\n",
    "```\n",
    "\n",
    "5\\. Using the Recognizer class to recognize speech\n",
    "--------------------------------------------------\n",
    "\n",
    "02:23 - 03:14\n",
    "\n",
    "Now we've got a recognizer instance ready, it's time to recognize some speech. We chose SpeechRecognition for its flexibility. Here's what I mean. SpeechRecognition has functions built-in to work with many of the best speech recognition APIs. Recognize bing accesses Microsoft's cognitive services, recognize Google uses Google's free web speech API, recognize Google Cloud accesses Google's cloud speed API. And recognize wit uses the wit dot ai platform. They all accept an audio file and return text, which is hopefully the transcribed speech from the audio file. Remember, speech recognition is still far from perfect.\n",
    "\n",
    "• `Recognizer` class has built-in functions which interact with speech APIs\n",
    "  - `recognize_bing()`\n",
    "  - `recognize_google()`\n",
    "  - `recognize_google_cloud()`\n",
    "  - `recognize_wit()`\n",
    "\n",
    "Input: `audio_file`\n",
    "\n",
    "Output: transcribed speech from `audio_file`\n",
    "\n",
    "6\\. SpeechRecognition Example\n",
    "-----------------------------\n",
    "\n",
    "03:14 - 04:18\n",
    "\n",
    "We'll be using the recognize google function since it's free and doesn't require an API key. However, this limits us to 50 requests per day and if our audio files are too long, it may time out. In my experience, I've had no issues with audio files under 5-minutes. So if you have more audio files or long audio files, you may want to look into one of the paid API services. Let's put everything together with an example. We'll start by importing the speech recognition library as sr. Then we'll initialize a recognizer class. Finally we call recognize google which takes the required parameter audio data. We can also pass it the language our audio file is in. The default language is US English. We're using a mocked version of recognize google for this course so we don't go over the API limit. Running the function returns the speech detected in the audio file as text.\n",
    "\n",
    "• Focus on `recognize_google()`\n",
    "\n",
    "• Recognize speech from an audio file with SpeechRecognition:\n",
    "\n",
    "```python\n",
    "# Import SpeechRecognition library  \n",
    "import speech_recognition as sr\n",
    "# Instantiate Recognizer class\n",
    "recognizer = sr.Recognizer()\n",
    "# Transcribe speech using Goole web API\n",
    "recognizer.recognize_google(audio_data=audio_file,\n",
    "                          language=\"en-US\")\n",
    "```\n",
    "\n",
    "```\n",
    "Learning speech recognition on DataCamp is awesome!\n",
    "```\n",
    "\n",
    "7\\. Your turn!\n",
    "--------------\n",
    "\n",
    "04:18 - 04:26\n",
    "\n",
    "Now you've seen a starter example of the SpeechRecognition library, it's time to try it out for yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the wrong speech_recognition API\n",
    "=====================================\n",
    "\n",
    "Which of the following is **not** a speech recognition API within the `speech_recognition` library?\n",
    "\n",
    "An instance of the `Recognizer` class has been created and saved to `recognizer`. You can try calling the API on `recognizer` to see what happens.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "`recognize_google()`\n",
    "\n",
    "`recognize_bing()`\n",
    "\n",
    "`recognize_wit()`\n",
    "\n",
    "[/] `what_does_this_say()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SpeechRecognition library\n",
    "===================================\n",
    "\n",
    "To save typing `speech_recognition` every time, we'll import it as `sr`.\n",
    "\n",
    "We'll also setup an instance of the `Recognizer`class to use later.\n",
    "\n",
    "The `energy_threshold` is a number between 0 and 4000 for how much the `Recognizer` class should listen to an audio file.\n",
    "\n",
    "`energy_threshold` will dynamically adjust whilst the recognizer class listens to audio.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the `speech_recognition` library as `sr`.\n",
    "-   Setup an instance of the `Recognizer` class and save it to `recognizer`.\n",
    "-   Set the `recognizer.energy_threshold` to 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the speech_recognition library\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Create an instance of the Recognizer class\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Set the energy threshold\n",
    "recognizer.energy_threshold = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Recognizer class\n",
    "==========================\n",
    "\n",
    "Now you've created an instance of the `Recognizer` class we'll use the `recognize_google()` method on it to access the Google web speech API and turn spoken language into text.\n",
    "\n",
    "`recognize_google()` requires an argument `audio_data` otherwise it will return an error.\n",
    "\n",
    "US English is the default language. If your audio file isn't in US English, you can change the language with the `language` argument. A list of language codes can be seen [here](https://cloud.google.com/speech-to-text/docs/languages).\n",
    "\n",
    "An audio file containing English speech has been imported as `clean_support_call_audio`. You can [listen to the audio file here](https://assets.datacamp.com/production/repositories/4637/datasets/393a2f76d057c906de27ec57ea655cb1dc999fce/clean-support-call.wav). SpeechRecognition has also been imported as `sr`.\n",
    "\n",
    "To avoid hitting the API request limit of Google's web API, we've mocked the `Recognizer` class to work with our audio files. This means some functionality will be limited.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Call the `recognize_google()` method on `recognizer` and pass it `clean_support_call_audio`.\n",
    "-   Set the language argument to `\"en-US\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recognizer class\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Transcribe the support call audio\n",
    "text = recognizer.recognize_google(\n",
    "  audio_data=clean_support_call_audio, \n",
    "  language=\"en-US\")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Reading audio files with SpeechRecognition\n",
    "----------------------------------------------\n",
    "\n",
    "00:00 - 00:14\n",
    "\n",
    "In the last lesson, we transcribed a portion of a customer support audio file. But as you'll remember from earlier lessons, audio files require a bit of preprocessing before they can be worked with.\n",
    "\n",
    "2\\. The AudioFile class\n",
    "-----------------------\n",
    "\n",
    "00:14 - 01:08\n",
    "\n",
    "Luckily, the SpeechRecognition library has a built-in class, AudioFile, along with another handy method in the Recognizer class, record. We can use these to take care of the preprocessing for us. It was done for us in the last lesson but in this lesson we'll go end-to-end. To begin, we import the SpeechRecognition library and instantiate a recognizer instance as before. Then to read in our audio file we access the AudioFile class and pass it our audio file filename and save it to a variable. In this case, our AudioFile variable is called clean support call. Now if we check the type of clean support call, we can see it's an instance of AudioFile.\n",
    "\n",
    "```python\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Setup recognizer instance \n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Read in audio file\n",
    "clean_support_call = sr.AudioFile(\"clean-support-call.wav\")\n",
    "\n",
    "# Check type of clean_support_call\n",
    "type(clean_support_call)\n",
    "```\n",
    "`<class 'speech_recognition.AudioFile'>`\n",
    "\n",
    "3\\. From AudioFile to AudioData\n",
    "-------------------------------\n",
    "\n",
    "01:08 - 02:08\n",
    "\n",
    "Let's see what happens if we pass our clean support call variable to the recognize google method. It errors, stating that the audio data parameter must be of type audio data. Our clean support call variable is currently of the type AudioFile. To convert it to the audio data type we can use the recognizer class's built-in record method. Let's see it. We use a context manager, also known as with, to open and read the audio file we've saved to clean support call as source. Then we create clean support call audio using the record method and passing it source. Now before we call recognize google again, let's check the type of clean support call audio. Beautiful, it's an instance of AudioData, just what we needed.\n",
    "\n",
    "```python\n",
    "recognizer.recognize_google(audio_data=clean_support_call)\n",
    "```\n",
    "\n",
    "`AssertionError: ``audio_data`` must be audio data`\n",
    "\n",
    "```python\n",
    "# Convert from AudioFile to AudioData\n",
    "with clean_support_call as source:\n",
    "    # Record the audio\n",
    "    clean_support_call_audio = recognizer.record(source)\n",
    "\n",
    "# Check the type\n",
    "type(clean_support_call_audio)\n",
    "```\n",
    "\n",
    "`<class 'speech_recognition.AudioData'>`\n",
    "\n",
    "4\\. Transcribing our AudioData\n",
    "------------------------------\n",
    "\n",
    "02:08 - 02:23\n",
    "\n",
    "Now our clean support call audio is in the AudioData format, let's call recognize google and pass it our instance of audio data. Much better. Before you try it out for yourself, there are two parameters of the record method you should know about, duration and offset.\n",
    "\n",
    "```python\n",
    "# Transcribe clean support call\n",
    "recognizer.recognize_google(audio_data=clean_support_call_audio)\n",
    "```\n",
    "\n",
    "`hello I'd like to get some help setting up my account please`\n",
    "\n",
    "5\\. Duration and offset\n",
    "-----------------------\n",
    "\n",
    "02:23 - 03:41\n",
    "\n",
    "The record method records up to duration seconds of audio from source starting at offset. They're both set to None by default. This means that by default, record will record from the beginning of the file until there is no more audio. You can change this by setting them to a float value. For example, let's say you only wanted the first 2 seconds of all your audio files, you could set duration to 2. The offset parameter can be used to cut off or skip over a specified amount of seconds at the start of an audio file. For example, if you didn't want the first 5 seconds of your audio files, you could set offset to 5. These parameters could be helpful if you knew there were parts of your audio files you didn't need. But remember, altering these parameters may cut off your audio in undesirable locations. The most ideal values will be found by experimentation. We'll see more audio file manipulation later in the course.\n",
    "\n",
    "```python\n",
    "# duration and offset both None by default\n",
    "\n",
    "# Leave duration and offset as default\n",
    "with clean_support_call as source:\n",
    "    clean_support_call_audio = recognizer.record(source,\n",
    "                                               duration=None,\n",
    "                                               offset=None)\n",
    "\n",
    "# Get first 2-seconds of clean support call  \n",
    "with clean_support_call as source:\n",
    "    clean_support_call_audio = recognizer.record(source,\n",
    "                                               duration=2.0)\n",
    "```\n",
    "\n",
    "`hello I'd like to get`\n",
    "\n",
    "6\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:41 - 03:51\n",
    "\n",
    "Alright, enough talk, let's see speech transcription with SpeechRecognition in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From AudioFile to AudioData\n",
    "===========================\n",
    "\n",
    "As you saw earlier, there are some transformation steps we have to take to make our audio data useful. The same goes for SpeechRecognition. \n",
    "\n",
    "In this exercise, we'll import the `clean_support_call.wav` [audio file](https://assets.datacamp.com/production/repositories/4637/datasets/393a2f76d057c906de27ec57ea655cb1dc999fce/clean-support-call.wav) and get it ready to be recognized.\n",
    "\n",
    "We first read our audio file using the `AudioFile` class. But the `recognize_google()` method requires an input of type `AudioData`.\n",
    "\n",
    "To convert our `AudioFile` to `AudioData`, we'll use the `Recognizer` class's method `record()` along with a context manager. The `record()` method takes an `AudioFile` as input and converts it to `AudioData`, ready to be used with `recognize_google()`.\n",
    "\n",
    "SpeechRecognition has already been imported as `sr`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Pass the AudioFile class `clean_support_call.wav`.\n",
    "-   Use the context manager to open and read `clean_support_call` as `source`.\n",
    "-   Record `source` and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Convert audio to AudioFile\n",
    "clean_support_call = sr.AudioFile(\"clean_support_call.wav\")\n",
    "\n",
    "# Convert AudioFile to AudioData\n",
    "with clean_support_call as source:\n",
    "    clean_support_call_audio = recognizer.record(source)\n",
    "\n",
    "# Transcribe AudioData to text\n",
    "text = recognizer.recognize_google(clean_support_call_audio,\n",
    "                                   language=\"en-US\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recording the audio we need\n",
    "===========================\n",
    "\n",
    "Sometimes you may not want the entire audio file you're working with. The `duration` and `offset`parameters of the `record()` method can help with this.\n",
    "\n",
    "After exploring your dataset, you find there's one file, imported as `nothing_at_end` which has [30-seconds of silence at the end](https://assets.datacamp.com/production/repositories/4637/datasets/ca799cf2a7b093c06e1a5ae1dd96a49d48d65efa/30-seconds-of-nothing-16k.wav) and a support call file, imported as `out_of_warranty` has [3-seconds of static at the front](https://assets.datacamp.com/production/repositories/4637/datasets/dbc47d8210fdf8de42b0da73d1c2ba92e883b2d2/static-out-of-warranty.wav).\n",
    "\n",
    "Setting `duration` and `offset` means the `record()` method will record up to `duration` audio starting at `offset`. They're both measured in seconds.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Let's get the first 10-seconds of `nothing_at_end_audio`. To do this, you can set `duration` to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert AudioFile to AudioData\n",
    "with nothing_at_end as source:\n",
    "    nothing_at_end_audio = recognizer.record(source,\n",
    "                                             duration=10,  # Set duration to 10 to get the first 10 seconds\n",
    "                                             offset=None)\n",
    "\n",
    "# Transcribe AudioData to text\n",
    "text = recognizer.recognize_google(nothing_at_end_audio,\n",
    "                                   language=\"en-US\")\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Let's remove the first 3-seconds of static of `static_at_start` by setting `offset` to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert AudioFile to AudioData\n",
    "with static_at_start as source:\n",
    "    static_art_start_audio = recognizer.record(source,\n",
    "                                               duration=None,\n",
    "                                               offset=3)\n",
    "\n",
    "# Transcribe AudioData to text\n",
    "text = recognizer.recognize_google(static_art_start_audio,\n",
    "                                   language=\"en-US\")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Dealing with different kinds of audio\n",
    "-----------------------------------------\n",
    "\n",
    "00:00 - 00:19\n",
    "\n",
    "The SpeechRecognition library is very powerful out of the box, however, since speech recognition is still an active area of research, there are some limitations to the library. In this lesson, we'll start exploring some of the challenges you might run into and what you can do about them.\n",
    "\n",
    "2\\. What language?\n",
    "------------------\n",
    "\n",
    "00:19 - 00:45\n",
    "\n",
    "Although SpeechRecognition is capable of transcribing audio, it doesn't necessarily know what kind of audio it's transcribing. For example, if you pass recognizer an audio file with Japanese speech but the language tag was English US, you'd get back the Japanese audio in English characters. That makes sense.\n",
    "\n",
    "3\\. What language?\n",
    "------------------\n",
    "\n",
    "00:45 - 01:25\n",
    "\n",
    "And if you passed the same audio file with the language tag set to jay ay for Japanese, you'd get the audio transcribed into Japanese characters. The thing to note is the SpeechRecognition library doesn't automatically detect languages. So you'll have to ensure this parameter is set manually and make sure the API you're using has the capability to transcribe the language your audio files are in. We've seen the language tag in previous lessons. But what happens with non-speech audio?\n",
    "\n",
    "4\\. Non-speech audio\n",
    "--------------------\n",
    "\n",
    "01:25 - 01:43\n",
    "\n",
    "If we pass SpeechRecogition an audio file of a leopard roaring, it will return an unknown value error because no speech was detected. Which also makes sense because a leopard roar, although very cool, isn't human speech.\n",
    "\n",
    "5\\. Non-speech audio\n",
    "--------------------\n",
    "\n",
    "01:43 - 02:07\n",
    "\n",
    "We can prevent errors by using the show all parameter. The show all parameter shows a list of all the potential transcriptions the recognize google function came up with. In the case of our leopard roar, the list comes back empty but we avoid raising an error.\n",
    "\n",
    "6\\. Showing all\n",
    "---------------\n",
    "\n",
    "02:07 - 02:16\n",
    "\n",
    "Or in the case of our Japanese file, you can see the different potential transcriptions.\n",
    "\n",
    "7\\. Multiple speakers\n",
    "---------------------\n",
    "\n",
    "02:16 - 02:54\n",
    "\n",
    "Next comes multiple speakers. The free Google Web API transcribes speech and returns it as a single block of text no matter how many speakers there are. A returned single text block can still be useful, however, if your problem requires knowing who said what, you may want to consider the free API we're using as a proof of a concept. And then use one of the paid versions for more complex tasks. The process of splitting more than one speaker from a single audio file is called speaker diarization, however, it is beyond the scope of this course.\n",
    "\n",
    "8\\. Multiple speakers\n",
    "---------------------\n",
    "\n",
    "02:54 - 03:10\n",
    "\n",
    "To get around the multiple speakers problem, you could ensure your audio files are recorded separately for each speaker. Then transcribe the individual speakers audio.\n",
    "\n",
    "9\\. Noisy audio\n",
    "---------------\n",
    "\n",
    "03:10 - 04:04\n",
    "\n",
    "Finally, there's the problem of background noise. A rule of thumb to remember is if you have trouble understanding what is being said on an audio file due to background noise, chances are, a speech recognition system will too. To try and accommodate for background noise, the recognizer class has a built-in function, adjust for ambient noise, which takes a parameter, duration. The recognizer class then listens for duration seconds at the start of the audio file and adjusts the energy threshold, or the amount the recognizer class listens, to a level suitable for the background noise. How much space you have at the start of your audio file will dictate what you can set the duration value to. The SpeechRecognition documentation recommends somewhere between zero point five to one second as a good starting point.\n",
    "\n",
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:04 - 04:23\n",
    "\n",
    "As you can see, speech has a whole lot of variability which makes transcribing it a tough challenge. But now we've talked about some of the ways to deal with different kinds of audio, let's head over to the console and see it all in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different kinds of audio\n",
    "========================\n",
    "\n",
    "Now you've seen an example of how the `Recognizer` class works. Let's try a few more. How about speech from a different language?\n",
    "\n",
    "What do you think will happen when we call the `recognize_google()` function on a Japanese version of `good_morning.wav` ([file](https://assets.datacamp.com/production/repositories/4637/datasets/cd9b801670d0664275cdbd3a24b6b70a8c2e5222/good-morning-japanense.wav)) (`japanese_audio`)? \n",
    "\n",
    "The default language is `\"en-US\"`, are the results the same with the `\"ja\"` tag?\n",
    "\n",
    "How about non-speech audio? Like this [leopard roaring](https://assets.datacamp.com/production/repositories/4637/datasets/5720832b2735089d8e735cac3e0b0ad9b5114864/leopard.wav) (`leopard_audio`).\n",
    "\n",
    "Or speech where the sounds may not be real words, such as [a baby talking](https://assets.datacamp.com/production/repositories/4637/datasets/e9fd46a06d74431e3baa942c489e1b119d85a233/charlie-bit-me-5.wav) (`charlie_audio`)?\n",
    "\n",
    "To familiarize more with the `Recognizer` class, we'll look at an example of each of these.\n",
    "\n",
    "Instructions 1/4\n",
    "----------------\n",
    "\n",
    "-   Pass the Japanese version of good morning (`japanese_audio`) to `recognize_google()` using `\"en-US\"` as the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recognizer class\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Pass the Japanese audio to recognize_google\n",
    "text = recognizer.recognize_google(japanese_audio, language=\"en-US\")\n",
    "\n",
    "# Print the text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/4\n",
    "----------------\n",
    "\n",
    "-   Pass the same Japanese audio (`japanese_audio`) using `\"ja\"` as the language parameter. Do you see a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recognizer class\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Pass the Japanese audio to recognize_google\n",
    "text = recognizer.recognize_google(japanese_audio, language=\"ja\")\n",
    "\n",
    "# Print the text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/4\n",
    "----------------\n",
    "\n",
    "-   What about about non-speech audio? Pass `leopard_audio` to `recognize_google()` with `show_all` as `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recognizer class\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Pass the leopard roar audio to recognize_google\n",
    "text = recognizer.recognize_google(leopard_audio, \n",
    "                                   language=\"en-US\", \n",
    "                                   show_all=True)\n",
    "\n",
    "# Print the text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 4/4\n",
    "----------------\n",
    "\n",
    "-   What if your speech files have non-audible human sounds? Pass `charlie_audio` to `recognize_google()` to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recognizer class\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Pass charlie_audio to recognize_google\n",
    "text = recognizer.recognize_google(charlie_audio, \n",
    "                                   language=\"en-US\")\n",
    "\n",
    "# Print the text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Speakers 1\n",
    "===================\n",
    "\n",
    "If your goal is to transcribe conversations, there will be more than one speaker. However, as you'll see, the `recognize_google()` function will only transcribe speech into a single block of text.\n",
    "\n",
    "You can hear in [this audio file](https://assets.datacamp.com/production/repositories/4637/datasets/925c8c31d6e4af9c291c692f13e4f41c7b5e86b2/multiple-speakers-16k.wav) there are three different speakers.\n",
    "\n",
    "But if you transcribe it on its own, `recognize_google()` returns a single block of text. Which is still useful but it doesn't let you know which speaker said what.\n",
    "\n",
    "We'll see an alternative to this in the next exercise.\n",
    "\n",
    "The multiple speakers audio file has been imported and converted to `AudioData` as `multiple_speakers`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create an instance of `Recognizer`.\n",
    "-   Recognize the `multiple_speakers` variable using the `recognize_google()` function.\n",
    "-   Set the language to US English (`\"en-US\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recognizer class\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Recognize the multiple speaker AudioData\n",
    "text = recognizer.recognize_google(multiple_speakers, \n",
    "                       \t\t\t   language=\"en-US\")\n",
    "\n",
    "# Print the text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Speakers 2\n",
    "===================\n",
    "\n",
    "Deciphering between multiple speakers in one audio file is called speaker diarization. However, you've seen the free function we've been using, `recognize_google()` doesn't have the ability to transcribe different speakers. \n",
    "\n",
    "One way around this, without using one of the paid speech to text services, is to ensure your audio files are single speaker.\n",
    "\n",
    "This means if you were working with phone call data, you would make sure the caller and receiver are recorded separately. Then you could transcribe each file individually.\n",
    "\n",
    "In this exercise, we'll transcribe each of the speakers in our [multiple speakers audio file](https://assets.datacamp.com/production/repositories/4637/datasets/925c8c31d6e4af9c291c692f13e4f41c7b5e86b2/multiple-speakers-16k.wav) individually.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Pass `speakers` to the `enumerate()` function to loop through the different speakers.\n",
    "-   Call `record()` on `recognizer` to convert the `AudioFile`s into `AudioData`.\n",
    "-   Use `recognize_google()` to transcribe each of the `speaker_audio` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Multiple speakers on different files\n",
    "speakers = [sr.AudioFile(\"speaker_0.wav\"), \n",
    "            sr.AudioFile(\"speaker_1.wav\"), \n",
    "            sr.AudioFile(\"speaker_2.wav\")]\n",
    "\n",
    "# Transcribe each speaker individually\n",
    "for i, speaker in enumerate(speakers):\n",
    "    with speaker as source:\n",
    "    # Call record() on recognizer to convert the AudioFiles into AudioData.\n",
    "        speaker_audio = recognizer.record(source)\n",
    "    print(f\"Text from speaker {i}:\")\n",
    "    # Use recognize_google() to transcribe each of the speaker_audio objects.\n",
    "    print(recognizer.recognize_google(speaker_audio,\n",
    "         \t\t\t\t  language=\"en-US\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with noisy audio\n",
    "========================\n",
    "\n",
    "In this exercise, we'll start by transcribing a clean speech sample to text and then see what happens when we add some background noise.\n",
    "\n",
    "A clean audio sample has been imported as `clean_support_call`.\n",
    "\n",
    "[Play clean support call](https://assets.datacamp.com/production/repositories/4637/datasets/393a2f76d057c906de27ec57ea655cb1dc999fce/clean-support-call.wav).\n",
    "\n",
    "We'll then do the same with the noisy audio file saved as `noisy_support_call`. It has the same speech as `clean_support_call` but with additional background noise.\n",
    "\n",
    "[Play noisy support call](https://assets.datacamp.com/production/repositories/4637/datasets/f3edd5024944eac2f424b592840475890c86d405/2-noisy-support-call.wav).\n",
    "\n",
    "To try and negate the background noise, we'll take advantage of `Recognizer`'s `adjust_for_ambient_noise()` function.\n",
    "\n",
    "Instructions 1/4\n",
    "----------------\n",
    "\n",
    "-   Let's transcribe some clean audio. Read in `clean_support_call` as the source and call `recognize_google()` on the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Record the audio from the clean support call\n",
    "with clean_support_call as source:\n",
    "    clean_support_call_audio = recognizer.record(source)\n",
    "\n",
    "# Transcribe the speech from the clean support call\n",
    "text = recognizer.recognize_google(clean_support_call_audio,  # Use the correct audio data here\n",
    "                                   language=\"en-US\")\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/4\n",
    "----------------\n",
    "\n",
    "\n",
    "-   Let's transcribe some clean audio. Read in `clean_support_call` as the source and call `recognize_google()` on the file.\n",
    "\n",
    "-   Let's do the same as before but with a noisy audio file saved as `noisy_support_call` and `show_all` parameter as `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Record the audio from the noisy support call\n",
    "with noisy_support_call as source:\n",
    "    noisy_support_call_audio = recognizer.record(source)\n",
    "\n",
    "# Transcribe the speech from the noisy support call\n",
    "text = recognizer.recognize_google(noisy_support_call_audio,\n",
    "                                   language=\"en-US\",\n",
    "                                   show_all=True)  # Set show_all to True to return all possible transcriptions\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/4\n",
    "----------------\n",
    "\n",
    "-   Set the `duration` parameter of `adjust_for_ambient_noise()` to 1 (second) so `recognizer`adjusts for background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Record the audio from the noisy support call\n",
    "with noisy_support_call as source:\n",
    "    # Adjust the recognizer energy threshold for ambient noise\n",
    "    recognizer.adjust_for_ambient_noise(source, duration=1)  # Set duration to 1 second\n",
    "    noisy_support_call_audio = recognizer.record(source)\n",
    "\n",
    "# Transcribe the speech from the noisy support call\n",
    "text = recognizer.recognize_google(noisy_support_call_audio,\n",
    "                                   language=\"en-US\",\n",
    "                                   show_all=True)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 4/4\n",
    "----------------\n",
    "\n",
    "-   A `duration` of 1 was too long and it cut off some of the audio. Try setting `duration` to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Record the audio from the noisy support call\n",
    "with noisy_support_call as source:\n",
    "    # Adjust the recognizer energy threshold for ambient noise\n",
    "    recognizer.adjust_for_ambient_noise(source, duration=0.5)  # Set duration to 0.5 seconds\n",
    "    noisy_support_call_audio = recognizer.record(source)\n",
    "\n",
    "# Transcribe the speech from the noisy support call\n",
    "text = recognizer.recognize_google(noisy_support_call_audio,\n",
    "                                   language=\"en-US\",\n",
    "                                   show_all=True)\n",
    "\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
