{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. SpeechRecognition Python library\n",
    "------------------------------------\n",
    "\n",
    "00:00 - 00:15\n",
    "\n",
    "To get started with spoken language recognition, let's check out the SpeechRecognition Python Library. We'll start with why the SpeechRecognition Library. And then we'll get into seeing how we can use Google's web speech API to transcribe speech to text.\n",
    "\n",
    "2\\. Why the SpeechRecognition library?\n",
    "--------------------------------------\n",
    "\n",
    "00:15 - 00:55\n",
    "\n",
    "Automatic speech recognition is a tough challenge. And there's no shortage of companies and research institutions working on libraries to help solve it. There's the Sphinx library by Carnegie Mellon University, Kaldi, SpeechRecognition, and more. Some have more robust features than others but they all have the same goal of transcribing audio files to text. We're going to be focused on the SpeechRecognition library because of its low barrier to entry and its compatibility with many available speech recognition APIs we'll see shortly.\n",
    "\n",
    "3\\. Getting started with SpeechRecognition\n",
    "------------------------------------------\n",
    "\n",
    "00:55 - 01:15\n",
    "\n",
    "We can get started with the SpeechRecognition library by installing it from PyPi using pip and running the pip install SpeechRecognition command in a terminal or shell. It's compatible with Python 2 and 3 but we'll be using Python 3.\n",
    "\n",
    "4\\. Using the Recognizer class\n",
    "------------------------------\n",
    "\n",
    "01:15 - 02:23\n",
    "\n",
    "Now we have SpeechRecognition installed, let's check out where all the magic happens, the recognizer class. So how do we use it? To access the Recognizer class, we'll first import the SpeechRecognition module as the abbreviation sr. Then we'll create an instance of the recognizer class by calling it from sr and assigning to a variable, recognizer. Finally, we'll set the recognizers energy threshold to 300. The energy threshold can be thought of as the loudness of audio which is considered speech. Values below the threshold are considered silence, values above are considered speech. A silent room is typically between 0 and 100. SpeechRecognition's documentation recommends 300 as a starting value which covers most speech files. The energy threshold value will adjust automatically as the recognizer listens to an audio file.\n",
    "\n",
    "5\\. Using the Recognizer class to recognize speech\n",
    "--------------------------------------------------\n",
    "\n",
    "02:23 - 03:14\n",
    "\n",
    "Now we've got a recognizer instance ready, it's time to recognize some speech. We chose SpeechRecognition for its flexibility. Here's what I mean. SpeechRecognition has functions built-in to work with many of the best speech recognition APIs. Recognize bing accesses Microsoft's cognitive services, recognize Google uses Google's free web speech API, recognize Google Cloud accesses Google's cloud speed API. And recognize wit uses the wit dot ai platform. They all accept an audio file and return text, which is hopefully the transcribed speech from the audio file. Remember, speech recognition is still far from perfect.\n",
    "\n",
    "6\\. SpeechRecognition Example\n",
    "-----------------------------\n",
    "\n",
    "03:14 - 04:18\n",
    "\n",
    "We'll be using the recognize google function since it's free and doesn't require an API key. However, this limits us to 50 requests per day and if our audio files are too long, it may time out. In my experience, I've had no issues with audio files under 5-minutes. So if you have more audio files or long audio files, you may want to look into one of the paid API services. Let's put everything together with an example. We'll start by importing the speech recognition library as sr. Then we'll initialize a recognizer class. Finally we call recognize google which takes the required parameter audio data. We can also pass it the language our audio file is in. The default language is US English. We're using a mocked version of recognize google for this course so we don't go over the API limit. Running the function returns the speech detected in the audio file as text.\n",
    "\n",
    "7\\. Your turn!\n",
    "--------------\n",
    "\n",
    "04:18 - 04:26\n",
    "\n",
    "Now you've seen a starter example of the SpeechRecognition library, it's time to try it out for yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the wrong speech_recognition API\n",
    "=====================================\n",
    "\n",
    "Which of the following is **not** a speech recognition API within the `speech_recognition` library?\n",
    "\n",
    "An instance of the `Recognizer` class has been created and saved to `recognizer`. You can try calling the API on `recognizer` to see what happens.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "`recognize_google()`\n",
    "\n",
    "`recognize_bing()`\n",
    "\n",
    "`recognize_wit()`\n",
    "\n",
    "[/] `what_does_this_say()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SpeechRecognition library\n",
    "===================================\n",
    "\n",
    "To save typing `speech_recognition` every time, we'll import it as `sr`.\n",
    "\n",
    "We'll also setup an instance of the `Recognizer`class to use later.\n",
    "\n",
    "The `energy_threshold` is a number between 0 and 4000 for how much the `Recognizer` class should listen to an audio file.\n",
    "\n",
    "`energy_threshold` will dynamically adjust whilst the recognizer class listens to audio.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the `speech_recognition` library as `sr`.\n",
    "-   Setup an instance of the `Recognizer` class and save it to `recognizer`.\n",
    "-   Set the `recognizer.energy_threshold` to 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the speech_recognition library\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Create an instance of the Recognizer class\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Set the energy threshold\n",
    "recognizer.energy_threshold = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Recognizer class\n",
    "==========================\n",
    "\n",
    "Now you've created an instance of the `Recognizer` class we'll use the `recognize_google()` method on it to access the Google web speech API and turn spoken language into text.\n",
    "\n",
    "`recognize_google()` requires an argument `audio_data` otherwise it will return an error.\n",
    "\n",
    "US English is the default language. If your audio file isn't in US English, you can change the language with the `language` argument. A list of language codes can be seen [here](https://cloud.google.com/speech-to-text/docs/languages).\n",
    "\n",
    "An audio file containing English speech has been imported as `clean_support_call_audio`. You can [listen to the audio file here](https://assets.datacamp.com/production/repositories/4637/datasets/393a2f76d057c906de27ec57ea655cb1dc999fce/clean-support-call.wav). SpeechRecognition has also been imported as `sr`.\n",
    "\n",
    "To avoid hitting the API request limit of Google's web API, we've mocked the `Recognizer` class to work with our audio files. This means some functionality will be limited.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Call the `recognize_google()` method on `recognizer` and pass it `clean_support_call_audio`.\n",
    "-   Set the language argument to `\"en-US\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recognizer class\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Transcribe the support call audio\n",
    "text = recognizer.recognize_google(\n",
    "  audio_data=clean_support_call_audio, \n",
    "  language=\"en-US\")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Reading audio files with SpeechRecognition\n",
    "----------------------------------------------\n",
    "\n",
    "00:00 - 00:14\n",
    "\n",
    "In the last lesson, we transcribed a portion of a customer support audio file. But as you'll remember from earlier lessons, audio files require a bit of preprocessing before they can be worked with.\n",
    "\n",
    "2\\. The AudioFile class\n",
    "-----------------------\n",
    "\n",
    "00:14 - 01:08\n",
    "\n",
    "Luckily, the SpeechRecognition library has a built-in class, AudioFile, along with another handy method in the Recognizer class, record. We can use these to take care of the preprocessing for us. It was done for us in the last lesson but in this lesson we'll go end-to-end. To begin, we import the SpeechRecognition library and instantiate a recognizer instance as before. Then to read in our audio file we access the AudioFile class and pass it our audio file filename and save it to a variable. In this case, our AudioFile variable is called clean support call. Now if we check the type of clean support call, we can see it's an instance of AudioFile.\n",
    "\n",
    "3\\. From AudioFile to AudioData\n",
    "-------------------------------\n",
    "\n",
    "01:08 - 02:08\n",
    "\n",
    "Let's see what happens if we pass our clean support call variable to the recognize google method. It errors, stating that the audio data parameter must be of type audio data. Our clean support call variable is currently of the type AudioFile. To convert it to the audio data type we can use the recognizer class's built-in record method. Let's see it. We use a context manager, also known as with, to open and read the audio file we've saved to clean support call as source. Then we create clean support call audio using the record method and passing it source. Now before we call recognize google again, let's check the type of clean support call audio. Beautiful, it's an instance of AudioData, just what we needed.\n",
    "\n",
    "4\\. Transcribing our AudioData\n",
    "------------------------------\n",
    "\n",
    "02:08 - 02:23\n",
    "\n",
    "Now our clean support call audio is in the AudioData format, let's call recognize google and pass it our instance of audio data. Much better. Before you try it out for yourself, there are two parameters of the record method you should know about, duration and offset.\n",
    "\n",
    "5\\. Duration and offset\n",
    "-----------------------\n",
    "\n",
    "02:23 - 03:41\n",
    "\n",
    "The record method records up to duration seconds of audio from source starting at offset. They're both set to None by default. This means that by default, record will record from the beginning of the file until there is no more audio. You can change this by setting them to a float value. For example, let's say you only wanted the first 2 seconds of all your audio files, you could set duration to 2. The offset parameter can be used to cut off or skip over a specified amount of seconds at the start of an audio file. For example, if you didn't want the first 5 seconds of your audio files, you could set offset to 5. These parameters could be helpful if you knew there were parts of your audio files you didn't need. But remember, altering these parameters may cut off your audio in undesirable locations. The most ideal values will be found by experimentation. We'll see more audio file manipulation later in the course.\n",
    "\n",
    "6\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:41 - 03:51\n",
    "\n",
    "Alright, enough talk, let's see speech transcription with SpeechRecognition in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From AudioFile to AudioData\n",
    "===========================\n",
    "\n",
    "As you saw earlier, there are some transformation steps we have to take to make our audio data useful. The same goes for SpeechRecognition. \n",
    "\n",
    "In this exercise, we'll import the `clean_support_call.wav` [audio file](https://assets.datacamp.com/production/repositories/4637/datasets/393a2f76d057c906de27ec57ea655cb1dc999fce/clean-support-call.wav) and get it ready to be recognized.\n",
    "\n",
    "We first read our audio file using the `AudioFile` class. But the `recognize_google()` method requires an input of type `AudioData`.\n",
    "\n",
    "To convert our `AudioFile` to `AudioData`, we'll use the `Recognizer` class's method `record()` along with a context manager. The `record()` method takes an `AudioFile` as input and converts it to `AudioData`, ready to be used with `recognize_google()`.\n",
    "\n",
    "SpeechRecognition has already been imported as `sr`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Pass the AudioFile class `clean_support_call.wav`.\n",
    "-   Use the context manager to open and read `clean_support_call` as `source`.\n",
    "-   Record `source` and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Convert audio to AudioFile\n",
    "clean_support_call = sr.AudioFile(\"clean_support_call.wav\")\n",
    "\n",
    "# Convert AudioFile to AudioData\n",
    "with clean_support_call as source:\n",
    "    clean_support_call_audio = recognizer.record(source)\n",
    "\n",
    "# Transcribe AudioData to text\n",
    "text = recognizer.recognize_google(clean_support_call_audio,\n",
    "                                   language=\"en-US\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recording the audio we need\n",
    "===========================\n",
    "\n",
    "Sometimes you may not want the entire audio file you're working with. The `duration` and `offset`parameters of the `record()` method can help with this.\n",
    "\n",
    "After exploring your dataset, you find there's one file, imported as `nothing_at_end` which has [30-seconds of silence at the end](https://assets.datacamp.com/production/repositories/4637/datasets/ca799cf2a7b093c06e1a5ae1dd96a49d48d65efa/30-seconds-of-nothing-16k.wav) and a support call file, imported as `out_of_warranty` has [3-seconds of static at the front](https://assets.datacamp.com/production/repositories/4637/datasets/dbc47d8210fdf8de42b0da73d1c2ba92e883b2d2/static-out-of-warranty.wav).\n",
    "\n",
    "Setting `duration` and `offset` means the `record()` method will record up to `duration` audio starting at `offset`. They're both measured in seconds.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Let's get the first 10-seconds of `nothing_at_end_audio`. To do this, you can set `duration` to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert AudioFile to AudioData\n",
    "with nothing_at_end as source:\n",
    "    nothing_at_end_audio = recognizer.record(source,\n",
    "                                             duration=10,  # Set duration to 10 to get the first 10 seconds\n",
    "                                             offset=None)\n",
    "\n",
    "# Transcribe AudioData to text\n",
    "text = recognizer.recognize_google(nothing_at_end_audio,\n",
    "                                   language=\"en-US\")\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Let's remove the first 3-seconds of static of `static_at_start` by setting `offset` to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert AudioFile to AudioData\n",
    "with static_at_start as source:\n",
    "    static_art_start_audio = recognizer.record(source,\n",
    "                                               duration=None,\n",
    "                                               offset=3)\n",
    "\n",
    "# Transcribe AudioData to text\n",
    "text = recognizer.recognize_google(static_art_start_audio,\n",
    "                                   language=\"en-US\")\n",
    "\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
