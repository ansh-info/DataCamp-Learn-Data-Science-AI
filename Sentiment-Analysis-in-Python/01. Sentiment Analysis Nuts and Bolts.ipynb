{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Welcome!\n",
    "------------\n",
    "\n",
    "00:00¬†-¬†00:15\n",
    "\n",
    "Welcome to the course! In this course, we will build upon some of your Python skills and introduce methods for sentiment analysis using movie and product reviews, Twitter data and a lot of literary examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. What is sentiment analysis?\n",
    "-------------------------------\n",
    "\n",
    "00:15¬†-¬†00:34\n",
    "\n",
    "Let's start with defining what sentiment analysis is. Sentiment analysis, also called opinion mining, is the process of understanding the opinion of an author about a subject. In other words, \"What is the emotion or opinion of the author of the text about the subject discussed?\"\n",
    "\n",
    "```markdown\n",
    "# Sentiment analysis is the process of understanding the opinion of an author about a subject.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. What goes into a sentiment analysis system?\n",
    "-----------------------------------------------\n",
    "\n",
    "00:34¬†-¬†01:02\n",
    "\n",
    "In a sentiment analysis system, depending on the context, we usually have 3 elements: First is the opinion or an emotion. An opinion (also called \"polarity\") can be positive, neutral or negative. An emotion could be qualitative (like joy, surprise, or anger) or quantitative (like rating a movie on the scale from 1 to 10).\n",
    "\n",
    "```markdown\n",
    "# First element: Opinion/emotion\n",
    "\n",
    "## Opinion (polarity): pos, neutral, neg\n",
    "\n",
    "üëç POSITIVE\n",
    "\n",
    "üëä NEUTRAL  \n",
    "\n",
    "üëé NEGATIVE\n",
    "\n",
    "## Emotion\n",
    "\n",
    "üòä JOY        üòØ SURPRISE\n",
    "\n",
    "üò† ANGER      ü§¢ DISGUST\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. What goes into a sentiment analysis system?\n",
    "-----------------------------------------------\n",
    "\n",
    "01:02¬†-¬†01:30\n",
    "\n",
    "The second element in a sentiment analysis system is the subject that is being talked about, such as a book, a movie, or a product. Sometimes one opinion could discuss multiple aspects of the same subject. For example: \"The camera on this phone is great but its battery life is rather disappointing.\"\" The third element is the opinion holder, or entity, expressing the opinion.\n",
    "\n",
    "```markdown\n",
    "# Second element: subject\n",
    "\n",
    "### Subject of discussion: What is being talked about?\n",
    "_The camera on this phone is great but its battery life is rather disappointing._\n",
    "\n",
    "# Third element: opinion holder\n",
    "\n",
    "### Opinion holder (entity): By whom?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Why sentiment analysis?\n",
    "---------------------------\n",
    "\n",
    "01:30¬†-¬†02:15\n",
    "\n",
    "Sentiment analysis has many practical applications. In social media monitoring, we don't just want to know if people are talking about a brand; we want to know how they are talking about it. Social media isn't our only source of information; we can also find sentiment on forums, blogs, and the news. Most brands analyze all of these sources to enrich their understanding of how customers interact with their brand, what they are happy or unhappy about, and what matters most to consumers. Sentiment analysis is thus very important in brand monitoring, and in fields such as customer and product analytics and market research and analysis.\n",
    "\n",
    "```markdown\n",
    "- Social media monitoring\n",
    "  - Not only what people are talking about but HOW they are talking about it\n",
    "  - Sentiment can be found also in forums, blogs, news\n",
    "\n",
    "- Brand monitoring\n",
    "\n",
    "- Customer service\n",
    "\n",
    "- Product analytics\n",
    "\n",
    "- Market research and analysis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Let's look at movie reviews!\n",
    "--------------------------------\n",
    "\n",
    "02:15¬†-¬†02:37\n",
    "\n",
    "Let's look at the first dataset we will use in this course: a sample of IMDB movie reviews. We have two columns: one for the text of the review, and a second one called \"label\", which expresses the overall sentiment: the category or class 1 means positive and 0 means negative.\n",
    "\n",
    "```markdown\n",
    "data.head()\n",
    "\n",
    "|   | review                                          | label |\n",
    "|---|------------------------------------------------|-------|\n",
    "| 0 | This short spoof can be found on Elite's Mille...| 0     |\n",
    "| 1 | A singularly unfunny musical comedy that artif...| 0     |\n",
    "| 2 | An excellent series, masterfully acted and dir...| 1     |\n",
    "| 3 | The master of movie spectacle Cecil B. De Mill...| 1     |\n",
    "| 4 | I was gifted with this movie as it had such a ...| 0     |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. How many positive and negative reviews?\n",
    "-------------------------------------------\n",
    "\n",
    "02:37¬†-¬†02:55\n",
    "\n",
    "Let's find out how many positive and negative reviews we have in the data. To do this, we call the .value_counts() method on the \"label\" column. The output is the number of negative reviews (the 0 class) and positive reviews (the class 1).\n",
    "\n",
    "```markdown\n",
    "data.label.value_counts()\n",
    "\n",
    "0    3782\n",
    "1    3719\n",
    "Name: label, dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Percentage of positive and negative reviews\n",
    "-----------------------------------------------\n",
    "\n",
    "02:55¬†-¬†03:11\n",
    "\n",
    "If we want to see the number of positives and negatives as a percentage, we can divide the expression by the number of rows, which we obtain with the len() method. We see that the sample is rather balanced: around half of the reviews are positive and half are negative.\n",
    "\n",
    "```markdown\n",
    "data.label.value_counts() / len(data)\n",
    "\n",
    "0    0.504199\n",
    "1    0.495801\n",
    "Name: label, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. How long is the longest review?\n",
    "-----------------------------------\n",
    "\n",
    "03:11¬†-¬†03:52\n",
    "\n",
    "How long is the longest review? To find that, we create a pandas Series called length_reviews by selecting the review column of the dataset, followed by .str.len(). Str is short for string. We need to call the string function to transform the Series of reviews to a string. If we skip it, we get an AttributeError when the len() function is called. The result returns a pandas Series with the number of characters in each review. To find the length of the longest review, we need to call the max() function on the length_reviews Series.\n",
    "\n",
    "```markdown\n",
    "length_reviews = data.review.str.len()\n",
    "\n",
    "type(length_reviews)\n",
    "pandas.core.series.Series\n",
    "\n",
    "# Finding the review with max length\n",
    "max(length_reviews)\n",
    "\n",
    "0     667\n",
    "1    2982\n",
    "2     669 \n",
    "3    1087\n",
    "....\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. How long is the shortest review?\n",
    "-------------------------------------\n",
    "\n",
    "03:52¬†-¬†03:59\n",
    "\n",
    "To find the shortest review, we call the min() function on the length_reviews Series, instead of the max() function.\n",
    "\n",
    "```markdown\n",
    "length_reviews = data.review.str.len()\n",
    "\n",
    "# Finding the review with min length\n",
    "min(length_reviews)\n",
    "\n",
    "0     667\n",
    "1    2982\n",
    "2     669\n",
    "3    1087\n",
    "4     724\n",
    "....\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "03:59¬†-¬†04:03\n",
    "\n",
    "Let's practice what we've learned in the exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements of a sentiment analysis problem\n",
    "========================================\n",
    "\n",
    "What are the three typical elements of a sentiment analysis system?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   Opinion, emotion, and subject.\n",
    "\n",
    "-   Opinion, subject, and opinion holder.\n",
    "\n",
    "-   Emotion, polarity, and opinion.\n",
    "\n",
    "-   Opinion, subject, and polarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many positive and negative reviews are there?\n",
    "=================================================\n",
    "\n",
    "As a first step in a sentiment analysis task, similar to other data science problems, we might want to explore the dataset in more detail.\n",
    "\n",
    "You will work with a sample of the IMDB movies reviews. A dataset called¬†`movies`¬†has been created for you. It is a sample of the data we saw in the slides. Feel free to explore it in the IPython Shell, calling the¬†`.head()`¬†method, for example.\n",
    "\n",
    "*Be aware that this exercise uses real data, and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real data).*\n",
    "\n",
    "\n",
    "-   Find the number of positive and negative reviews in the¬†`movies`¬†dataset.\n",
    "-   Find the proportion (percentage) of positive and negative reviews in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of positive and negative reviews\n",
    "print('Number of positive and negative reviews: ', movies.label.value_counts())\n",
    "\n",
    "# Find the proportion of positive and negative reviews\n",
    "print('Proportion of positive and negative reviews: ', movies.label.value_counts() / len(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longest and shortest reviews\n",
    "============================\n",
    "\n",
    "In this exercise, you will continue to work with the¬†`movies`¬†dataset. You explored how many positive and negative reviews there are. Now your task is to explore the¬†`review`¬†column in more detail.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Use the¬†`review`¬†column of the¬†`movies`¬†dataset to find the length of the longest review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_reviews = movies.review.str.len()\n",
    "\n",
    "# How long is the longest review\n",
    "print(max(length_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Use the¬†`review`¬†column of the¬†`movies`¬†dataset to find the length of the longest review.\n",
    "\n",
    "-   Similarly, find the length of the shortest review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_reviews = movies.review.str.len()\n",
    "\n",
    "# How long is the shortest review\n",
    "print(min(length_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Sentiment analysis types and approaches\n",
    "-------------------------------------------\n",
    "\n",
    "00:00¬†-¬†00:11\n",
    "\n",
    "Welcome back! In the previous video, we learned what sentiment analysis is and why it is useful. But how do we even start with a sentiment analysis task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Levels of granularity\n",
    "-------------------------\n",
    "\n",
    "00:11¬†-¬†01:01\n",
    "\n",
    "Sentiment analysis tasks can be carried out at different levels of granularity. First is document level. This is when we look at the whole review of a product, for example. Second is the sentence level. This refers to determining whether the opinion expressed in each sentence is positive, negative, or neutral. The last level of granularity is the aspect level. The aspect refers to expressing opinions about different features of a product. Imagine a sentence such as \"The camera in this phone is pretty good but the battery life is disappointing.\" It expresses both positive and negative opinions about a phone and we might want to be able to say which features of the product clients like and which they don't.\n",
    "\n",
    "```markdown\n",
    "1. Document level\n",
    "2. Sentence level\n",
    "3. Aspect level\n",
    "\n",
    "_The camera in this phone is pretty good but the battery life is disappointing._\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Type of sentiment analysis algorithms\n",
    "-----------------------------------------\n",
    "\n",
    "01:01¬†-¬†02:12\n",
    "\n",
    "The algorithms used for sentiment analysis could be split into 2 main categories. The first is rule or lexicon based. Such methods most commonly have a predefined list of words with a valence score. For example, nice could be +2, good +1, terrible -3, and so on. The algorithm then matches the words from the lexicon to the words in the text and either sums or averages the scores in some way. As an example, let's take the sentence, 'Today was a good day.' Each word gets a score, and to get the total valence we sum the words. In this case, we have a positive sentence. A second category is automated systems, which are based on machine learning. This is going to be our focus in this course. The task is usually modeled as a classification problem where using some historical data with known sentiment, we need to predict the sentiment of a new piece of text.\n",
    "\n",
    "‚Ä¢ Rule/lexicon-based\n",
    "\n",
    "*nice:+2, good:+1, terrible:-3 ...*\n",
    "\n",
    "Today was a good day.\n",
    "\n",
    "```\n",
    "Today: 0, was:0, a:0, good:+1, day:0\n",
    "Total valence: +1\n",
    "```\n",
    "\n",
    "‚Ä¢ Automatic/ Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. What is the valence of a sentence?\n",
    "--------------------------------------\n",
    "\n",
    "02:12¬†-¬†03:22\n",
    "\n",
    "We can calculate the valence score of a text, using Python's textblob library. We continue working with our 'Today was a good day' string. We import the TextBlob function from the textblob package and apply it to our string. A TextBlob object is like a Python string, which has obtained some natural language processing skills. We can call different properties of the TextBlob object. We are interested in its sentiment; that's why we call sentiment on our TextBlob. The sentiment property returns a tuple: polarity, which is measured on the scale from [-1.0 to 1.0], where -1.0 is very negative, 0 is neutral and +1.0 is very positive. Our example 'Today was a good day' carries positive emotion and thus will have a positive polarity score: 0.7. The second element in the tuple displays the subjectivity, measured from [0.0 to 1.0] where 0.0 is very objective and 1.0 is very subjective. So our example is rather positive and subjective.\n",
    "\n",
    "```python\n",
    "text = \"Today was a good day.\"\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "my_valence = TextBlob(text)\n",
    "my_valence.sentiment\n",
    "\n",
    "Sentiment(polarity=0.7, subjectivity=0.6000000000000001)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Automated or rule-based?\n",
    "----------------------------\n",
    "\n",
    "03:22¬†-¬†04:11\n",
    "\n",
    "Which method should one use? A machine learning sentiment analysis relies on having labeled historical data whereas lexicon-based methods rely on having manually created rules or dictionaries. Lexicon-based methods fail at certain tasks because the polarity of words might change with the problem, which will not be reflected in a predefined dictionary. However, lexicon-based approaches can be quite fast, whereas Machine learning models might take a while to train. At the same time, machine learning models can be quite powerful. So, the jury is still out on that one. Many people find that a hybrid approach tends to work best in many, usually complex scenarios.\n",
    "\n",
    "# Automated/Machine learning\n",
    "\n",
    "* Rely on having labelled historical data\n",
    "* Might take a while to train  \n",
    "* Latest machine learning models can be quite powerful\n",
    "\n",
    "# Rule/lexicon-based\n",
    "\n",
    "* Rely on manually crafted valence scores\n",
    "* Different words might have different polarity in different contexts\n",
    "* Can be quite fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:11¬†-¬†04:16\n",
    "\n",
    "Now let's test what we've learned by solving some exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting the sentiment of Tale of Two Cities\n",
    "=============================================\n",
    "\n",
    "In the video we saw that one type of algorithms for detecting the sentiment are based on a lexicon of predefined words and their corresponding polarity score. Your task in this exercise is to detect the sentiment, including polarity and subjectivity of a given string using such a rule-based approach and the¬†`textblob`library in Python.\n",
    "\n",
    "You will work with the¬†`two_cities`¬†string. It contains the first sentence of Dickens's¬†*A Tale of Two Cities*¬†novel. Feel free to explore it in the Shell.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create a text blob object from the¬†`two_cities`¬†string.\n",
    "-   Print out the polarity and subjectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a textblob object  \n",
    "blob_two_cities = TextBlob(two_cities)\n",
    "\n",
    "# Print out the sentiment \n",
    "print(blob_two_cities.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the sentiment of two strings\n",
    "======================================\n",
    "\n",
    "In this exercise, you will compare the sentiment of two different strings. A string called¬†`annak`has been defined for you and it contains the first sentence of¬†*Anna Karenina*. A second string called¬†`catcher`¬†has been created and it contains the first sentence of¬†*The Catcher in the Rye*. Feel free to explore both in the IPython Shell.\n",
    "\n",
    "Your task is again to detect the sentiment of each string - both their polarity and subjectivity. Which one has higher sentiment score? Did you expect that to be the case?\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the required function from the appropriate package.\n",
    "-   Create a text blob object from the¬†`annak`string.\n",
    "-   Create a text blob from the¬†`catcher`¬†string as well.\n",
    "-   Print out the polarity and subjectivity of each of the created blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a textblob object \n",
    "blob_annak = TextBlob(annak)\n",
    "blob_catcher = TextBlob(catcher)\n",
    "\n",
    "# Print out the sentiment   \n",
    "print('Sentiment of annak: ', blob_annak.sentiment)\n",
    "print('Sentiment of catcher: ', blob_catcher.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the sentiment of a movie review?\n",
    "========================================\n",
    "\n",
    "In a previous exercise, you detected the sentiment of the first sentence of the¬†*Tale of Two Cities*¬†novel by Dickens. Now you will continue to work with the¬†**movie reviews**dataset. Do you remember how you found the longest and shortest reviews? One of the longest reviews has been imported for you. It is called¬†`titanic`¬†as it discusses the Titanic movie. Feel free to explore it in the Shell.\n",
    "\n",
    "Can you calculate the polarity and subjectivity of the¬†`titanic`¬†string? This review is positive (i.e. has a¬†`label`¬†of¬†`1`). Is the polarity score also positive?\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the required functionality.¬†\n",
    "-   Create a text blob object from the¬†`titanic`string.\n",
    "-   Print out the result of its sentiment property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a textblob object  \n",
    "blob_titanic = TextBlob(titanic)\n",
    "\n",
    "# Print out its sentiment  \n",
    "print(blob_titanic.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Let's build a word cloud!\n",
    "-----------------------------\n",
    "\n",
    "00:00¬†-¬†00:05\n",
    "\n",
    "Welcome back! Chances are high that you have seen a word cloud before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Word cloud example\n",
    "----------------------\n",
    "\n",
    "00:05¬†-¬†00:23\n",
    "\n",
    "A word cloud is an image composed of words with different sizes and colors. They can be especially useful in sentiment analysis. Have you ever wondered how such an image is generated? In this video, we will learn how to create a word cloud in Python.\n",
    "\n",
    "```\n",
    "epoch age\n",
    "    period us   season\n",
    "              times        wisdom\n",
    "winter    light    best     hope\n",
    "  spring        worst     direct\n",
    "Heaven     belief        despair\n",
    "  foolishness     darkness\n",
    "    incredulity     way     present\n",
    "authorities    received    insisted\n",
    "  everything    nothing\n",
    "```\n",
    "\n",
    "Note: This is an approximation of the word cloud layout in plain text/markdown format. The original has different colors and precise positioning that can't be perfectly replicated in markdown, but I've tried to maintain the general spatial relationships and relative prominence of the words.\n",
    "\n",
    "The prominent words include \"epoch,\" \"age,\" \"times,\" \"season,\" \"period,\" along with contrasting concepts like \"wisdom/foolishness,\" \"light/darkness,\" \"best/worst,\" and themes related to time periods like \"winter\" and \"spring.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. How do word clouds work?\n",
    "----------------------------\n",
    "\n",
    "00:23¬†-¬†00:47\n",
    "\n",
    "Word clouds (also called tag clouds) are used across different contexts. In the most common type of word clouds - and the one we will be using in this course - the size of the text corresponds to the frequency of the word. The more frequent a word is, the bigger and bolder it will appear on the word cloud.\n",
    "\n",
    "The more frequent a word is, the BIGGER and **bolder** it will appear on the word cloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. A word cloud example\n",
    "------------------------\n",
    "\n",
    "00:47¬†-¬†01:02\n",
    "\n",
    "Remember how we found the longest movie review? This word cloud is generated using only the words in one of the longest reviews. Which movie do you guess the review is talking about? I think we can agree it is about the Titanic!\n",
    "\n",
    "```\n",
    "                                    York\n",
    "                            Women California\n",
    "                  historical can White\n",
    "Titanic's depicted  characters James \n",
    "          Cameron class story\n",
    "    scene time people   Rose\n",
    "     events      ship film\n",
    "    Edward      Line       hits\n",
    " officer           movie  Captain\n",
    "    wreck voyage  passengers\n",
    "        love   fictional   Star\n",
    "            speed   tragedy\n",
    "```\n",
    "\n",
    "Note: The word \"Titanic\" appears as the largest word in the original image, forming the background. I've tried to maintain the relative sizes and positioning of other key words in the simplified markdown format, though the actual image has more words and uses different colors and precise positioning that can't be exactly replicated in plain text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Why word clouds?\n",
    "--------------------\n",
    "\n",
    "01:02¬†-¬†01:54\n",
    "\n",
    "Why are word clouds so popular? First of all, they can reveal the essential. We saw in our word cloud, the word Titanic really popped out. Second, unless told otherwise, they will plot all the words in a text, and a quick scan of the image can provide an overall sense of the text. Last but not least, they are easy to understand and quite fun. However, they have their drawbacks. Sometimes they tend to work less well. All the words plotted on the cloud might seem unrelated and it could be difficult to draw a conclusion based on a crowded word cloud. Secondly, if the text we work with is large, a word cloud might require quite a lot of preprocessing steps before it appears sensible and uncluttered.\n",
    "\n",
    "# Why word clouds?\n",
    "\n",
    "## Pros\n",
    "* Can reveal the essential\n",
    "* Provide an overall sense of the text\n",
    "* Easy to grasp and engaging\n",
    "\n",
    "## Cons\n",
    "* Sometimes confusing and uninformative\n",
    "* With larger text, require more work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Let's build a word cloud in Python!\n",
    "---------------------------------------\n",
    "\n",
    "01:54¬†-¬†02:23\n",
    "\n",
    "Now let's create a word cloud in Python.To do that, we can use the WordCloud function from the wordcloud package. We will have to import matplotlib.pyplot as well, which will allow wordcloud to plot on its base. Let's define a string, called two_cities, which captures the first sentence of Dickens' A Tale of Two Cities. Note how the text carries many emotionally charged words.\n",
    "\n",
    "```python\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "two_cities = \"It was the best of times, it was the worst of times,\n",
    "            it was the age of wisdom, it was the age of foolishness,\n",
    "            it was the epoch of belief, it was the epoch of incredulity,\n",
    "            it was the season of Light, it was the season of Darkness,\n",
    "            it was the spring of hope, it was the winter of despair,\n",
    "            we had everything before us, we had nothing before us,\n",
    "            we were all going direct to Heaven, we were all going\n",
    "            direct the other way - in short, the period was so far\n",
    "            like the present period, that some of its noisiest\n",
    "            authorities insisted on its being received, for good\n",
    "            or for evil, in the superlative degree of comparison only.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Define the WordCloud object\n",
    "-------------------------------\n",
    "\n",
    "02:23¬†-¬†03:15\n",
    "\n",
    "After we have imported the package, we build the cloud by calling the WordCloud function, followed by the generate method, which takes as argument the text, in our case - the two_cities string. The WordCloud function has many arguments. We will not cover all of them here but if you want to learn what they are, just type ?WordCloud in the Shell. You can change things such as the background color, the size and font of the words, their scaling and others. One interesting argument you can specify is the stopwords, which will remove words such as 'the', 'and', 'to', 'from', and so on. We will cover what stopwords are in detail in a later video. The result cloud_two_cities is a wordcloud object.\n",
    "\n",
    "```python\n",
    "cloud_two_cities = WordCloud().generate(two_cities)\n",
    "\n",
    "# To see all arguments of the function\n",
    "?WordCloud\n",
    "\n",
    "‚Ä¢ Background color\n",
    "\n",
    "‚Ä¢ Size and font of the words, scaling\n",
    "\n",
    "‚Ä¢ Stopwords\n",
    "\n",
    "# How does cloud_two_cities look like?\n",
    "cloud_two_cities\n",
    "<wordcloud.wordcloud.WordCloud at 0x2585f286d68>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Displaying the word cloud!\n",
    "------------------------------\n",
    "\n",
    "03:15¬†-¬†03:59\n",
    "\n",
    "If we want to display the generated word cloud object, we need to use some matplotlib functionality. We call plt.imshow(), specifying our cloud_two_cities as an argument. We also specify the interpolation to be bilinear. All this does is make the image appear more smoothly. The imshow() function thus creates the figure. We specify we don't want the image to display x and y axis, and finally, call the show method. The imshow() function has created the figure but we need to call show() to display it. We see the word cloud we generated on this piece of text. Which words pop out the most?\n",
    "\n",
    "```python\n",
    "plt.imshow(cloud_two_cities, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Word Cloud Visualization**\n",
    "\n",
    "```text\n",
    "       foolishness       evil\n",
    "          epoch         nois\n",
    "           us        short\n",
    "         period          hope\n",
    "   age         season      comparison\n",
    "            winter         belief\n",
    "             far             degree\n",
    "               times      incredulity\n",
    "             despair         Heaven\n",
    "               spring         worst\n",
    "               direct        superlative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:59¬†-¬†04:03\n",
    "\n",
    "Let's practice building different word clouds in the exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first word cloud\n",
    "=====================\n",
    "\n",
    "We saw in the video that word clouds are very intuitive and a great and fast way to get a first impression on what a piece of text is talking about.¬†\n",
    "\n",
    "In this exercise, you will build your first word cloud. A string¬†`east_of_eden`¬†has been defined for you. It contains one of the first sentences of John Steinbeck's novel¬†*East of Eden*. You can inspect its contents in the IPython Shell.¬†\n",
    "\n",
    "The¬†`matplotlib.pyplot`¬†package has been imported for you as¬†`plt`.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Import the required package to build a word cloud.¬†\n",
    "-   Generate a word cloud using the¬†`east_of_eden`¬†string. The background color has been specified as¬†`white`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate the word cloud from the east_of_eden string\n",
    "cloud_east_of_eden = WordCloud(background_color=\"white\").generate(east_of_eden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Create a figure from the word cloud object you generated in the previous step.\n",
    "-   Display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the word cloud from the east_of_eden string\n",
    "cloud_east_of_eden = WordCloud(background_color=\"white\").generate(east_of_eden)\n",
    "\n",
    "# Create a figure of the generated cloud\n",
    "plt.imshow(cloud_east_of_eden, interpolation='bilinear')  \n",
    "plt.axis('off')\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words are in the word cloud?\n",
    "==================================\n",
    "\n",
    "Let's continue with the word cloud exercises. A string called¬†`illuminated`¬†has been defined for you. It contains an emotionally charged quote from the book '*Everything is Illuminated*' by Jonathan Safran Foer. Generate and display a word cloud using the¬†`illuminated`¬†string. Note that all required packages have been imported for you.¬†\n",
    "\n",
    "Which of the following answers contains¬†**only**¬†words you see in the word cloud?\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "Sad, other, happy.\n",
    "\n",
    "Repeat, conversation , convince.\n",
    "\n",
    "[/] Day, worse, others.\n",
    "\n",
    "Know, thing, everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Cloud on movie reviews\n",
    "===========================\n",
    "\n",
    "You have been working with the¬†**movie reviews**dataset. You have explored the distribution of the reviews and have seen how long the longest and the shortest reviews are. But what do positive and negative reviews talk about?\n",
    "\n",
    "In this exercise, you will practice building a word cloud of the top 100 positive reviews.\n",
    "\n",
    "What are the words that pop up? Do they make sense to you?¬†\n",
    "\n",
    "The string¬†`descriptions`¬†has been created for you by concatenating the descriptions of the top 100 positive reviews. A movie-specific set of stopwords (very frequent words, such as¬†*the, a/an, and*, which will not be very informative and we'd like to exclude from the graph) is available as¬†`my_stopwords`. Recall that the¬†`interpolation`¬†argument makes the word cloud appear more smoothly.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the wordcloud function from the respective package.\n",
    "-   Apply the word cloud function to the¬†`descriptions`¬†string. Set the background color as 'white', and change the¬†`stopwords`argument.¬†\n",
    "-   Create a wordcloud image.¬†\n",
    "-   Finally, do not forget to display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function  \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create and generate a word cloud image \n",
    "my_cloud = WordCloud(background_color='white', stopwords=my_stopwords).generate(descriptions)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
