{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Bag-of-words\n",
    "----------------\n",
    "\n",
    "00:00 - 00:24\n",
    "\n",
    "Welcome to the next chapter of this course! We proceed on our journey by embarking on the first step in performing a sentiment analysis task: transforming our text data to numeric form. Why do we need to do that? A machine learning model cannot work with the text data directly, but rather with numeric features we create from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. What is a bag-of-words (BOW) ?\n",
    "----------------------------------\n",
    "\n",
    "00:24 - 00:47\n",
    "\n",
    "We start with a basic and crude, but often quite useful method, called bag-of-words (BOW). A bag-of-words approach describes the occurrence, or frequency, of words within a document, or a collection of documents (called corpus). It basically comes down to building a vocabulary of all the words occurring in the document and keeping track of their frequencies.\n",
    "\n",
    "• Describes the occurrence of words within a document or a collection of documents (corpus)\n",
    "\n",
    "• Builds a vocabulary of the words and a measure of their presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Amazon product reviews\n",
    "--------------------------\n",
    "\n",
    "00:47 - 01:10\n",
    "\n",
    "Before we continue with the discussion of BOW, we will introduce the data we will use throughout the chapter, namely reviews of Amazon products. The dataset consists of two columns: the first contains the score, which is 1 if positive and 0 if negative; The second column contains the actual review of the product.\n",
    "\n",
    "| score | review |\n",
    "|--------|---------|\n",
    "| 0 | 1 | Stuning even for the non-gamer. This sound tr... |\n",
    "| 1 | 1 | The best soundtrack ever to anything.: I'm re... |\n",
    "| 2 | 1 | Amazing!: This soundtrack is my favorite musi... |\n",
    "| 3 | 1 | Excellent Soundtrack: I truly like this sound... |\n",
    "| 4 | 1 | Remember, Pull Your Jaw Off The Floor After H... |\n",
    "| 5 | 1 | an absolute masterpiece: I am quite sure any ... |\n",
    "| 6 | 0 | Buyer beware: This is a self-published book, ... |\n",
    "| 7 | 1 | Glorious story: I loved Whisper of the wicked... |\n",
    "| 8 | 1 | A FIVE STAR BOOK: I just finished reading Whi... |\n",
    "| 9 | 1 | Whispers of the Wicked Saints: This was a eas... |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Sentiment analysis with BOW: Example\n",
    "----------------------------------------\n",
    "\n",
    "01:10 - 02:02\n",
    "\n",
    "Let's see how BOW would work applied to an example review. Imagine you have the following string: \"This is the best book ever. I loved the book and highly recommend it.\" The goal of a BOW approach would be to build the following dictionary-like output: 'This', occurs once in our string, so it has a count of 1, 'is' occurs once, 'the' occurs two times and so on. One thing to note is that we lose the word order and grammar rules, that's why this approach is called a 'bag' of words, resembling dropping a bunch of items in a bag and losing any sense of their order. This sounds straightforward but sometimes deciding how to build the vocabulary can be complex. We discuss some of the trade-offs we need to consider in later chapters.\n",
    "\n",
    "This is the best book ever. I loved the book and highly recommend it!!!\n",
    "\n",
    "```python\n",
    "{\n",
    "    'This': 1,\n",
    "    'is': 1,\n",
    "    'the': 2,\n",
    "    'best': 1,\n",
    "    'book': 2,\n",
    "    'ever': 1, \n",
    "    'I': 1,\n",
    "    'loved': 1,\n",
    "    'and': 1,\n",
    "    'highly': 1,\n",
    "    'recommend': 1,\n",
    "    'it': 1\n",
    "}\n",
    "```\n",
    "\n",
    "• Lose word order and grammar rules!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. BOW end result\n",
    "------------------\n",
    "\n",
    "02:02 - 02:18\n",
    "\n",
    "When we transform the text column with a BOW, the end result looks something like the table that we see: where the column is the word (also called token), and the row represents how many times we have encountered it in the respective review.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. CountVectorizer function\n",
    "----------------------------\n",
    "\n",
    "02:18 - 03:13\n",
    "\n",
    "How do we execute a BOW process in Python? The simplest way to do this is by using the CountVectorizer from the text library in the sklearn.feature_extraction submodule. In Python, we import the CountVectorizer() from sklearn.feature_extraction.text. In the CountVectorizer function, for the moment we leave the default functional options, except for the max_features argument, which only considers the features with highest term frequency, i.e. it will pick the 1000 most frequent words across the corpus of reviews. We need to do that sometimes for memory's sake. We use the `fit()` method from the CountVectorizer, calling fit() on our text column. To create a BOW representation, we call the transform() method, applied again to our text column.\n",
    "\n",
    "| | 10 | 100 | 12 | 15 | 1984 | 20 | 30 | 40 | 451 | 50 | ... | wrong | wrote | year | years | yes | yet | you | young | your | yourself |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
    "| 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 1 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
    "| 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 3 | 0 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. CountVectorizer output\n",
    "--------------------------\n",
    "\n",
    "03:13 - 03:27\n",
    "\n",
    "The result is a sparse matrix. A sparse matrix only stores entities that are non-zero, where the rows correspond to the number of rows in the dataset, and the columns to the BOW vocabulary.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(max_features=1000)\n",
    "vect.fit(data.review)\n",
    "X = vect.transform(data.review)\n",
    "```\n",
    "### CountVectorizer output\n",
    "```python\n",
    "X\n",
    "\n",
    "<10000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
    "    with 406668 stored elements in Compressed Sparse Row format>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Transforming the vectorizer\n",
    "-------------------------------\n",
    "\n",
    "03:27 - 03:53\n",
    "\n",
    "To look at the actual contents of a sparse matrix, we need to perform an additional step to transform it back to a 'dense' NumPy array, using the .toarray() method. We can build a pandas DataFrame from the array, where the columns' names are obtained from the `.get_feature_names()` method of the vectorizer. This returns a list where every entry corresponds to one feature.\n",
    "\n",
    "```python\n",
    "# Transform to an array \n",
    "my_array = X.toarray()\n",
    "\n",
    "# Transform back to a dataframe, assign column names\n",
    "X_df = pd.DataFrame(my_array, columns=vect.get_feature_names())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:53 - 03:59\n",
    "\n",
    "That was our introduction to BOW! Let's apply what we've learned in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which statement about BOW is true?\n",
    "==================================\n",
    "\n",
    "You were introduced to a bag-of-words(BOW) and some of its characteristics in the video. Which of the following statements about BOW **is** true?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   Bag-of-words preserves the word order and grammar rules.\n",
    "\n",
    "-   Bag-of-words describes the order and frequency of words or tokens within a corpus of documents.\n",
    "\n",
    "-   Bag-of-words is a simple but effective method to build a vocabulary of all the words occurring in a document.\n",
    "\n",
    "-   Bag-of-words can only be applied to a large document, not to shorter documents or single sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first BOW\n",
    "==============\n",
    "\n",
    "A bag-of-words is an approach to transform text to numeric form. \n",
    "\n",
    "In this exercise, you will apply a BOW to the `annak` list before moving on to a larger dataset in the next exercise. \n",
    "\n",
    "Your task will be to work with this list and apply a BOW using the `CountVectorizer()`. This transformation is your first step in being able to understand the sentiment of a text. Pay attention to words which might carry a strong sentiment. \n",
    "\n",
    "Remember that the output of a `CountVectorizer()` is a sparse matrix, which stores only entries which are non-zero. To look at the actual content of this matrix, we convert it to a dense array using the `.toarray()`method.\n",
    "\n",
    "Note that in this case you don't need to specify the `max_features` argument because the text is short.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the count vectorizer function from `sklearn.feature_extraction.text`.\n",
    "-   Build and fit the vectorizer on the small dataset.\n",
    "-   Create the BOW representation with name `anna_bow` by calling the `transform()`method.\n",
    "-   Print the BOW result as a dense array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Build the vectorizer and fit it\n",
    "anna_vect = CountVectorizer()\n",
    "anna_vect.fit(annak)\n",
    "\n",
    "# Create the bow representation\n",
    "anna_bow = anna_vect.transform(annak)\n",
    "\n",
    "# Print the bag-of-words result \n",
    "print(anna_bow.toarray())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW using product reviews\n",
    "=========================\n",
    "\n",
    "You practiced a BOW on a small dataset. Now you will apply it to a sample of Amazon product reviews. The data has been imported for you and is called `reviews`. It contains two columns. The first one is called `score` and it is `0` when the review is negative, and `1` when it is positive. The second column is called `review`and it contains the text of the review that a customer wrote. Feel free to explore the data in the IPython Shell.\n",
    "\n",
    "Your task is to build a BOW vocabulary, using the `review` column.\n",
    "\n",
    "Remember that we can call the `.get_feature_names()` method on the vectorizer to obtain a list of all the vocabulary elements.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create a CountVectorizer object, specifying the maximum number of features. \n",
    "-   Fit the vectorizer. \n",
    "-   Transform the fitted vectorizer.\n",
    "-   Create a DataFrame where you transform the sparse matrix to a dense array and make sure to correctly specify the names of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify max features \n",
    "vect = CountVectorizer(max_features=100)\n",
    "# Fit the vectorizer\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Getting granular with n-grams\n",
    "---------------------------------\n",
    "\n",
    "00:00 - 00:07\n",
    "\n",
    "You might remember from an earlier video that with a bag-of-words approach the word order is discarded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Context matters\n",
    "-------------------\n",
    "\n",
    "00:07 - 00:31\n",
    "\n",
    "Imagine you have a sentence such as 'I am happy, not sad' and another one 'I am sad, not happy'. They will have the same representation with a BOW, even though the meanings are inverted. In this case, putting NOT in front of the word (which is also called negation) changes the whole meaning and demonstrates why context is important.\n",
    "\n",
    "#### I am happy, not sad.\n",
    "\n",
    "#### I am sad, not happy.\n",
    "\n",
    "```\n",
    "Putting 'not' in front of a word (negation) is one example of how context matters.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Capturing context with a BOW\n",
    "--------------------------------\n",
    "\n",
    "00:31 - 01:01\n",
    "\n",
    "There is a way to capture the context when using a BOW by, for example, considering pairs or triples of tokens that appear next to each other. Let's define some terms. Single tokens are what we used so far and are also called 'unigrams'. Bigrams are pairs of tokens, trigrams are triples of tokens, and a sequence of n-tokens is called 'n-grams.'\n",
    "\n",
    "```\n",
    "Unigrams : single tokens\n",
    "Bigrams: pairs of tokens\n",
    "Trigrams: triples of tokens\n",
    "n - grams: sequence of n-tokens\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Capturing context with BOW\n",
    "------------------------------\n",
    "\n",
    "01:01 - 01:22\n",
    "\n",
    "Let's illustrate that with an example. Take the sentence 'The weather today is wonderful' and split it using unigrams, bigrams and trigrams. With unigrams we have single tokens, with bigrams, pairs of neighboring tokens, with trigrams: triples of neighboring tokens.\n",
    "\n",
    "```\n",
    "The weather today is wonderful.\n",
    "Unigrams : { The, weather, today, is, wonderful }\n",
    "Bigrams: {The weather, weather today, today is, is wonderful}\n",
    "Trigrams: {The weather today, weather today is, today is wonderful}\n",
    "```\n",
    "\n",
    "5\\. n-grams with the CountVectorizer\n",
    "------------------------------------\n",
    "\n",
    "01:22 - 01:56\n",
    "\n",
    "It is easy to implement n-grams with the CountVectorizer method. To specify the n-grams, we use the ngram_range parameter. The ngram_range is a tuple where the first parameter is the minimum length and the second parameter is the maximum length of tokens. For instance, ngram_range =(1, 1) means we will use only unigrams, (1, 2) means we will use unigrams and bigrams and so on.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(min_n, max_n))\n",
    "\n",
    "# Only unigrams\n",
    "ngram_range=(1, 1)\n",
    "\n",
    "# Uni- and bigrams  \n",
    "ngram_range=(1, 2)\n",
    "```\n",
    "\n",
    "6\\. What is the best n?\n",
    "-----------------------\n",
    "\n",
    "01:56 - 02:37\n",
    "\n",
    "It's not easy to determine what is the optimal sequence you should use for your problem. If we use longer token sequence, this will result in more features. In principle, the number of bigrams could be the number of unigrams squared; trigrams the number of unigrams to the power of 3 and so forth. In general, having longer sequences results in more precise machine learning models, but this also increases the risk of overfitting. An approach to find the optimal sequence length would be to try different lengths in something like a grid search and see which results in the best model.\n",
    "\n",
    "### Longer sequence of tokens\n",
    "\n",
    "```\n",
    "Results in more features\n",
    "Higher precision of machine learning models\n",
    "Risk of overing\n",
    "```\n",
    "\n",
    "7\\. Specifying vocabulary size\n",
    "------------------------------\n",
    "\n",
    "02:37 - 04:03\n",
    "\n",
    "Determining the length of token sequence is not the only way to determine the size of the vocabulary. There are a few parameters in the CountVectorizer that can also do that. You might remember we set the max_features parameter. The max_features can tell the CountVectorizer to take the top most frequent tokens in the corpus. If it is set to None, all the words in the corpus will be included. So this parameter can remove rare words, which depending on the context may or may not be a good idea. Another parameter you can specify is max_df. If given, it tells CountVectorizer to ignore terms with a higher than the given frequency. We can specify it as an integer - which will be an absolute count, or float - which will be a proportion. The default value of max_df is 1.0, which means it does not ignore any terms. Very similar to max_df is min_df. It is used to remove terms that appear too infrequently. It again can be specified either as an integer, in which case it will be a count, or a float, in which case it will be a proportion. The default value is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "```\n",
    "CountVectorizer(max_features, max_df, min_df)\n",
    "\n",
    "• max_features: if specified, it will include only the top most frequent words in the vocabulary\n",
    "  ○ If max_features = None, all words will be included\n",
    "\n",
    "• max_df: ignore terms with higher than specified frequency \n",
    "  ○ If it is set to integer, then absolute count; if a float, then it is a proportion\n",
    "  ○ Default is 1, which means it does not ignore any terms\n",
    "\n",
    "• min_df: ignore terms with lower than specified frequency\n",
    "  ○ If it is set to integer, then absolute count; if a float, then it is a proportion \n",
    "  ○ Default is 1, which means it does not ignore any terms\n",
    "```\n",
    "\n",
    "8\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:03 - 04:11\n",
    "\n",
    "Let's go to the exercises where you will specify the token sequence length and the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify token sequence length with BOW\n",
    "======================================\n",
    "\n",
    "We saw in the video that by specifying different length of tokens - what we called n-grams - we can better capture the context, which can be very important.\n",
    "\n",
    "In this exercise, you will work with a sample of the Amazon product reviews. Your task is to build a BOW vocabulary, using the `review`column and specify the sequence length of tokens.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Build the vectorizer, specifying the token sequence length to be uni- and bigrams.\n",
    "-   Fit the vectorizer.\n",
    "-   Transform the fitted vectorizer.\n",
    "-   In the DataFrame, make sure to correctly specify the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify token sequence and fit\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of vocabulary of movies reviews\n",
    "====================================\n",
    "\n",
    "In this exercise, you will practice different ways to limit the size of the vocabulary using a sample of the `movies` reviews dataset. The first column is the `review`, which is of type `object`and the second column is the `label`, which is `0` for a negative review and `1` for a positive one. \n",
    "\n",
    "The three methods that you will use will transform the text column to new numeric columns, capturing the count of a word or a phrase in each review. Each method will ultimately result in building a different number of new features.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "-   Using the `movies` dataset, limit the size of the vocabulary to  `100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify size of vocabulary and fit\n",
    "vect = CountVectorizer(max_features=100)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "----------------\n",
    "\n",
    "-   Using the `movies` dataset, limit the size of the vocabulary to include terms which occur in no more than 200 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(max_df=200)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "----------------\n",
    "\n",
    "-   Using the `movies` dataset, limit the size of the vocabulary to ignore terms which occur in less than 50 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(min_df=50)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW with n-grams and vocabulary size\n",
    "====================================\n",
    "\n",
    "In this exercise, you will practice building a bag-of-words once more, using the `reviews`dataset of Amazon product reviews. Your main task will be to limit the size of the vocabulary and specify the length of the token sequence.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the vectorizer from `sklearn`.\n",
    "-   Build the vectorizer and make sure to specify the following parameters: the size of the vocabulary should be limited to 1000, include only bigrams, and ignore terms that appear in more than 500 documents.\n",
    "-   Fit the vectorizer to the `review` column.\n",
    "-   Create a DataFrame from the BOW representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Build the vectorizer, specify max features and fit\n",
    "vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create a DataFrame from the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Build new features from text\n",
    "--------------------------------\n",
    "\n",
    "00:00 - 00:09\n",
    "\n",
    "When we have a sentiment analysis task, which we will solve with machine learning, having extra features usually results in a better model.\n",
    "\n",
    "2\\. Goal of the video\n",
    "---------------------\n",
    "\n",
    "00:09 - 00:18\n",
    "\n",
    "Our goal in this video is to enrich the dataset of choice with extra features related to the text capturing the sentiment.\n",
    "\n",
    "3\\. Product reviews data\n",
    "------------------------\n",
    "\n",
    "00:18 - 00:29\n",
    "\n",
    "We continue to work with the Amazon product reviews dataset. Remember that the first column contains the numeric score, and the second column - the review itself.\n",
    "\n",
    "```python\n",
    "reviews.head()\n",
    "```\n",
    "\n",
    "```\n",
    "            score                                               review\n",
    "0              1       Stuning even for the non-gamer. This sound tr...\n",
    "1              1       The best soundtrack ever to anything.: I'm re...\n",
    "2              1       Amazing!: This soundtrack is my favorite musi...\n",
    "3              1       Excellent Soundtrack: I truly like this sound...\n",
    "4              1       Remember, Pull Your Jaw Off The Floor After H...\n",
    "```\n",
    "\n",
    "4\\. Features from the review column\n",
    "-----------------------------------\n",
    "\n",
    "00:29 - 00:48\n",
    "\n",
    "In my own experience, some very predictive features say something about the complexity of the text column. For example, one could measure how long each review is, how many sentences it contains, or say something about the parts of speech involved, punctuation marks, etc.\n",
    "\n",
    "```\n",
    "How long is each review?\n",
    "How many sentences does it contain?\n",
    "What parts of speech are involved?\n",
    "How many punctuation marks?\n",
    "```\n",
    "\n",
    "5\\. Tokenizing a string\n",
    "-----------------------\n",
    "\n",
    "00:48 - 01:38\n",
    "\n",
    "Remember we employed a BOW approach to transform each review to numeric features, counting how many times a word occurred in the respective review. Here, we stop one step earlier and only split the reviews in individual words (usually called tokens, though a token can be a whole sentence as well.) We will work with the nltk package, and concretely the word_tokenize function. Let's apply the word_tokenize function to our familiar anna_k string. The returned result is a list, where each item is a token from the string. Note that not only words but also punctuation marks are originally assigned as tokens. The same would have been the case with digits, if we had any in our string.\n",
    "\n",
    "```python\n",
    "from nltk import word_tokenize\n",
    "anna_k = 'Happy families are all alike, every unhappy family is unhappy in its own way.'\n",
    "word_tokenize(anna_k)\n",
    "['Happy','families','are', 'all','alike',',',\n",
    "'every','unhappy', 'family', 'is','unhappy','in',\n",
    "'its','own','way','.']\n",
    "```\n",
    "\n",
    "6\\. Tokens from a column\n",
    "------------------------\n",
    "\n",
    "01:38 - 02:29\n",
    "\n",
    "Now we want to apply the same logic but to our column of reviews. One fast way to iterate over strings is by using list comprehension. A quick reminder on list comprehensions. They are like flattened-out for loops. The syntax is an operation we perform on each item in an iterable object (such as a list). In our case, a list comprehension will allow us to iterate over the review column, tokenizing every review. The result is going to be a list; if we explore the type of the first item, for example, we see it is also of type list. This means that our word_tokens is a list of lists. Each item stores the tokens from a single review.\n",
    "\n",
    "```python\n",
    "# General form of list comprehension\n",
    "[expression for item in iterable]\n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "type(word_tokens)\n",
    "list\n",
    "type(word_tokens[ 0 ])\n",
    "list\n",
    "```\n",
    "\n",
    "\n",
    "7\\. Tokens from a column\n",
    "------------------------\n",
    "\n",
    "02:29 - 03:16\n",
    "\n",
    "Now that we have our word_tokens list, we only need to count how many tokens there are in each item of word_tokens. We start by creating an empty list, to which we will append the length of each review as we iterate over the word_tokens list. In the first line of the for loop, we find the number of items in the word_tokens list using the len() function. Since we want to iterate over this number, we need to surround the len() by the the range() function. In the second line, we find the length of each iterable, and append that number to our empty list len_tokens. Lastly, we create a new feature for the length of each review.\n",
    "\n",
    "```python\n",
    "len_tokens = []\n",
    "# Iterate over the word_tokens list\n",
    "for i in range(len(word_tokens)):\n",
    "len_tokens.append(len(word_tokens[i]))\n",
    "# Create a new feature for the length of each review\n",
    "reviews['n_tokens'] = len_tokens\n",
    "```\n",
    "\n",
    "8\\. Dealing with punctuation\n",
    "----------------------------\n",
    "\n",
    "03:16 - 03:47\n",
    "\n",
    "Note that we did not address punctuation but you can exclude it if it suits your context better. You can even create a new feature that measures the number of punctuation signs. In our context, a review with more punctuation signs could signal a very emotionally charged opinion. It's also good to know that we can follow the same logic and create a feature that counts the number of sentences, where one token will be equal to a sentence and not to a single word.\n",
    "\n",
    "```\n",
    "We did not address it but you can exclude it\n",
    "A feature that measures the number of punctuation signs\n",
    "A review with many punctuation signs could signal a very emotionally charged opinion\n",
    "```\n",
    "\n",
    "9\\. Reviews with a feature for the length\n",
    "-----------------------------------------\n",
    "\n",
    "03:47 - 03:57\n",
    "\n",
    "If we check how the product reviews dataset looks like, we see the 'n_tokens' column we created. It shows the number of words in each review.\n",
    "\n",
    "```python\n",
    "reviews.head()\n",
    "```\n",
    "\n",
    "```\n",
    "            score                                               review  n_tokens\n",
    "0              1       Stuning even for the non-gamer. This sound tr...        87\n",
    "1              1       The best soundtrack ever to anything.: I'm re...       109\n",
    "2              1       Amazing!: This soundtrack is my favorite musi...       165\n",
    "3              1       Excellent Soundtrack: I truly like this sound...       145\n",
    "4              1       Remember, Pull Your Jaw Off The Floor After H...       109\n",
    "```\n",
    "\n",
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "03:57 - 04:01\n",
    "\n",
    "Let's solve some exercises to practice what we've learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize a string from GoT\n",
    "==========================\n",
    "\n",
    "A first standard step when working with text is to tokenize it, in other words, split a bigger string into individual strings, which are usually single words (tokens). \n",
    "\n",
    "A string `GoT` has been created for you and it contains a quote from George R.R. Martin's *Game of Thrones*. Your task is to split it into individual tokens.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the word tokenizing function from `nltk`.\n",
    "-   Transform the `GoT` string to word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Transform the GoT string to word tokens\n",
    "print(word_tokenize(GoT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokens from the Avengers\n",
    "=============================\n",
    "\n",
    "Now that you have tokenized your first string, it is time to iterate over items of a list and tokenize them as well. An easy way to do that with one line of code is with a list comprehension.\n",
    "\n",
    "A list `avengers` has been created for you. It contains a few quotes from the *Avengers*movies. You can explore it in the IPython Shell.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the required function and package.\n",
    "-   Apply the word tokenizing function on each item of our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenizing function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the avengers \n",
    "tokens_avengers = [word_tokenize(item) for item in avengers]\n",
    "\n",
    "print(tokens_avengers)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature for the length of a review\n",
    "====================================\n",
    "\n",
    "You have now worked with a string and a list with string items, it is time to use a larger sample of data.\n",
    "\n",
    "Your task in this exercise is to create a new feature for the length of a review, using the familiar `reviews` dataset.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Import the word tokenizing function from the required package.\n",
    "-   Apply the function to the `review` column of the `reviews` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed packages\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the review column \n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Print out the first item of the word_tokens list\n",
    "print(word_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Iterate over the created `word_tokens` list. \n",
    "-   As you iterate, find the length of each item in the list and append it to the empty `len_tokens` list. \n",
    "-   Create a new feature `n_words` in the `reviews` for the length of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the length of reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Can you guess the language?\n",
    "-------------------------------\n",
    "\n",
    "00:00 - 00:12\n",
    "\n",
    "Often in real applications not all documents carrying sentiment will be in English. We might want to detect what language is used or build specific features related to language.\n",
    "\n",
    "2\\. Language of a string in Python\n",
    "----------------------------------\n",
    "\n",
    "00:12 - 01:20\n",
    "\n",
    "In Python there are a few libraries that can detect the language of a string. In this course, we will use langdetect because it is one of the better performing packages. But you can follow the same structure using another package. We first import the detect_langs function from the langdetect package. Now imagine we have a string called foreign, which is a sentence in another language. Our goal is to identify its language. We apply the detect_langs function to our string. This function will return a list. Each item of the list contains a pair of a language and a number saying how likely it is that the string is in this particular language. In this case, we observe only 1 item in the list, namely Spanish. That's because the function is fairly certain the language is Spanish. In other cases we might get longer lists, where the most likely candidate languages will appear first, followed by less likely ones.\n",
    "\n",
    "```python\n",
    "from langdetect import detect_langs\n",
    "foreign = 'Este libro ha sido uno de los mejores libros que he leido.'\n",
    "detect_langs(foreign)\n",
    "[es:0.9999945352697024]\n",
    "```\n",
    "\n",
    "3\\. Language of a column\n",
    "------------------------\n",
    "\n",
    "01:20 - 01:51\n",
    "\n",
    "In real applications, we usually work not with a single string but with many strings, often contained in a column of a dataset. A common problem is to detect the language of each of the strings and capture the most likely language in a new column. How to do that? We again start by importing the detect_langs function from the langdetect package. We import our familiar dataset with product reviews.\n",
    "\n",
    "- Problem: Detect the language of each of the strings and capture the most likely language in\n",
    "a new column\n",
    "```python\n",
    "from langdetect import detect_langs\n",
    "reviews = pd.read_csv('product_reviews.csv')\n",
    "reviews.head()\n",
    "```\n",
    "\n",
    "```python\n",
    "reviews.head()\n",
    "```\n",
    "\n",
    "| score | review |\n",
    "|-------|---------|\n",
    "| 0 | 1 | Stuning even for the non-gamer. This sound tr... |\n",
    "| 1 | 1 | The best soundtrack ever to anything.: I'm re... |\n",
    "| 2 | 1 | Amazing!: This soundtrack is my favorite musi... |\n",
    "| 3 | 1 | Excellent Soundtrack: I truly like this sound... |\n",
    "| 4 | 1 | Remember, Pull Your Jaw Off The Floor After H... |\n",
    "\n",
    "4\\. Building a feature for the language\n",
    "---------------------------------------\n",
    "\n",
    "01:51 - 02:56\n",
    "\n",
    "The steps we follow next are quite similar to our approach when capturing the length of a review. First, we create an empty list, called languages. We want to iterate over the rows of our dataset using a for loop. In the first line of the loop, we apply the len() function to our dataset, which returns the number of rows. We still need to call the range() function since we want to iterate over the number of rows. In the second line of the loop, we apply the detect_lang function on the review column of the dataset, which is the second column in our case, while selecting one row at a time. We want to store each detected language as an item in a list, therefore we append the result of detect_langs to the empty list languages. When we print languages, we see that it is a list of lists, where each element contains the detected language of the respective row and how likely that language is. In some cases, the individual lists contain more than one item.\n",
    "\n",
    "```python\n",
    "languages = []\n",
    "for row in range(len(reviews)):\n",
    "languages.append(detect_langs(reviews.iloc[row, 1 ]))\n",
    "languages\n",
    "[it:0.9999982541301151],\n",
    "[es:0.9999954153640488],\n",
    "[es:0.7142833997345875, en:0.2857160465706441],\n",
    "[es:0.9999942365605781],\n",
    "[es:0.999997956049055] ...\n",
    "```\n",
    "\n",
    "5\\. Building a feature for the language\n",
    "---------------------------------------\n",
    "\n",
    "02:56 - 03:46\n",
    "\n",
    "We have one more step before we create our language feature. We saw that languages is a list of lists. We want to extract the first element of each list within languages since the first item is always the most likely language. One fast way to do that is by list comprehension. Let's break down the command in steps. For example, let's take the first element of the languages and split it on a colon sign. After that, we extract the first element of the resulting split, returning '[es'. Finally,since there is a left bracket before the language, we select everything from the 2nd element onwards, resulting in 'es' for Spanish.\n",
    "\n",
    "```python\n",
    "# Transform the first list to a string and split on a colon\n",
    "str(languages[ 0 ]).split(':')\n",
    "['[es', '0.9999954153640488]']\n",
    "str(languages[ 0 ]).split(':')[ 0 ]\n",
    "'[es'\n",
    "str(languages[ 0 ]).split(':')[ 0 ][ 1 :]\n",
    "'es'\n",
    "```\n",
    "\n",
    "6\\. Building a feature for the language\n",
    "---------------------------------------\n",
    "\n",
    "03:46 - 03:59\n",
    "\n",
    "To write the list comprehension, we put these steps together by iterating over each item in our list of lists. Lastly, we assign the cleaned list to a new feature, called language.\n",
    "\n",
    "```python\n",
    "languages = [str(lang).split(':')[ 0 ][ 1 :] for lang in languages]\n",
    "reviews['language'] = languages\n",
    "```\n",
    "\n",
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:59 - 04:05\n",
    "\n",
    "I know this is a lot of code but the exercises will help you digest it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the language of a string\n",
    "=================================\n",
    "\n",
    "Sometimes you might need to analyze the sentiment of non-English text. Your first task in such a case will be to identify the foreign language. \n",
    "\n",
    "In this exercise you will identify the language of a single string. A string called `foreign` has been created for you. It has been randomly extracted from the `reviews` dataset and may contain some grammatical errors. Feel free to explore it in the IPython Shell.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the required function from the language detection package.\n",
    "-   Detect the language of the `foreign` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the language detection function and package\n",
    "from langdetect import detect_langs\n",
    "\n",
    "# Detect the language of the foreign string\n",
    "print(detect_langs(foreign))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect language of a list of strings\n",
    "====================================\n",
    "\n",
    "Now you will detect the language of each item in a list. A list called `sentences` has been created for you and it contains 3 sentences, each in a different language. They have been randomly extracted from the product reviews dataset.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Iterate over the sentences in the list.\n",
    "-   Detect the language of each sentence and append the detected language to the empty list `languages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "\n",
    "languages = []\n",
    "\n",
    "# Loop over the sentences in the list and detect their language\n",
    "for sentence in sentences:\n",
    "    languages.append(detect_langs(sentence))\n",
    "    \n",
    "print('The detected languages are: ', languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language detection of product reviews\n",
    "=====================================\n",
    "\n",
    "You will practice language detection on a small dataset called `non_english_reviews`. It is a sample of non-English reviews from the Amazon product reviews. \n",
    "\n",
    "You will iterate over the rows of the dataset, detecting the language of each row and appending it to an empty list. The list needs to be cleaned so that it only contains the language of the review such as `'en'` for English instead of the regular output `en:0.9987654`. Remember that the language detection function might detect more than one language and the first item in the returned list is the most likely candidate. Finally, you will assign the list to a new column. \n",
    "\n",
    "The logic is the same as used in the slides and the exercise before but instead of applying the function to a list, you work with a dataset.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Iterate over the rows of the `non_english_reviews` dataset. \n",
    "-   Inside the loop, detect the language of the second column of the dataset.\n",
    "-   Clean the string by splitting on a `:` inside the list comprehension expression.\n",
    "-   Finally, assign the cleaned list to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "languages = [] \n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "for row in range(len(non_english_reviews)):\n",
    "    languages.append(detect_langs(non_english_reviews.iloc[row, 1]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "print(languages)\n",
    "\n",
    "# Assign the list to a new feature \n",
    "non_english_reviews['language'] = languages\n",
    "\n",
    "print(non_english_reviews.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
