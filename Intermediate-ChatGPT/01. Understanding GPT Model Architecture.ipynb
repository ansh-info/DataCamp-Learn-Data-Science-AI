{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6988cd51",
   "metadata": {},
   "source": [
    "1\\. Introduction\n",
    "----------------\n",
    "\n",
    "00:00 - 00:09\n",
    "\n",
    "Welcome to this course on Intermediate ChatGPT. I'm Alex Banks, your guide to help you unlock the full potential of ChatGPT for creative exploration.\n",
    "\n",
    "2\\. ChatGPT's explosive growth\n",
    "------------------------------\n",
    "\n",
    "00:09 - 00:17\n",
    "\n",
    "OpenAI released ChatGPT on November 30, 2022. In just five days, it had 1 million users.\n",
    "\n",
    "1.  1 Images created using DALLE-3\n",
    "\n",
    "3\\. ChatGPT's explosive growth\n",
    "------------------------------\n",
    "\n",
    "00:17 - 00:31\n",
    "\n",
    "After two months, this number grew to 100 million, making it the fastest-growing consumer application at the time. This staggering growth beat both TikTok and Instagram to hit the 100 million benchmark.\n",
    "\n",
    "4\\. ChatGPT's explosive growth\n",
    "------------------------------\n",
    "\n",
    "00:31 - 00:42\n",
    "\n",
    "But what catalyzed this viral growth? To understand what made generative AI go mainstream, we must first understand what makes ChatGPT remarkable --- large language models.\n",
    "\n",
    "5\\. A primer on Large Language Models (LLMs)\n",
    "--------------------------------------------\n",
    "\n",
    "00:42 - 00:57\n",
    "\n",
    "Large language models simply predict the next word in a sequence of words. For example, if we have a string of words: \"the cat sat on a\", what could the next word be? Well, with a 97% likelihood, the model predicts it will be \"mat.\"\n",
    "\n",
    "6\\. A primer on Large Language Models (LLMs)\n",
    "--------------------------------------------\n",
    "\n",
    "00:57 - 01:19\n",
    "\n",
    "The standard training for LLMs involves forecasting the next word in a sequence of words. The LLM prediction is compared to the actual words in the text until it can generate accurate responses. This is known as next-token prediction, predicting the next word in a sequence, and masked-language modeling, predicting the masked word in the middle.\n",
    "\n",
    "7\\. The dawn of AI: Recurrent models\n",
    "------------------------------------\n",
    "\n",
    "01:19 - 01:55\n",
    "\n",
    "Next-token prediction and masked-language modeling are both common training tasks for recurrent neural networks or RNNs. RNNs gained popularity in the 1990s and early 2000s because they can process data one piece at a time and remember what they have seen. But when sequences get really long, recurrent models struggle to maintain relevance and context. They use a type of memory called a \"hidden state,\" which can only retain information for a limited amount of time. This limitation, known as the vanishing gradient problem, hampered their effectiveness in complex tasks.\n",
    "\n",
    "8\\. Groundbreaking architecture: The transformer\n",
    "------------------------------------------------\n",
    "\n",
    "01:55 - 02:43\n",
    "\n",
    "The limitations of RNNs set the stage for a revolution. In 2017, a group of researchers at Google unveiled a groundbreaking architecture that would reshape the landscape of natural language processing or NLP. The paper, \"Attention is All You Need,\" introduced the world to transformer architecture, a groundbreaking approach to redefining NLP. Unlike RNNs, transformers do not process data sequentially. Instead, they use attention mechanisms to weigh the significance of each word in a sequence, irrespective of its position. This ability to examine all parts of the input data simultaneously allowed for improvements in processing speed and efficiency, enabling the processing of longer text and eventually ChatGPT.\n",
    "\n",
    "9\\. Welcome to Intermediate ChatGPT!\n",
    "------------------------------------\n",
    "\n",
    "02:43 - 03:00\n",
    "\n",
    "Now that you've had a taste of the technology that underpins ChatGPT let me introduce myself properly. I'm Alex Banks. I've been building and scaling AI products since 2021. I also write Sunday Signal, a newsletter that covers AI highlights and broader insights that intrigue and inspire.\n",
    "\n",
    "10\\. Welcome to Intermediate ChatGPT\n",
    "------------------------------------\n",
    "\n",
    "03:00 - 03:11\n",
    "\n",
    "At the conclusion of these three chapters, you'll be armed with the tools and techniques to impress your friends around the dinner table by not only describing the technology behind ChatGPT which you'll learn in Chapter 1...\n",
    "\n",
    "11\\. Welcome to Intermediate ChatGPT\n",
    "------------------------------------\n",
    "\n",
    "03:11 - 03:23\n",
    "\n",
    "...but also an arsenal of prompting strategies you'll learn in chapter 2, to elicit specific, accurate, and creative responses from ChatGPT across exciting and very practical use cases.\n",
    "\n",
    "12\\. Welcome to Intermediate ChatGPT\n",
    "------------------------------------\n",
    "\n",
    "03:23 - 03:38\n",
    "\n",
    "Chapter 3 will look at the advanced functions of ChatGPT, including writing custom instructions and building your very own GPTs. Let's embark on this voyage together, unraveling the mysteries of AI, which powers so much of our digital world today.\n",
    "\n",
    "13\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "03:38 - 03:43\n",
    "\n",
    "Now, it's time to get hands-on in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c69f847",
   "metadata": {},
   "source": [
    "Advancements of transformer architecture\n",
    "========================================\n",
    "\n",
    "The transformer architecture, introduced by Vaswani et al. in 2017, brought significant advancements over the previously used Recurrent Neural Networks (RNNs). \n",
    "\n",
    "What key advancement does the transformer architecture provide over RNNs?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "-   Sequential data processing for better memory retention.\n",
    "\n",
    "[x] -   Attention mechanisms that weigh the significance of each word, irrespective of its position.\n",
    "\n",
    "-   Enhanced ability to handle short text sequences.\n",
    "\n",
    "-   Reduced need for large training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6d663",
   "metadata": {},
   "source": [
    "#### Features of RNNs versus transformers\n",
    "\n",
    "When comparing different types of neural network architectures, it's important to understand the distinctive features and advantages of each. Recurrent Neural Networks (RNNs) and Transformer models are two such architectures, each with their own unique characteristics and applications. \n",
    "\n",
    "##### Instructions\n",
    "\n",
    "-   Classify the following features into those typical of Recurrent Neural Networks (RNNs) and those of Transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30009231",
   "metadata": {},
   "source": [
    "### RNNs\n",
    "\n",
    "* Processes data sequentially.\n",
    "* Uses hidden states to retain memory.\n",
    "* Suffers from the vanishing gradient problem.\n",
    "\n",
    "### Transformers\n",
    "\n",
    "* Processes all parts of input data simultaneously.\n",
    "* More efficient at processing longer sequences of text.\n",
    "* Uses attention mechanisms to weigh the significance of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451c102",
   "metadata": {},
   "source": [
    "1\\. Tokenization and Transformers\n",
    "---------------------------------\n",
    "\n",
    "00:00 - 00:01\n",
    "\n",
    "Welcome back!\n",
    "\n",
    "2\\. Tokenization\n",
    "----------------\n",
    "\n",
    "00:01 - 00:11\n",
    "\n",
    "Tokens are the building blocks of LLMs like ChatGPT. Understanding how tokenization works is essential to grasping the inner workings of the Transformer architecture.\n",
    "\n",
    "3\\. Tokenization\n",
    "----------------\n",
    "\n",
    "00:11 - 00:32\n",
    "\n",
    "Consider the following sentence from The Terminator: \"I'll be back.\" This sentence contains 5 tokens and 13 characters. Tokenization transforms the words into something computers can understand. Firstly, syntax. This is the structure of sentences. Secondly, semantics. This is the meaning conveyed by the sentences.\n",
    "\n",
    "4\\. Encoding\n",
    "------------\n",
    "\n",
    "00:32 - 00:52\n",
    "\n",
    "When you input text into a language model, the first step is to convert it into tokens. Tokens are numerical representations of pieces of text. Each token is assigned a unique identifier called a token ID. In our example, \"I'll be back\" is split into 5 tokens, each with a corresponding ID.\n",
    "\n",
    "5\\. Vectorization\n",
    "-----------------\n",
    "\n",
    "00:52 - 01:03\n",
    "\n",
    "Once we have token IDs, the next step is to convert these IDs into vectors, which are numerical arrays that capture the semantic meaning of the tokens. This process is called embedding.\n",
    "\n",
    "6\\. Decoding\n",
    "------------\n",
    "\n",
    "01:03 - 01:18\n",
    "\n",
    "After processing and generating a response, the model needs to convert the numerical data back into human-readable text. This is where decoding comes into play. Decoding translates the generated sequence of token IDs back into words and sentences.\n",
    "\n",
    "7\\. Self-attention\n",
    "------------------\n",
    "\n",
    "01:18 - 01:46\n",
    "\n",
    "The Transformer uses self-attention, sometimes referred to as intra-attention, to compute a representation of its input and output without using sequence-aligned RNNs. Self-attention allows the model to weigh the importance of each token in the input sequence relative to all other tokens. This mechanism enables the model to capture relationships and dependencies between tokens, regardless of their distance from each other in the sequence.\n",
    "\n",
    "8\\. Self-attention\n",
    "------------------\n",
    "\n",
    "01:46 - 02:04\n",
    "\n",
    "For example, in the sentence \"The cat sat on the mat because it was tired,\" the word \"it\" refers to \"the cat.\" Self-attention helps the model recognize that \"it\" is linked to \"the cat,\" ensuring the context is preserved when generating text or translating languages.\n",
    "\n",
    "9\\. Token relationships\n",
    "-----------------------\n",
    "\n",
    "02:04 - 02:53\n",
    "\n",
    "In the sentence \"The cat sat on the mat because it was tired,\" the model compares each token with every other token in the sequence. For instance: \"it\" vs. \"the cat\": The model evaluates the relevance of \"it\" to \"the cat\" and determines that \"it\" refers to \"the cat.\" \"it\" vs. \"sat\": The model assesses the relationship between \"it\" and \"sat,\" but finds a lower relevance because \"it\" is more likely to refer to a noun (in this case, \"the cat\") than to a verb. \"tired\" vs. \"the cat\": The model considers how \"tired\" relates to \"the cat\" and establishes that \"tired\" is a state attributed to \"the cat.\" Through these comparisons, the model assigns attention scores that quantify the importance of each token in relation to others.\n",
    "\n",
    "10\\. Contextual understanding\n",
    "-----------------------------\n",
    "\n",
    "02:53 - 03:10\n",
    "\n",
    "With the attention scores calculated, the model focuses on the most relevant tokens to build contextual understanding. Because the attention score between \"it\" and \"the cat\" is high, the model understands that \"it\" is referring to \"the cat,\" not any other noun.\n",
    "\n",
    "11\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "03:10 - 03:24\n",
    "\n",
    "Now that you've learned about the Transformer architecture and the fundamental concepts of tokenization, encoding, vectorization, decoding, and self-attention, it's time to put your knowledge into practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a75326",
   "metadata": {},
   "source": [
    "The role of decoders\n",
    "====================\n",
    "\n",
    "In the Transformer architecture, encoding and decoding are two fundamental processes. Understanding these processes is crucial for tasks like language translation and text generation.\n",
    "\n",
    "What is the primary function of the **decoder** in the Transformer architecture?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "-   To map an input sequence of tokens to a sequence of continuous representations.\n",
    "\n",
    "[x] -   To convert a sequence of continuous representations back into an output sequence.\n",
    "\n",
    "-   To assign unique token IDs to pieces of text.\n",
    "\n",
    "-   To compute numerical arrays that capture the semantic meaning of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda41a5",
   "metadata": {},
   "source": [
    "#### Encoding versus Decoding\n",
    "\n",
    "In this exercise, you will classify steps into either \"Encoding\" or \"Decoding.\"\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "Classify the following steps into either \"Encoding\" or \"Decoding\":\n",
    "\n",
    "-   Converting numerical data back into human-readable text\n",
    "-   Mapping an input sequence of tokens to a sequence of continuous representations\n",
    "-   Assigning unique identifiers called token IDs\n",
    "-   Translating generated sequence of token IDs back into words and sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa606326",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "* Mapping an input sequence of tokens to a sequence of continuous representations\n",
    "* Assigning unique identifiers called token IDs\n",
    "\n",
    "### Decoding\n",
    "\n",
    "* Converting numerical data back into human-readable text\n",
    "* Translating generated sequence of token IDs back into words and sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIAgents4Pharma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
