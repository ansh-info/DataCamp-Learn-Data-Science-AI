{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Named Entity Recognition\n",
    "----------------------------\n",
    "\n",
    "00:00 - 00:05\n",
    "\n",
    "Welcome to our first video on named entity recognition!\n",
    "\n",
    "2\\. What is Named Entity Recognition?\n",
    "-------------------------------------\n",
    "\n",
    "00:05 - 00:40\n",
    "\n",
    "Named Entity Recognition or NER for short is a natural language processing task used to identify important named entities in the text -- such as people, places and organizations -- they can even be dates, states, works of art and other categories depending on the libraries and notation you use. NER can be used alongside topic identification, or on its own to determine important items in a text or answer basic natural language understanding questions such as who? what? when and where?\n",
    "\n",
    "3\\. Example of NER\n",
    "------------------\n",
    "\n",
    "00:40 - 01:30\n",
    "\n",
    "For example, take this piece of text which is from the English Wikipedia article on Albert Einstein. The text has been highlighted for different types of named entities that were found using the Stanford NER library. You can see the dates, locations, persons and organizations found and extract infomation on the text based on these named entities. You can use NER to solve problems like fact extraction as well as which entities are related using computational language models. For example, in this text we can see that Einstein has something to do with the United States, Adolf Hitler and Germany. We can also see by token proximity that Betrand Russel and Einstein created the Russel-Einstein manifesto -- all from simple entity highlighting.\n",
    "\n",
    "4\\. nltk and the Stanford CoreNLP Library\n",
    "-----------------------------------------\n",
    "\n",
    "01:30 - 02:20\n",
    "\n",
    "NLTK allows you to interact with named entity recognition via it's own model, but also the aforementioned Stanford library. The Stanford library integration requires you to perform a few steps before you can use it, including installing the required Java files and setting system environment variables. You can also use the standford library on its own without integrating it with NLTK or operate it as an API server. The stanford CoreNLP library has great support for named entity recognition as well as some related nlp tasks such as coreference (or linking pronouns and entities together) and dependency trees to help with parsing meaning and relationships amongst words or phrases in a sentence.\n",
    "\n",
    "5\\. Using nltk for Named Entity Recognition\n",
    "-------------------------------------------\n",
    "\n",
    "02:20 - 02:49\n",
    "\n",
    "For our simple use case, we will use the built-in named entity recognition with NLTK. To do so, we take a normal sentence, and preprocess it via tokenization. Then, we can tag the sentence for parts of speech. This will add tags for proper nouns, pronouns, adjective, verbs and other part of speech that NLTK uses based on an english grammar. When we take a look at the tags, we see New and York are tagged NNP which is the tag for a proper noun, singular.\n",
    "\n",
    "6\\. nltk's ne_chunk()\n",
    "---------------------\n",
    "\n",
    "02:49 - 03:31\n",
    "\n",
    "Then we pass this tagged sentence into the ne_chunk function, or named entity chunk, which will return the sentence as a tree. NLTK Tree's might look a bit different than trees you might use in other libraries, but they do have leaves and subtrees representing more complex grammar. This tree shows the named entities tagged as their own chunks such as GPE or geopolitical entity for New York, or MOMA and Metro as organizations. It also identifies Ruth Reichl as a person. It does so without consulting a knowledge base, like wikipedia, but instead uses trained statistical and grammatical parsers.\n",
    "\n",
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:31 - 03:37\n",
    "\n",
    "Now it's your turn to practice some named entity recognition using nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER with NLTK\n",
    "=============\n",
    "\n",
    "You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use `nltk` to find the named entities in this article. \n",
    "\n",
    "What might the article be about, given the names you found?\n",
    "\n",
    "Along with `nltk`, `sent_tokenize` and `word_tokenize` from `nltk.tokenize` have been pre-imported.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Tokenize `article` into sentences.\n",
    "-   Tokenize each sentence in `sentences` into words using a list comprehension.\n",
    "-   Inside a list comprehension, tag each tokenized sentence into parts of speech using `nltk.pos_tag()`.\n",
    "-   Chunk each tagged sentence into named-entity chunks using `nltk.ne_chunk_sents()`. Along with `pos_sentences`, specify the additional keyword argument `binary=True`.\n",
    "-   Loop over each sentence and each chunk, and test whether it is a named-entity chunk by testing if it has the attribute `label`, and if the `chunk.label()` is equal to `\"NE\"`. If so, print that chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charting practice\n",
    "=================\n",
    "\n",
    "In this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n",
    "\n",
    "You'll use a `defaultdict` called `ner_categories`, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called `chunked_sentences` similar to the last exercise, but this time with non-binary category names.\n",
    "\n",
    "You can use `hasattr()` to determine if each chunk has a `'label'` and then simply use the chunk's `.label()` method as the dictionary key.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "-   Create a `defaultdict` called `ner_categories`, with the default type set to `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "----------------\n",
    "\n",
    "-   Fill up the dictionary with values for each of the keys. Remember, the keys will represent the `label()`.\n",
    "    -   In the outer `for` loop, iterate over `chunked_sentences`, using `sent` as your iterator variable.\n",
    "    -   In the inner `for` loop, iterate over `sent`. If the condition is true, increment the value of each key by 1. \n",
    "    -   *Remember to use the chunk's `.label()`method as the key!*\n",
    "-   For the pie chart labels, create a list called `labels` from the keys of `ner_categories`, which can be accessed using `.keys()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "----------------\n",
    "\n",
    "-   Use a list comprehension to create a list called `values`, using the `.get()` method on `ner_categories` to compute the values of each label `v`.\n",
    "-   Use `plt.pie()` to create a pie chart for each of the NER categories. Along with `values` and `labels=labels`, pass the extra keyword arguments `autopct='%1.1f%%'` and `startangle=140`to add percentages to the chart and rotate the initial start angle. \n",
    "    -   *This step has been done for you.*\n",
    "-   Display your pie chart. Was the distribution what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford library with NLTK\n",
    "==========================\n",
    "\n",
    "When using the Stanford library with NLTK, what is needed to get started?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   A normal installation of NLTK.\n",
    "\n",
    "-   An installation of the Stanford Java Library.\n",
    "\n",
    "-   Both NLTK and an installation of the Stanford Java Library.\n",
    "\n",
    "-   NLTK, the Stanford Java Libraries and some environment variables to help with integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Introduction to SpaCy\n",
    "-------------------------\n",
    "\n",
    "00:00 - 00:07\n",
    "\n",
    "In this video, we'll take a look at SpaCy, another great library for natural language processing.\n",
    "\n",
    "2\\. What is SpaCy?\n",
    "------------------\n",
    "\n",
    "00:07 - 00:31\n",
    "\n",
    "SpaCy is a NLP library similar to Gensim, but with different implementations, including a particular focus on creating NLP pipelines to generate models and corpora. SpaCy is open-source and has several extra libraries and tools built by the same team, including Displacy - a visualization tool for viewing parse trees which uses Node-js to create interactive text.\n",
    "\n",
    "3\\. Displacy entity recognition visualizer\n",
    "------------------------------------------\n",
    "\n",
    "00:31 - 00:55\n",
    "\n",
    "For example, if we use the displacy entity recognition visualizer which has a live demo online, we can enter the sentence used in the last video. Here, we can see the SpaCy has identified three named entities and tagged them with the appropriate entity label -- such as location or person. SpaCy also has tools to build word and document vectors from text.\n",
    "\n",
    "4\\. SpaCy NER\n",
    "-------------\n",
    "\n",
    "00:55 - 02:19\n",
    "\n",
    "To start using spacy for Named entity recognition, we must first install it and download all the appropriate pre-trained word vectors. You can also train vectors yourself and load them; but the pretrained ones let us get started immediately. We can load those into an object, NLP, which functions similarly to our Gensim dictionary and corpus. It has several linked objects, including entity which is an Entity Recognizer object from the pipeline module. This is what is used to find entities in the text. Then we load a new document by passing a string into the NLP variable. When the document is loaded, the named entities are stored as a document attribute called ents. We see Spacy properly tagged and identified the three main entities in the sentence. We can also investigate the labels of each entity by using indexing to pick out the first entity and the label_ attribute to see the label for that particular entity. Here we see the label for Berlin is GPE or Geopolitical entity. Spacy has several other language models available, including advanced German and Chinese implementations. It's a great tool especially if you want to build your own extraction and natural language processing pipeline quickly and iteratively.\n",
    "\n",
    "5\\. Why use SpaCy for NER?\n",
    "--------------------------\n",
    "\n",
    "02:19 - 02:49\n",
    "\n",
    "Why use Spacy for NER? Outside of being able to integrate with the other great Spacy features like easy pipeline creation, it has a different set of entity types and often labels entities differently than nltk. In addition, Spacy comes with informal language corpora, allowing you to more easily find entities in documents like Tweets and chat messages. It's a quickly growing library, so it might even have more languages supported by the time you are watching this video!\n",
    "\n",
    "6\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "02:49 - 02:55\n",
    "\n",
    "For now, however, you can get started using Spacy for named entity recognition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing NLTK with spaCy NER\n",
    "=============================\n",
    "\n",
    "Using the same text you used in the first exercise of this chapter, you'll now see the results using spaCy's NER annotator. How will they compare?\n",
    "\n",
    "The article has been pre-loaded as `article`. To minimize execution times, you'll be asked to specify the keyword argument `disable=['tagger', 'parser', 'matcher']`when loading the spaCy model, because you only care about the `entity` in this exercise.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import `spacy`.\n",
    "-   Load the `'en_core_web_sm'` model using `spacy.load()`. Specify the additional keyword arguments `disable=['tagger', 'parser', 'matcher']`.\n",
    "-   Create a `spacy` document object by passing `article` into `nlp()`.\n",
    "-   Using `ent` as your iterator variable, iterate over the entities of `doc` and print out the labels (`ent.label_`) and text (`ent.text`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'matcher'])\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy NER Categories\n",
    "====================\n",
    "\n",
    "Which are the *extra* categories that `spacy` uses compared to `nltk` in its named-entity recognition?\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "GPE, PERSON, MONEY\n",
    "\n",
    "ORGANIZATION, WORK*OF*ART\n",
    "\n",
    "[/] NORP, CARDINAL, MONEY, WORK*OF*ART, LANGUAGE, EVENT\n",
    "\n",
    "EVENT_LOCATION, FIGURE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
