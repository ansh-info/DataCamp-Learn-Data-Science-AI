{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Named Entity Recognition\n",
    "----------------------------\n",
    "\n",
    "00:00 - 00:05\n",
    "\n",
    "Welcome to our first video on named entity recognition!\n",
    "\n",
    "2\\. What is Named Entity Recognition?\n",
    "-------------------------------------\n",
    "\n",
    "00:05 - 00:40\n",
    "\n",
    "Named Entity Recognition or NER for short is a natural language processing task used to identify important named entities in the text -- such as people, places and organizations -- they can even be dates, states, works of art and other categories depending on the libraries and notation you use. NER can be used alongside topic identification, or on its own to determine important items in a text or answer basic natural language understanding questions such as who? what? when and where?\n",
    "\n",
    "3\\. Example of NER\n",
    "------------------\n",
    "\n",
    "00:40 - 01:30\n",
    "\n",
    "For example, take this piece of text which is from the English Wikipedia article on Albert Einstein. The text has been highlighted for different types of named entities that were found using the Stanford NER library. You can see the dates, locations, persons and organizations found and extract infomation on the text based on these named entities. You can use NER to solve problems like fact extraction as well as which entities are related using computational language models. For example, in this text we can see that Einstein has something to do with the United States, Adolf Hitler and Germany. We can also see by token proximity that Betrand Russel and Einstein created the Russel-Einstein manifesto -- all from simple entity highlighting.\n",
    "\n",
    "4\\. nltk and the Stanford CoreNLP Library\n",
    "-----------------------------------------\n",
    "\n",
    "01:30 - 02:20\n",
    "\n",
    "NLTK allows you to interact with named entity recognition via it's own model, but also the aforementioned Stanford library. The Stanford library integration requires you to perform a few steps before you can use it, including installing the required Java files and setting system environment variables. You can also use the standford library on its own without integrating it with NLTK or operate it as an API server. The stanford CoreNLP library has great support for named entity recognition as well as some related nlp tasks such as coreference (or linking pronouns and entities together) and dependency trees to help with parsing meaning and relationships amongst words or phrases in a sentence.\n",
    "\n",
    "5\\. Using nltk for Named Entity Recognition\n",
    "-------------------------------------------\n",
    "\n",
    "02:20 - 02:49\n",
    "\n",
    "For our simple use case, we will use the built-in named entity recognition with NLTK. To do so, we take a normal sentence, and preprocess it via tokenization. Then, we can tag the sentence for parts of speech. This will add tags for proper nouns, pronouns, adjective, verbs and other part of speech that NLTK uses based on an english grammar. When we take a look at the tags, we see New and York are tagged NNP which is the tag for a proper noun, singular.\n",
    "\n",
    "6\\. nltk's ne_chunk()\n",
    "---------------------\n",
    "\n",
    "02:49 - 03:31\n",
    "\n",
    "Then we pass this tagged sentence into the ne_chunk function, or named entity chunk, which will return the sentence as a tree. NLTK Tree's might look a bit different than trees you might use in other libraries, but they do have leaves and subtrees representing more complex grammar. This tree shows the named entities tagged as their own chunks such as GPE or geopolitical entity for New York, or MOMA and Metro as organizations. It also identifies Ruth Reichl as a person. It does so without consulting a knowledge base, like wikipedia, but instead uses trained statistical and grammatical parsers.\n",
    "\n",
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:31 - 03:37\n",
    "\n",
    "Now it's your turn to practice some named entity recognition using nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER with NLTK\n",
    "=============\n",
    "\n",
    "You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use `nltk` to find the named entities in this article. \n",
    "\n",
    "What might the article be about, given the names you found?\n",
    "\n",
    "Along with `nltk`, `sent_tokenize` and `word_tokenize` from `nltk.tokenize` have been pre-imported.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Tokenize `article` into sentences.\n",
    "-   Tokenize each sentence in `sentences` into words using a list comprehension.\n",
    "-   Inside a list comprehension, tag each tokenized sentence into parts of speech using `nltk.pos_tag()`.\n",
    "-   Chunk each tagged sentence into named-entity chunks using `nltk.ne_chunk_sents()`. Along with `pos_sentences`, specify the additional keyword argument `binary=True`.\n",
    "-   Loop over each sentence and each chunk, and test whether it is a named-entity chunk by testing if it has the attribute `label`, and if the `chunk.label()` is equal to `\"NE\"`. If so, print that chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charting practice\n",
    "=================\n",
    "\n",
    "In this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n",
    "\n",
    "You'll use a `defaultdict` called `ner_categories`, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called `chunked_sentences` similar to the last exercise, but this time with non-binary category names.\n",
    "\n",
    "You can use `hasattr()` to determine if each chunk has a `'label'` and then simply use the chunk's `.label()` method as the dictionary key.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "-   Create a `defaultdict` called `ner_categories`, with the default type set to `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "----------------\n",
    "\n",
    "-   Fill up the dictionary with values for each of the keys. Remember, the keys will represent the `label()`.\n",
    "    -   In the outer `for` loop, iterate over `chunked_sentences`, using `sent` as your iterator variable.\n",
    "    -   In the inner `for` loop, iterate over `sent`. If the condition is true, increment the value of each key by 1. \n",
    "    -   *Remember to use the chunk's `.label()`method as the key!*\n",
    "-   For the pie chart labels, create a list called `labels` from the keys of `ner_categories`, which can be accessed using `.keys()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "----------------\n",
    "\n",
    "-   Use a list comprehension to create a list called `values`, using the `.get()` method on `ner_categories` to compute the values of each label `v`.\n",
    "-   Use `plt.pie()` to create a pie chart for each of the NER categories. Along with `values` and `labels=labels`, pass the extra keyword arguments `autopct='%1.1f%%'` and `startangle=140`to add percentages to the chart and rotate the initial start angle. \n",
    "    -   *This step has been done for you.*\n",
    "-   Display your pie chart. Was the distribution what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford library with NLTK\n",
    "==========================\n",
    "\n",
    "When using the Stanford library with NLTK, what is needed to get started?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   A normal installation of NLTK.\n",
    "\n",
    "-   An installation of the Stanford Java Library.\n",
    "\n",
    "-   Both NLTK and an installation of the Stanford Java Library.\n",
    "\n",
    "-   NLTK, the Stanford Java Libraries and some environment variables to help with integration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
