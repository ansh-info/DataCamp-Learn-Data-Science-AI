{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Word counts with bag-of-words\n",
    "---------------------------------\n",
    "\n",
    "00:00 - 00:08\n",
    "\n",
    "Welcome to chapter two! We'll begin with using word counts with a bag of words approach.\n",
    "\n",
    "2\\. Bag-of-words\n",
    "----------------\n",
    "\n",
    "00:08 - 00:35\n",
    "\n",
    "Bag of words is a very simple and basic method to finding topics in a text. For bag of words, you need to first create tokens using tokenization, and then count up all the tokens you have. The theory is that the more frequent a word or token is, the more central or important it might be to the text. Bag of words can be a great way to determine the significant words in a text based on the number of times they are used.\n",
    "\n",
    "3\\. Bag-of-words example\n",
    "------------------------\n",
    "\n",
    "00:35 - 01:17\n",
    "\n",
    "Here we see an example series of sentences, mainly about a cat and a box. If we just us a simple bag of words model with tokenization like we learned in chapter one and remove the punctuation, we can see the example result. Box, cat, The and the are some of the most important words because they are the most frequent. Notice that the word THE appears twice in the bag of words, once with uppercase and once lowercase. If we added a preprocessing step to handle this issue, we could lowercase all of the words in the text so each word is counted only once.\n",
    "\n",
    "4\\. Bag-of-words in Python\n",
    "--------------------------\n",
    "\n",
    "01:17 - 02:18\n",
    "\n",
    "We can use the NLP fundamentals we already know, such as tokenization with NLTK to create a list of tokens. We will use a new class called Counter which we import from the standard library module collections. The list of tokens generated using word_tokenize can be passed as the initialization argument for the Counter class. The result is a counter object which has similar structure to a dictionary and allows us to see each token and the frequency of the token. Counter objects also have a method called `most_common`, which takes an integer argument, such as 2 and would then return the top 2 tokens in terms of frequency. The return object is a series of tuples inside a list. For each tuple, the first element holds the token and the second element represents the frequency. Note: other than ordering by token frequency, the most_common method does not sort the tokens it returns or tell us there are more tokens with that same frequency.\n",
    "\n",
    "5\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "02:18 - 02:25\n",
    "\n",
    "Now you know a bit about bag of words and can get started building your own using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words picker\n",
    "===================\n",
    "\n",
    "It's time for a quick check on your understanding of bag-of-words. Which of the below options, with basic `nltk` tokenization, map the bag-of-words for the following text?\n",
    "\n",
    "\"The cat is in the box. The cat box.\"\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "50 XP\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "('the', 3), ('box.', 2), ('cat', 2), ('is', 1)\n",
    "\n",
    "('The', 3), ('box', 2), ('cat', 2), ('is', 1), ('in', 1), ('.', 1)\n",
    "\n",
    "('the', 3), ('cat box', 1), ('cat', 1), ('box', 1), ('is', 1), ('in', 1)\n",
    "\n",
    "[/] ('The', 2), ('box', 2), ('.', 2), ('cat', 2), ('is', 1), ('in', 1), ('the', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Counter with bag-of-words\n",
    "====================================\n",
    "\n",
    "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as `article`. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as `article_title`. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.\n",
    "\n",
    "`word_tokenize` has been imported for you.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "100 XP\n",
    "\n",
    "-   Import `Counter` from `collections`.\n",
    "-   Use `word_tokenize()` to split the article into tokens.\n",
    "-   Use a list comprehension with `t` as the iterator variable to convert all the tokens into lowercase. The `.lower()` method converts text into lowercase.\n",
    "-   Create a bag-of-words counter called `bow_simple` by using `Counter()` with `lower_tokens` as the argument.\n",
    "-   Use the `.most_common()` method of `bow_simple` to print the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Simple text preprocessing\n",
    "-----------------------------\n",
    "\n",
    "00:00 - 00:06\n",
    "\n",
    "In this video, we will cover some simple text preprocessing.\n",
    "\n",
    "2\\. Why preprocess?\n",
    "-------------------\n",
    "\n",
    "00:06 - 01:06\n",
    "\n",
    "Text processing helps make for better input data when performing machine learning or other statistical methods. For example, in the last few exercises you have applied small bits of preprocessing (like tokenization) to create a bag of words. You also noticed that applying simple techniques like lowercasing all of the tokens, can lead to slightly better results for a bag-of-words model. Preprocessing steps like tokenization or lowercasing words are commonly used in NLP. Other common techniques are things like lemmatization or stemming, where you shorten the words to their root stems, or techniques like removing stop words, which are common words in a language that don't carry a lot of meaning -- such as and or the, or removing punctuation or unwanted tokens. Of course, each model and process will have different results -- so it's good to try a few different approaches to preprocessing and see which works best for your task and goal.\n",
    "\n",
    "3\\. Preprocessing example\n",
    "-------------------------\n",
    "\n",
    "01:06 - 01:31\n",
    "\n",
    "We have here some example input and output text we might expect from preprocessing. First we have a simple two sentence string about pets. Then we have some example output tokens we want. You can see that the text has been tokenized and that everything is lowercase. We also notice that stopwords have been removed and the plural nouns have been made singular.\n",
    "\n",
    "4\\. Text preprocessing with Python\n",
    "----------------------------------\n",
    "\n",
    "01:31 - 02:53\n",
    "\n",
    "We can perform text preprocessing using many of the tools we already know and have learned. In this code, we are using the same text as from our previous video, a few sentences about a cat with a box. We can use list comprehensions to tokenize the sentences which we first make lowercase using the string lower method. The string is_alpha method will return True if the string has only alphabetical characters. We use the is_alpha method along with an if statement iterating over our tokenized result to only return only alphabetic strings (this will effectively strip tokens with numbers or punctuation). To read out the process in both code and English we say we take each token from the word_tokenize output of the lowercase text if it contains only alphabetical characters. In the next line, we use another list comprehension to remove words that are in the stopwords list. This stopwords list for english comes built in with the NLTK library. Finally, we can create a counter and check the two most common words, which are now cat and box (unlike the and box which were the two tokens returned in our first result). Preprocessing has already improved our bag of words and made it more useful by removing the stopwords and non-alphabetic words.\n",
    "\n",
    "5\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "02:53 - 02:59\n",
    "\n",
    "You can now get started by preprocessing your own text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing steps\n",
    "========================\n",
    "\n",
    "Which of the following are useful text preprocessing steps?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "50XP\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   Stems, spelling corrections, lowercase.\n",
    "\n",
    "-   Lemmatization, lowercasing, removing unwanted tokens.\n",
    "\n",
    "-   Removing stop words, leaving in capital words.\n",
    "\n",
    "-   Strip stop words, word endings and digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing practice\n",
    "===========================\n",
    "\n",
    "Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n",
    "\n",
    "You start with the same tokens you created in the last exercise: `lower_tokens`. You also have the `Counter` class imported.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "100 XP\n",
    "\n",
    "-   Import the `WordNetLemmatizer` class from `nltk.stem`. \n",
    "-   Create a list `alpha_only` that contains **only** alphabetical characters. You can use the `.isalpha()` method to check for this.\n",
    "-   Create another list called `no_stops` consisting of words from `alpha_only` that **are not** contained in `english_stops`.\n",
    "-   Initialize a `WordNetLemmatizer` object called `wordnet_lemmatizer` and use its `.lemmatize()`method on the tokens in `no_stops` to create a new list called `lemmatized`.\n",
    "-   Create a new `Counter` called `bow` with the lemmatized words.\n",
    "-   Lastly, print the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Introduction to gensim\n",
    "--------------------------\n",
    "\n",
    "00:00 - 00:07\n",
    "\n",
    "In this video, we will get started using a new tool called Gensim.\n",
    "\n",
    "2\\. What is gensim?\n",
    "-------------------\n",
    "\n",
    "00:07 - 00:25\n",
    "\n",
    "**Gensim** is a popular open-source natural language processing library. It uses top academic models to perform complex tasks like building document or word vectors, corpora and performing topic identification and document comparisons.\n",
    "\n",
    "3\\. What is a word vector?\n",
    "--------------------------\n",
    "\n",
    "00:25 - 01:19\n",
    "\n",
    "You might be wondering what a word or document vector is? Here are some examples here in visual form. A word embedding or vector is trained from a larger corpus and is a multi-dimensional representation of a word or document. You can think of it as a multi-dimensional array normally with sparse features (lots of zeros and some ones). With these vectors, we can then see relationships among the words or documents based on how near or far they are and also what similar comparisons we find. For example, in this graphic we can see that the vector operation king minus queen is approximately equal to man minus woman. Or that Spain is to Madrid as Italy is to Rome. The deep learning algorithm used to create word vectors has been able to distill this meaning based on how those words are used throughout the text.\n",
    "\n",
    "4\\. Gensim example\n",
    "------------------\n",
    "\n",
    "01:19 - 01:46\n",
    "\n",
    "The graphic we have here is an example of LDA visualization. LDA stands for latent dirichlet allocation, and it is a statistical model we can apply to text using Gensim for topic analysis and modelling. This graph is just a portion of a blog post written in 2015 using Gensim to analyze US presidential addresses. The article is really neat and you can find the link here.\n",
    "\n",
    "5\\. Creating a gensim dictionary\n",
    "--------------------------------\n",
    "\n",
    "01:46 - 02:51\n",
    "\n",
    "Gensim allows you to build corpora and dictionaries using simple classes and functions. A corpus (or if plural, corpora) is a set of texts used to help perform natural language processing tasks. Here, our documents are a list of strings that look like movie reviews about space or sci-fi films. First we need to do some basic preprocessing. For brevity, we will only tokenize and lowercase. For better results, we would want to apply more of the preprocessing we have learned in this chapter, such as removing punctuation and stop words. Then we can pass the tokenized documents to the Gensim Dictionary class. This will create a mapping with an id for each token. This is the beginning of our corpus. We now can represent whole documents using just a list of their token ids and how often those tokens appear in each document. We can take a look at the tokens and their ids by looking at the token2id attribute, which is a dictionary of all of our tokens and their respective ids in our new dictionary.\n",
    "\n",
    "6\\. Creating a gensim corpus\n",
    "----------------------------\n",
    "\n",
    "02:51 - 03:59\n",
    "\n",
    "Using the dictionary we built in the last slide, we can then create a Gensim corpus. This is a bit different than a normal corpus -- which is just a collection of documents. Gensim uses a simple bag-of-words model which transforms each document into a bag of words using the token ids and the frequency of each token in the document. Here, we can see that the Gensim corpus is a list of lists, each list item representing one document. Each document a series of tuples, the first item representing the tokenid from the dictionary and the second item representing the token frequency in the document. In only a few lines, we have a new bag-of-words model and corpus thanks to Gensim. And unlike our previous Counter-based bag of words, this Gensim model can be easily saved, updated and reused thanks to the extra tools we have available in Gensim. Our dictionary can also be updated with new texts and extract only words that meet particular thresholds. We are building a more advanced and feature-rich bag-of-words model which can then be used for future exercises.\n",
    "\n",
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:59 - 04:04\n",
    "\n",
    "Now you can get started building your own dictionary with Gensim!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are word vectors?\n",
    "======================\n",
    "\n",
    "What are word vectors and how do they help with NLP?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   They are similar to bags of words, just with numbers. You use them to count how many tokens there are.\n",
    "\n",
    "-   Word vectors are sparse arrays representing bigrams in the corpora. You can use them to compare two sets of words to one another.\n",
    "\n",
    "-   Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus.\n",
    "\n",
    "-   Word vectors don't actually help NLP and are just hype."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
