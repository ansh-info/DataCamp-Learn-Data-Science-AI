{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Bag-of-words\n",
    "----------------\n",
    "\n",
    "00:00 - 00:24\n",
    "\n",
    "Welcome to the next chapter of this course! We proceed on our journey by embarking on the first step in performing a sentiment analysis task: transforming our text data to numeric form. Why do we need to do that? A machine learning model cannot work with the text data directly, but rather with numeric features we create from the data.\n",
    "\n",
    "2\\. What is a bag-of-words (BOW) ?\n",
    "----------------------------------\n",
    "\n",
    "00:24 - 00:47\n",
    "\n",
    "We start with a basic and crude, but often quite useful method, called bag-of-words (BOW). A bag-of-words approach describes the occurrence, or frequency, of words within a document, or a collection of documents (called corpus). It basically comes down to building a vocabulary of all the words occurring in the document and keeping track of their frequencies.\n",
    "\n",
    "3\\. Amazon product reviews\n",
    "--------------------------\n",
    "\n",
    "00:47 - 01:10\n",
    "\n",
    "Before we continue with the discussion of BOW, we will introduce the data we will use throughout the chapter, namely reviews of Amazon products. The dataset consists of two columns: the first contains the score, which is 1 if positive and 0 if negative; The second column contains the actual review of the product.\n",
    "\n",
    "4\\. Sentiment analysis with BOW: Example\n",
    "----------------------------------------\n",
    "\n",
    "01:10 - 02:02\n",
    "\n",
    "Let's see how BOW would work applied to an example review. Imagine you have the following string: \"This is the best book ever. I loved the book and highly recommend it.\" The goal of a BOW approach would be to build the following dictionary-like output: 'This', occurs once in our string, so it has a count of 1, 'is' occurs once, 'the' occurs two times and so on. One thing to note is that we lose the word order and grammar rules, that's why this approach is called a 'bag' of words, resembling dropping a bunch of items in a bag and losing any sense of their order. This sounds straightforward but sometimes deciding how to build the vocabulary can be complex. We discuss some of the trade-offs we need to consider in later chapters.\n",
    "\n",
    "5\\. BOW end result\n",
    "------------------\n",
    "\n",
    "02:02 - 02:18\n",
    "\n",
    "When we transform the text column with a BOW, the end result looks something like the table that we see: where the column is the word (also called token), and the row represents how many times we have encountered it in the respective review.\n",
    "\n",
    "6\\. CountVectorizer function\n",
    "----------------------------\n",
    "\n",
    "02:18 - 03:13\n",
    "\n",
    "How do we execute a BOW process in Python? The simplest way to do this is by using the CountVectorizer from the text library in the sklearn.feature_extraction submodule. In Python, we import the CountVectorizer() from sklearn.feature_extraction.text. In the CountVectorizer function, for the moment we leave the default functional options, except for the max_features argument, which only considers the features with highest term frequency, i.e. it will pick the 1000 most frequent words across the corpus of reviews. We need to do that sometimes for memory's sake. We use the `fit()` method from the CountVectorizer, calling fit() on our text column. To create a BOW representation, we call the transform() method, applied again to our text column.\n",
    "\n",
    "7\\. CountVectorizer output\n",
    "--------------------------\n",
    "\n",
    "03:13 - 03:27\n",
    "\n",
    "The result is a sparse matrix. A sparse matrix only stores entities that are non-zero, where the rows correspond to the number of rows in the dataset, and the columns to the BOW vocabulary.\n",
    "\n",
    "8\\. Transforming the vectorizer\n",
    "-------------------------------\n",
    "\n",
    "03:27 - 03:53\n",
    "\n",
    "To look at the actual contents of a sparse matrix, we need to perform an additional step to transform it back to a 'dense' NumPy array, using the .toarray() method. We can build a pandas DataFrame from the array, where the columns' names are obtained from the `.get_feature_names()` method of the vectorizer. This returns a list where every entry corresponds to one feature.\n",
    "\n",
    "9\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:53 - 03:59\n",
    "\n",
    "That was our introduction to BOW! Let's apply what we've learned in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which statement about BOW is true?\n",
    "==================================\n",
    "\n",
    "You were introduced to a bag-of-words(BOW) and some of its characteristics in the video. Which of the following statements about BOW **is** true?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   Bag-of-words preserves the word order and grammar rules.\n",
    "\n",
    "-   Bag-of-words describes the order and frequency of words or tokens within a corpus of documents.\n",
    "\n",
    "-   Bag-of-words is a simple but effective method to build a vocabulary of all the words occurring in a document.\n",
    "\n",
    "-   Bag-of-words can only be applied to a large document, not to shorter documents or single sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first BOW\n",
    "==============\n",
    "\n",
    "A bag-of-words is an approach to transform text to numeric form. \n",
    "\n",
    "In this exercise, you will apply a BOW to the `annak` list before moving on to a larger dataset in the next exercise. \n",
    "\n",
    "Your task will be to work with this list and apply a BOW using the `CountVectorizer()`. This transformation is your first step in being able to understand the sentiment of a text. Pay attention to words which might carry a strong sentiment. \n",
    "\n",
    "Remember that the output of a `CountVectorizer()` is a sparse matrix, which stores only entries which are non-zero. To look at the actual content of this matrix, we convert it to a dense array using the `.toarray()`method.\n",
    "\n",
    "Note that in this case you don't need to specify the `max_features` argument because the text is short.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the count vectorizer function from `sklearn.feature_extraction.text`.\n",
    "-   Build and fit the vectorizer on the small dataset.\n",
    "-   Create the BOW representation with name `anna_bow` by calling the `transform()`method.\n",
    "-   Print the BOW result as a dense array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Build the vectorizer and fit it\n",
    "anna_vect = CountVectorizer()\n",
    "anna_vect.fit(annak)\n",
    "\n",
    "# Create the bow representation\n",
    "anna_bow = anna_vect.transform(annak)\n",
    "\n",
    "# Print the bag-of-words result \n",
    "print(anna_bow.toarray())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW using product reviews\n",
    "=========================\n",
    "\n",
    "You practiced a BOW on a small dataset. Now you will apply it to a sample of Amazon product reviews. The data has been imported for you and is called `reviews`. It contains two columns. The first one is called `score` and it is `0` when the review is negative, and `1` when it is positive. The second column is called `review`and it contains the text of the review that a customer wrote. Feel free to explore the data in the IPython Shell.\n",
    "\n",
    "Your task is to build a BOW vocabulary, using the `review` column.\n",
    "\n",
    "Remember that we can call the `.get_feature_names()` method on the vectorizer to obtain a list of all the vocabulary elements.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create a CountVectorizer object, specifying the maximum number of features. \n",
    "-   Fit the vectorizer. \n",
    "-   Transform the fitted vectorizer.\n",
    "-   Create a DataFrame where you transform the sparse matrix to a dense array and make sure to correctly specify the names of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify max features \n",
    "vect = CountVectorizer(max_features=100)\n",
    "# Fit the vectorizer\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Getting granular with n-grams\n",
    "---------------------------------\n",
    "\n",
    "00:00 - 00:07\n",
    "\n",
    "You might remember from an earlier video that with a bag-of-words approach the word order is discarded.\n",
    "\n",
    "2\\. Context matters\n",
    "-------------------\n",
    "\n",
    "00:07 - 00:31\n",
    "\n",
    "Imagine you have a sentence such as 'I am happy, not sad' and another one 'I am sad, not happy'. They will have the same representation with a BOW, even though the meanings are inverted. In this case, putting NOT in front of the word (which is also called negation) changes the whole meaning and demonstrates why context is important.\n",
    "\n",
    "3\\. Capturing context with a BOW\n",
    "--------------------------------\n",
    "\n",
    "00:31 - 01:01\n",
    "\n",
    "There is a way to capture the context when using a BOW by, for example, considering pairs or triples of tokens that appear next to each other. Let's define some terms. Single tokens are what we used so far and are also called 'unigrams'. Bigrams are pairs of tokens, trigrams are triples of tokens, and a sequence of n-tokens is called 'n-grams.'\n",
    "\n",
    "4\\. Capturing context with BOW\n",
    "------------------------------\n",
    "\n",
    "01:01 - 01:22\n",
    "\n",
    "Let's illustrate that with an example. Take the sentence 'The weather today is wonderful' and split it using unigrams, bigrams and trigrams. With unigrams we have single tokens, with bigrams, pairs of neighboring tokens, with trigrams: triples of neighboring tokens.\n",
    "\n",
    "5\\. n-grams with the CountVectorizer\n",
    "------------------------------------\n",
    "\n",
    "01:22 - 01:56\n",
    "\n",
    "It is easy to implement n-grams with the CountVectorizer method. To specify the n-grams, we use the ngram_range parameter. The ngram_range is a tuple where the first parameter is the minimum length and the second parameter is the maximum length of tokens. For instance, ngram_range =(1, 1) means we will use only unigrams, (1, 2) means we will use unigrams and bigrams and so on.\n",
    "\n",
    "6\\. What is the best n?\n",
    "-----------------------\n",
    "\n",
    "01:56 - 02:37\n",
    "\n",
    "It's not easy to determine what is the optimal sequence you should use for your problem. If we use longer token sequence, this will result in more features. In principle, the number of bigrams could be the number of unigrams squared; trigrams the number of unigrams to the power of 3 and so forth. In general, having longer sequences results in more precise machine learning models, but this also increases the risk of overfitting. An approach to find the optimal sequence length would be to try different lengths in something like a grid search and see which results in the best model.\n",
    "\n",
    "7\\. Specifying vocabulary size\n",
    "------------------------------\n",
    "\n",
    "02:37 - 04:03\n",
    "\n",
    "Determining the length of token sequence is not the only way to determine the size of the vocabulary. There are a few parameters in the CountVectorizer that can also do that. You might remember we set the max_features parameter. The max_features can tell the CountVectorizer to take the top most frequent tokens in the corpus. If it is set to None, all the words in the corpus will be included. So this parameter can remove rare words, which depending on the context may or may not be a good idea. Another parameter you can specify is max_df. If given, it tells CountVectorizer to ignore terms with a higher than the given frequency. We can specify it as an integer - which will be an absolute count, or float - which will be a proportion. The default value of max_df is 1.0, which means it does not ignore any terms. Very similar to max_df is min_df. It is used to remove terms that appear too infrequently. It again can be specified either as an integer, in which case it will be a count, or a float, in which case it will be a proportion. The default value is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "8\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:03 - 04:11\n",
    "\n",
    "Let's go to the exercises where you will specify the token sequence length and the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify token sequence length with BOW\n",
    "======================================\n",
    "\n",
    "We saw in the video that by specifying different length of tokens - what we called n-grams - we can better capture the context, which can be very important.\n",
    "\n",
    "In this exercise, you will work with a sample of the Amazon product reviews. Your task is to build a BOW vocabulary, using the `review`column and specify the sequence length of tokens.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Build the vectorizer, specifying the token sequence length to be uni- and bigrams.\n",
    "-   Fit the vectorizer.\n",
    "-   Transform the fitted vectorizer.\n",
    "-   In the DataFrame, make sure to correctly specify the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify token sequence and fit\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of vocabulary of movies reviews\n",
    "====================================\n",
    "\n",
    "In this exercise, you will practice different ways to limit the size of the vocabulary using a sample of the `movies` reviews dataset. The first column is the `review`, which is of type `object`and the second column is the `label`, which is `0` for a negative review and `1` for a positive one. \n",
    "\n",
    "The three methods that you will use will transform the text column to new numeric columns, capturing the count of a word or a phrase in each review. Each method will ultimately result in building a different number of new features.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "-   Using the `movies` dataset, limit the size of the vocabulary to  `100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify size of vocabulary and fit\n",
    "vect = CountVectorizer(max_features=100)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "----------------\n",
    "\n",
    "-   Using the `movies` dataset, limit the size of the vocabulary to include terms which occur in no more than 200 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(max_df=200)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "----------------\n",
    "\n",
    "-   Using the `movies` dataset, limit the size of the vocabulary to ignore terms which occur in less than 50 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(min_df=50)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW with n-grams and vocabulary size\n",
    "====================================\n",
    "\n",
    "In this exercise, you will practice building a bag-of-words once more, using the `reviews`dataset of Amazon product reviews. Your main task will be to limit the size of the vocabulary and specify the length of the token sequence.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the vectorizer from `sklearn`.\n",
    "-   Build the vectorizer and make sure to specify the following parameters: the size of the vocabulary should be limited to 1000, include only bigrams, and ignore terms that appear in more than 500 documents.\n",
    "-   Fit the vectorizer to the `review` column.\n",
    "-   Create a DataFrame from the BOW representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Build the vectorizer, specify max features and fit\n",
    "vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create a DataFrame from the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Build new features from text\n",
    "--------------------------------\n",
    "\n",
    "00:00 - 00:09\n",
    "\n",
    "When we have a sentiment analysis task, which we will solve with machine learning, having extra features usually results in a better model.\n",
    "\n",
    "2\\. Goal of the video\n",
    "---------------------\n",
    "\n",
    "00:09 - 00:18\n",
    "\n",
    "Our goal in this video is to enrich the dataset of choice with extra features related to the text capturing the sentiment.\n",
    "\n",
    "3\\. Product reviews data\n",
    "------------------------\n",
    "\n",
    "00:18 - 00:29\n",
    "\n",
    "We continue to work with the Amazon product reviews dataset. Remember that the first column contains the numeric score, and the second column - the review itself.\n",
    "\n",
    "4\\. Features from the review column\n",
    "-----------------------------------\n",
    "\n",
    "00:29 - 00:48\n",
    "\n",
    "In my own experience, some very predictive features say something about the complexity of the text column. For example, one could measure how long each review is, how many sentences it contains, or say something about the parts of speech involved, punctuation marks, etc.\n",
    "\n",
    "5\\. Tokenizing a string\n",
    "-----------------------\n",
    "\n",
    "00:48 - 01:38\n",
    "\n",
    "Remember we employed a BOW approach to transform each review to numeric features, counting how many times a word occurred in the respective review. Here, we stop one step earlier and only split the reviews in individual words (usually called tokens, though a token can be a whole sentence as well.) We will work with the nltk package, and concretely the word_tokenize function. Let's apply the word_tokenize function to our familiar anna_k string. The returned result is a list, where each item is a token from the string. Note that not only words but also punctuation marks are originally assigned as tokens. The same would have been the case with digits, if we had any in our string.\n",
    "\n",
    "6\\. Tokens from a column\n",
    "------------------------\n",
    "\n",
    "01:38 - 02:29\n",
    "\n",
    "Now we want to apply the same logic but to our column of reviews. One fast way to iterate over strings is by using list comprehension. A quick reminder on list comprehensions. They are like flattened-out for loops. The syntax is an operation we perform on each item in an iterable object (such as a list). In our case, a list comprehension will allow us to iterate over the review column, tokenizing every review. The result is going to be a list; if we explore the type of the first item, for example, we see it is also of type list. This means that our word_tokens is a list of lists. Each item stores the tokens from a single review.\n",
    "\n",
    "7\\. Tokens from a column\n",
    "------------------------\n",
    "\n",
    "02:29 - 03:16\n",
    "\n",
    "Now that we have our word_tokens list, we only need to count how many tokens there are in each item of word_tokens. We start by creating an empty list, to which we will append the length of each review as we iterate over the word_tokens list. In the first line of the for loop, we find the number of items in the word_tokens list using the len() function. Since we want to iterate over this number, we need to surround the len() by the the range() function. In the second line, we find the length of each iterable, and append that number to our empty list len_tokens. Lastly, we create a new feature for the length of each review.\n",
    "\n",
    "8\\. Dealing with punctuation\n",
    "----------------------------\n",
    "\n",
    "03:16 - 03:47\n",
    "\n",
    "Note that we did not address punctuation but you can exclude it if it suits your context better. You can even create a new feature that measures the number of punctuation signs. In our context, a review with more punctuation signs could signal a very emotionally charged opinion. It's also good to know that we can follow the same logic and create a feature that counts the number of sentences, where one token will be equal to a sentence and not to a single word.\n",
    "\n",
    "9\\. Reviews with a feature for the length\n",
    "-----------------------------------------\n",
    "\n",
    "03:47 - 03:57\n",
    "\n",
    "If we check how the product reviews dataset looks like, we see the 'n_tokens' column we created. It shows the number of words in each review.\n",
    "\n",
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "03:57 - 04:01\n",
    "\n",
    "Let's solve some exercises to practice what we've learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize a string from GoT\n",
    "==========================\n",
    "\n",
    "A first standard step when working with text is to tokenize it, in other words, split a bigger string into individual strings, which are usually single words (tokens). \n",
    "\n",
    "A string `GoT` has been created for you and it contains a quote from George R.R. Martin's *Game of Thrones*. Your task is to split it into individual tokens.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the word tokenizing function from `nltk`.\n",
    "-   Transform the `GoT` string to word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Transform the GoT string to word tokens\n",
    "print(word_tokenize(GoT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokens from the Avengers\n",
    "=============================\n",
    "\n",
    "Now that you have tokenized your first string, it is time to iterate over items of a list and tokenize them as well. An easy way to do that with one line of code is with a list comprehension.\n",
    "\n",
    "A list `avengers` has been created for you. It contains a few quotes from the *Avengers*movies. You can explore it in the IPython Shell.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the required function and package.\n",
    "-   Apply the word tokenizing function on each item of our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenizing function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the avengers \n",
    "tokens_avengers = [word_tokenize(item) for item in avengers]\n",
    "\n",
    "print(tokens_avengers)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature for the length of a review\n",
    "====================================\n",
    "\n",
    "You have now worked with a string and a list with string items, it is time to use a larger sample of data.\n",
    "\n",
    "Your task in this exercise is to create a new feature for the length of a review, using the familiar `reviews` dataset.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Import the word tokenizing function from the required package.\n",
    "-   Apply the function to the `review` column of the `reviews` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed packages\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the review column \n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Print out the first item of the word_tokens list\n",
    "print(word_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Iterate over the created `word_tokens` list. \n",
    "-   As you iterate, find the length of each item in the list and append it to the empty `len_tokens` list. \n",
    "-   Create a new feature `n_words` in the `reviews` for the length of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the length of reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
