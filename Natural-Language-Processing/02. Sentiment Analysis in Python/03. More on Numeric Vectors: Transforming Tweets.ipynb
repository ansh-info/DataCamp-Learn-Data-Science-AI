{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Stop words\n",
    "--------------\n",
    "\n",
    "00:00 - 00:11\n",
    "\n",
    "In every language, there are words that occur too frequently and are not very informative. Sometimes, it is useful to get rid of them before we build a machine learning model.\n",
    "\n",
    "2\\. What are stop words and how to find them?\n",
    "---------------------------------------------\n",
    "\n",
    "00:11 - 01:00\n",
    "\n",
    "Words that occur too frequently and are not very informative are called stop words. But how do we know which words are not informative? In every language, there is a set of words that most practitioners agree are not useful and should be removed when performing a natural language processing task. For instance, in English the definite and indefinite article (the, a/an), conjunctions ('and','but','for'), propositions('on', 'in', 'at'), etc. are stop words. Secondly, depending on the context, we might want to expand the standard set of stop words. For example, in the movie reviews dataset, we might want to exclude words such as 'film', 'movie', 'cinema', etc.\n",
    "\n",
    "3\\. Stop words with word clouds\n",
    "-------------------------------\n",
    "\n",
    "01:00 - 01:31\n",
    "\n",
    "Maybe you recall from a previous video that we built word clouds using movie reviews. Here is an example of two word clouds using the movie reviews. In the picture on the left, the stop words have not been removed. Words that pop up are 'film' and 'br', which is an indication for a line break. In the cloud on the right side, stop words have been removed and now we see words such as 'character', 'see', 'good', 'story'.\n",
    "\n",
    "4\\. Remove stop words from word clouds\n",
    "--------------------------------------\n",
    "\n",
    "01:31 - 02:21\n",
    "\n",
    "How do we remove stop words when creating a word cloud? Let's start by reviewing how we built a word cloud. First, we import the WordCloud function from wordcloud. We also import the default list of STOPWORDS from wordcloud. To create our list of stop words, we can take a set of the default list. A set is like a list but with unique, not repeating items. We can update the set of stop words by calling update and providing a list to it. We pass our list of stopwords, called my_stopwords to the stopwords argument in the WordCloud function. Then we display it. So, the only new argument we added here is defining the list of stop words. Everything else stays the same.\n",
    "\n",
    "5\\. Stop words with BOW\n",
    "-----------------------\n",
    "\n",
    "02:21 - 03:34\n",
    "\n",
    "Removing non-informative words when we are building a BOW transformation can also be very useful. This can easily be incorporated in the countvectorizer function. First, we need to import the list of default English stop words from the same feature_extraction.text package from sci-kit learn. Let's assume we want to enrich this default list with movie-specific words. To do that, we call the union function on the default list. Remember that a union of two sets A and B consists of all elements of A and all elements of B such that no elements are repeated. In our case, the union will add the new words to the list of default stop words, if that word is not already there. To use the constructed set, we specify the stop_words argument in the CountVectorizer to be equal to our defined set. Everything else stays the same and should look pretty familiar by now. One important thing to note is that using stopwords will reduce the size of the vocabulary we built using a BOW or another approach.\n",
    "\n",
    "6\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:34 - 03:40\n",
    "\n",
    "Let's solve some exercises where you will practice removing stop words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word cloud of tweets\n",
    "====================\n",
    "\n",
    "Your task in this exercise is to plot a word cloud using a sample of Twitter data, expressing customers' sentiments about airlines. A string `text_tweet` has been created for you and it contains the messages of a 1000 customers shared on Twitter. \n",
    "\n",
    "In the first step, your are asked to build the word cloud without removing the stop words, and in the second step to build the same cloud after you have removed the stop words. \n",
    "\n",
    "Feel free to familiarize yourself with the `text_tweet` list.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   -   Import the word cloud function and package.\n",
    "    -   Create and generate the word cloud, using the `text_tweet` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(background_color='white').generate(text_tweet)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   -   Define the default list of stop words and update it.\n",
    "    -   Specify the stop words argument in the `WordCloud` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function and stop words list\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "# Define and update the list of stopwords\n",
    "my_stop_words = STOPWORDS.update(['airline', 'airplane'])\n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(stopwords=my_stop_words).generate(text_tweet)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "# Don't forget to show the final image\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airline sentiment with stop words\n",
    "=================================\n",
    "\n",
    "You are given a dataset, called `tweets`, which contains customers' reviews and sentiments about airlines. It consists of two columns: `airline_sentiment` and `text` where the sentiment can be positive, negative or neutral, and the `text` is the text of the tweet.\n",
    "\n",
    "In this exercise, you will create a BOW representation but will account for the stop words. Remember that stop words are not informative and you might want to remove them. That will result in a smaller vocabulary and eventually, fewer features. Keep in mind that we can enrich a default list of stop words with ones that are specific to our context.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the default list of English stop words.\n",
    "-   Update the default list of stop words with the given list `['airline', 'airlines', '@']` to create `my_stop_words`. \n",
    "-   Specify the stop words argument in the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@'])\n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(stop_words=my_stop_words)\n",
    "vect.fit(tweets.text)\n",
    "\n",
    "# Create the bow representation\n",
    "X_review = vect.transform(tweets.text)\n",
    "# Create the data frame\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple text columns\n",
    "=====================\n",
    "\n",
    "In this exercise, you will continue working with the airline Twitter data. A dataset `tweets` has been imported for you. \n",
    "\n",
    "In some situations, you might have more than one text column in a dataset and you might want to create a numeric representation for each of the text columns. Here, besides the `text` column, which contains the body of the tweet, there is a second text column, called `negativereason`. It contains the reason the customer left a negative review. \n",
    "\n",
    "Your task is to build BOW representations for both columns and specify the required stop words.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the vectorizer package and the default list of English stop words.\n",
    "-   Update the default list of English stop words and create the `my_stop_words` set.\n",
    "-   Specify the stop words argument in the first vectorizer to the updated set, and in the second vectorizer - the default set of English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the vectorizer and default English stop words list\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@', 'am', 'pm'])\n",
    " \n",
    "# Build and fit the vectorizers\n",
    "vect1 = CountVectorizer(stop_words=my_stop_words)\n",
    "vect2 = CountVectorizer(stop_words=ENGLISH_STOP_WORDS) \n",
    "vect1.fit(tweets.text)\n",
    "vect2.fit(tweets.negative_reason)\n",
    "\n",
    "# Print the last 15 features from the first, and all from second vectorizer\n",
    "print(vect1.get_feature_names()[-15:])\n",
    "print(vect2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Capturing a token pattern\n",
    "-----------------------------\n",
    "\n",
    "00:00 - 00:14\n",
    "\n",
    "You may have noticed while working with the airline sentiment data from Twitter that the text contains many digits and other characters. Sometimes we may want to exclude them from our numeric representation.\n",
    "\n",
    "2\\. String operators and comparisons\n",
    "------------------------------------\n",
    "\n",
    "00:14 - 00:49\n",
    "\n",
    "If we work with a string, how can we make sure we extract only certain characters? There are a few useful functionalities we will review here. We can use string comparison operators, such as .isaplha(), which returns true if a string is composed only of letters and false otherwise; .isdigits() returns true if a string is composed only of digits; and finally .isalnum() returns true if a string is composed only of alphanumeric characters, i.e. letters and digits.\n",
    "\n",
    "3\\. String operators with list comprehension\n",
    "--------------------------------------------\n",
    "\n",
    "00:49 - 02:01\n",
    "\n",
    "String operators can improve some of the features we created earlier. As a reminder, in a previous video we used a list comprehension to iterate over each review of the product reviews dataset and create word tokens from each review. We can adjust our original code. If we want to retain only tokens consisting of letters, for example, we can use the .isaplha() operator in a second list comprehension. Since the result of the first list comprehension is a list of lists, we first need to iterate over the items in each inner list, filtering out those tokens that are not letters. This is what happens in the first part of the list comprehension, enclosed in the inner brackets. In the second part, we are iterating over the lists, basically saying that we want to perform this filtering across all lists in the word_tokens list. When we compare the length of the first item of word_tokens and the cleaned_tokens lists, we see that the filtering decreased the number of tokens, as we might expect.\n",
    "\n",
    "4\\. Regular expressions\n",
    "-----------------------\n",
    "\n",
    "02:01 - 02:58\n",
    "\n",
    "Regular expressions are a standard way to extract certain characters from a string. Python has a built-in package, called re, which allows you to work with regular expressions. We will not cover regular expressions in depth here but, a quick reminder on the syntax. We import the re package. Then imagine we have a string #Wonderfulday and we want to extract a hash(#) followed by any letter, capital or small. One standard way to do is by calling the search function on our string, specifying the regular expression. In our case, it starts with a #, and is followed by either an upper or lower case letter. When we print the result, we see that it is a match object, showing how large the match is - in our case, the span is 2, and also the exact characters that were matched.\n",
    "\n",
    "5\\. Token pattern with a BOW\n",
    "----------------------------\n",
    "\n",
    "02:58 - 04:01\n",
    "\n",
    "Our familiar CountVectorizer takes a regular expression as an argument. The default pattern used matches words that consists of at least two letters or numbers (\\w) and which are separated by word boundaries (\\b). It will ignore single-lettered words, and will split words such as 'don't' and 'haven't'. If we are fine with this default pattern, we don't need to change any arguments in the CountVectorizer. If we want to change it, we can specify the token_pattern argument. If we want the vectorizer to ignore digits and other characters and only consider words of two or more letters, we can use the specified token pattern. In fact, there are multiple ways to specify this. It doesn't mean the one specified here is the only correct or best way to accomplish this. Feel free to experiment with this. Note, however, that we need to add an 'r' before the regular expression itself.\n",
    "\n",
    "6\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:01 - 04:05\n",
    "\n",
    "Let's go to the exercises where you can apply the things you learned here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the token pattern\n",
    "=========================\n",
    "\n",
    "In this exercise, you will work with the `text`column of the `tweets` dataset. Your task is to vectorize the object column using `CountVectorizer`. You will apply different patterns of tokens in the vectorizer. Remember that by specifying the token pattern, you can filter out characters. \n",
    "\n",
    "The `CountVectorizer` has been imported for you.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Build a vectorizer from the `text` column, specifying the pattern of tokens to be equal to `r'\\b[^\\d\\W][^\\d\\W]'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(tweets.text)\n",
    "vect_text = vect.transform(tweets.text)\n",
    "print('Length of vectorizer: ', len(vect.get_feature_names()))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Build a vectorizer from the `text` column using the default values of the function's arguments. \n",
    "-   Build a second vectorizer, specifying the pattern of tokens to be equal to `r'\\b[^\\d\\W][^\\d\\W]'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the first vectorizer\n",
    "vect1 = CountVectorizer().fit(tweets.text)\n",
    "vect1.transform(tweets.text)\n",
    "\n",
    "# Build the second vectorizer\n",
    "vect2 = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]').fit(tweets.text)\n",
    "vect2.transform(tweets.text)\n",
    "\n",
    "# Print out the length of each vectorizer\n",
    "print('Length of vectorizer 1: ', len(vect1.get_feature_names()))\n",
    "print('Length of vectorizer 2: ', len(vect2.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String operators with the Twitter data\n",
    "======================================\n",
    "\n",
    "You continue working with the `tweets` data where the `text` column stores the content of each tweet. \n",
    "\n",
    "Your task is to turn the `text` column into a list of tokens. Then, using string operators, remove all non-alphabetic characters from the created list of tokens.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the word tokenizing function.\n",
    "-   Create word tokens from each tweet.\n",
    "-   Filter out all non-alphabetic characters from the created list, i.e. retain only letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenizing package\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize the text column\n",
    "word_tokens = [word_tokenize(review) for review in tweets.text]\n",
    "print('Original tokens: ', word_tokens[0])\n",
    "\n",
    "# Filter out non-letter characters\n",
    "cleaned_tokens = [[word for word in item if word.isalpha()] for item in word_tokens]\n",
    "print('Cleaned tokens: ', cleaned_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More string operators and Twitter\n",
    "=================================\n",
    "\n",
    "In this exercise, you will apply different string operators to three strings, selected from the `tweets` dataset. A `tweets_list` has been created for you.\n",
    "\n",
    "You need to construct three new lists by applying different string operators:\n",
    "\n",
    "-   a list retaining only letters\n",
    "-   a list retaining only characters \n",
    "-   a list retaining only digits \n",
    "\n",
    "The required functions have been imported for you from `nltk`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create a list of the tokens from `tweets_list`.\n",
    "-   In the list `letters` remove all digits and other characters, i.e. keep only letters.\n",
    "-   Retain alphanumeric characters but remove all other characters in `let_digits`.\n",
    "-   Create `digits` by removing letters and characters and keeping only numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists, containing the tokens from list_tweets\n",
    "tokens = [word_tokenize(item) for item in tweets_list]\n",
    "\n",
    "# Remove characters and digits , i.e. retain only letters\n",
    "letters = [[word for word in item if word.isalpha()] for item in tokens]\n",
    "# Remove characters, i.e. retain only letters and digits\n",
    "let_digits = [[word for word in item if word.isalnum()] for item in tokens]\n",
    "# Remove letters and characters, retain only digits\n",
    "digits = [[word for word in item if word.isdigit()] for item in tokens]\n",
    "\n",
    "# Print the last item in each list\n",
    "print('Last item in alphabetic list: ', letters[2])\n",
    "print('Last item in list of alphanumerics: ', let_digits[2])\n",
    "print('Last item in the list of digits: ', digits[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Stemming and lemmatization\n",
    "------------------------------\n",
    "\n",
    "00:00 - 00:17\n",
    "\n",
    "In a language, words are often derived from other words, meaning words can share the same root. When we create a numeric transformation of a text feature, we might want to strip a word down to its root. This is the topic of this lesson.\n",
    "\n",
    "2\\. What is stemming?\n",
    "---------------------\n",
    "\n",
    "00:17 - 00:53\n",
    "\n",
    "This process is called stemming. More formally, stemming can be defined as the transformation of words to their root forms, even if the stem itself is not a valid word in the language. For example, staying, stays, stayed will be mapped to the root 'stay', and house, houses, housing will be mapped to the root 'hous'. In general, stemming will tend to chop off suffixes such as '-ed', '-ing', '-er', as well as plural or possessive forms.\n",
    "\n",
    "3\\. What is lemmatization?\n",
    "--------------------------\n",
    "\n",
    "00:53 - 01:15\n",
    "\n",
    "Lemmatization is quite a similar process to stemming, with the main difference that with lemmatization, the resulting roots are valid words in the language. Going back to our examples of words derived from 'stay', lemmatization reduces them to 'stay'; and words derived from 'house' are reduced to the noun 'house'.\n",
    "\n",
    "4\\. Stemming vs. lemmatization\n",
    "------------------------------\n",
    "\n",
    "01:15 - 02:02\n",
    "\n",
    "You might wonder when to use stemming and when lemmatization. The main difference is in the obtained roots. With lemmatization they are actual words and with stemming they might not be. So if in your problem it's important to retain words, not only roots, lemmatization would be more suitable. However, if you use nltk - which is what we will use in this course - stemming follows an algorithm which makes it faster than the lemmatization process in nltk. Furthermore, lemmatization is dependent on knowing the part of speech of the word you want to lemmatize. For example, whether we want to transform a noun, a verb, an adjective, etc.\n",
    "\n",
    "5\\. Stemming of strings\n",
    "-----------------------\n",
    "\n",
    "02:02 - 02:31\n",
    "\n",
    "One popular stemming library is the PorterStemmer in the nltk.stem package. The PorterStemmer is not the only stemmer in nltk but it's quite fast and easy to use, so it's often a standard choice. We call the PorterStemmer function and store it under the name porter. We can then call porter.stem on a string, for example, 'wonderful'. The result is 'wonder'.\n",
    "\n",
    "6\\. Non-English stemmers\n",
    "------------------------\n",
    "\n",
    "02:31 - 03:01\n",
    "\n",
    "Stemming is possible using other languages as well, such as Danish, Dutch, French, Spanish, German, etc. To use foreign language stemmers we need to use the SnowballStemmer package. We can specify in the stemmer the foreign language we want to use. Then we apply the stem function on our string. For example, we have imported a Dutch stemmer and fed it a Dutch verb. The result is the root of the verb.\n",
    "\n",
    "7\\. How to stem a sentence?\n",
    "---------------------------\n",
    "\n",
    "03:01 - 03:30\n",
    "\n",
    "If you apply the PorterStemmer on a sentence, the result is the original sentence. We see nothing has changed about our 'Today is a wonderful day!' sentence. We need to stem each word in the sentence separately. Therefore, as a first step, we need to transform the sentence into tokens using the familiar word_tokenize function. In the second step, we apply the stemming function on each word of the sentence, using a list comprehension.\n",
    "\n",
    "8\\. Lemmatization of a string\n",
    "-----------------------------\n",
    "\n",
    "03:30 - 04:17\n",
    "\n",
    "The lemmatization of strings is similar to stemming. We import the WordNetLemmatizer from the nltk.stem library. It uses the WordNet database to look up lemmas of words. We call the WordNetLemmatizer function and store it under the name WNlemmatizer. We can then call WNlemmatizer.lemmatize() on 'wonderful'. Note that we have specified a part-of-speech, given by the 'pos' argument. The default pos is noun, or 'n'. Here we specify an adjective, that's why pos = 'a'. The result is 'wonderful'. If you'd recall, stemming returned 'wonder' as a result.\n",
    "\n",
    "9\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:17 - 04:25\n",
    "\n",
    "Let's solve some exercises and reinforce the concepts related to stemming and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stems and lemmas from GoT\n",
    "=========================\n",
    "\n",
    "In this exercise, you are given a couple of sentences from George R.R. Martin's **Game of Thrones**. Your task is to create stems and lemmas from the given `GoT` string.\n",
    "\n",
    "Remember that stems reduce a word to its root whereas lemmas produce an actual word. However, speed can differ significantly between the methods with stemming being much faster. In Steps 2 and 3, pay attention to the total time it takes to perform each operation. We're making use of the `time.time()` method to measure the time it takes to perform stemming and lemmatization.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "-   Import the stemming and lemmatization functions.\n",
    "-   Build a list of tokens from the `GoT` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages from nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "porter = PorterStemmer()\n",
    "WNlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the GoT string\n",
    "tokens = word_tokenize(GoT) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "----------------\n",
    "\n",
    "-   Using list comprehension and the `porter`stemmer you imported, create the `stemmed_tokens` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a stemmed list\n",
    "stemmed_tokens = [porter.stem(token) for token in tokens] \n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for stemming in seconds: ', end_time - start_time)\n",
    "print('Stemmed tokens: ', stemmed_tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "----------------\n",
    "\n",
    "-   Using list comprehension and the `WNlemmatizer` you imported, create the `lem_tokens` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a lemmatized list\n",
    "lem_tokens = [WNlemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for lemmatizing in seconds: ', end_time - start_time)\n",
    "print('Lemmatized tokens: ', lem_tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem Spanish reviews\n",
    "====================\n",
    "\n",
    "You will recall that in a previous chapter we used a language detection package to determine the language of different Amazon product reviews. In this exercise, you will first detect the languages in the `non_english_reviews`. The reviews are in multiple languages but you will select ONLY those in Spanish. Feel free to go back to the video discussing foreign language detection if you have forgotten some of the concepts. \n",
    "\n",
    "In the second step, you will create word tokens from the Spanish reviews and will stem them using a SnowBall stemmer for the Spanish language. The language detection package is not perfect, unfortunately. Therefore, it is possible that sometimes the detected language is not correct.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Import the `langdetect` package.\n",
    "-   Iterate over the rows of the `non_english_reviews` using the `len()`method and `range()` function.\n",
    "-   Use `detect_langs()` to detect the language of each review in the `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the language detection package\n",
    "import langdetect\n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "languages = [] \n",
    "for i in range(len(non_english_reviews)):\n",
    "    languages.append(langdetect.detect_langs(non_english_reviews.iloc[i, 1]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "# Assign the list to a new feature \n",
    "non_english_reviews['language'] = languages\n",
    "\n",
    "# Select the Spanish ones\n",
    "filtered_reviews = non_english_reviews[non_english_reviews.language == 'es']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Import the `SnowballStemmer` from the respective package.\n",
    "-   Create word tokens from the `review` column of the `filtered_reviews` from the previous step.\n",
    "-   Use the Spanish stemmer you imported to stem the created list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Import the Spanish SnowballStemmer\n",
    "SpanishStemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# Create a list of tokens\n",
    "tokens = [word_tokenize(review) for review in filtered_reviews.review]\n",
    "# Stem the list of tokens\n",
    "stemmed_tokens = [[SpanishStemmer.stem(word) for word in token] for token in tokens]\n",
    "\n",
    "# Print the first item of the stemmed tokenss\n",
    "print(stemmed_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stems from tweets\n",
    "=================\n",
    "\n",
    "In this exercise, you will work with an array called `tweets`. It contains the text of the airline sentiment data collected from Twitter. \n",
    "\n",
    "Your task is to work with this array and transform it into a list of tokens using list comprehension. After that, iterate over the list of tokens and create a stem out of each token. Remember that list comprehensions are a one-line alternative to **for** loops.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the function we used to transform strings into stems. \n",
    "-   Call the Porter stemmer function you just imported.\n",
    "-   Using a list comprehension, create the list `tokens`. It should contain all the word tokens from the `tweets` array.\n",
    "-   Iterate over the `tokens` list and apply the stemming function to each item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function to perform stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Call the stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Transform the array of tweets to tokens\n",
    "tokens = [word_tokenize(word) for word in tweets]\n",
    "# Stem the list of tokens\n",
    "stemmed_tokens = [[porter.stem(word) for word in tweet] for tweet in tokens] \n",
    "# Print the first element of the list\n",
    "print(stemmed_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. TfIdf: More ways to transform text\n",
    "--------------------------------------\n",
    "\n",
    "00:00 - 00:23\n",
    "\n",
    "We have extensively worked with a BOW and applied it using a CountVectorizer in Python. As powerful as BOW can be, sometimes we might want to try slightly more sophisticated approaches. In this video we will talk about one of them, an approach called TfIdf - term frequency inverse document frequency.\n",
    "\n",
    "2\\. What are the components of TfIdf?\n",
    "-------------------------------------\n",
    "\n",
    "00:23 - 00:52\n",
    "\n",
    "The term frequency tells us how often a given word appears within a document in the corpus. Each word in a document has its own term frequency. The inverse document frequency is commonly defined as the log-ratio between the total number of documents and the number of documents that contain a specific word. What inverse document frequency means is that rare words will have a high inverse document frequency.\n",
    "\n",
    "3\\. TfIdf score of a word\n",
    "-------------------------\n",
    "\n",
    "00:52 - 01:38\n",
    "\n",
    "When we multiply the tf and the idf scores, we obtain the TfIdf score of a word in a corpus. With BOW, words could have different frequency counts across documents but we did not account for the length of a document; whereas the TfIdf score of a word incorporates the length of a document. TfIdf will also highlight words that are more interesting, i.e. words that are common in a document but not across all documents. However, note that interesting does not have to relate to a positive or a negative review. It is purely an unsupervised approach.\n",
    "\n",
    "4\\. How is TfIdf useful?\n",
    "------------------------\n",
    "\n",
    "01:38 - 02:32\n",
    "\n",
    "In our Twitter sentiment analysis, names of airline companies such as United and Virgin America are likely to have low TfIdf scores since they occur many times and across many documents, i.e. tweets. If a tweet talks a lot about the check-in service of a company and there are not many other tweets discussing the topic, words in this tweet are likely to have a high TfIdf score. Note that since TfIdf penalizes frequent words, there is less of a need to explicitly define stop words. We can still remove stop words, of course, to restrict the size of our vocabulary. Even though TfIdf is relatively simple, it is quite commonly used in information retrieval and search engines as a way to rank the relevance of the returned queries.\n",
    "\n",
    "5\\. TfIdf in Python\n",
    "-------------------\n",
    "\n",
    "02:32 - 03:14\n",
    "\n",
    "In Python, you can apply TfIdf by importing the TfidfVectorizer from sklearn.feature_extraction.text. The TfIdfVectorizer is similar to the CountVectorizer, and so are the arguments it takes. We can define the maximum number of features by max_features, the type of n-grams to use by specifying ngram_range, the stop_words argument, token_pattern, max_df and min_df. We fit the TfidfVectorizer to the text column of the tweets dataset. Then we transform it, the same way we did with the CountVectorizer.\n",
    "\n",
    "6\\. TfidfVectorizer\n",
    "-------------------\n",
    "\n",
    "03:14 - 03:56\n",
    "\n",
    "The Tfidfvectorizer also returns a sparse matrix. If you recall, a sparse matrix is a matrix with mostly zero values, storing only the non-zero values. We need to transform the sparse matrix to an array and specify the feature names, using the same syntax as with the CountVectorizer. Inspecting the top 5 rows of the newly created dataset, we see that the output is quite similar to a BOW. Each column is a feature and each row contains the TfIdf score of the feature in a given tweet. The values are floating numbers, and many of them are zero.\n",
    "\n",
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:56 - 04:08\n",
    "\n",
    "Let's wrap up our discussion on numerical transformation of text data by solving some exercises using TfIdf. See you in the next video!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first TfIdf\n",
    "================\n",
    "\n",
    "In this exercise, you will apply the TfIdf method to the small `annak` dataset, containing the first sentence of *Anna Karenina* by Leo Tolstoy.\n",
    "\n",
    "Your task will be to work with this dataset and apply the `TfidfVectorizer()` function. Recall that performing a numeric transformation of text is your first step in being able to understand the sentiment of the text. The Tfidf vectorizer is another way to construct a vocabulary from our sentiment column.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the function for building a TfIdf vectorizer from `sklearn.feature_extraction.text`.\n",
    "-   Call the `TfidfVectorizer()` function and fit it on the `annak` dataset .\n",
    "-   Transform the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Call the vectorizer and fit it\n",
    "anna_vect = TfidfVectorizer().fit(annak)\n",
    "\n",
    "# Create the tfidf representation\n",
    "anna_tfidf = anna_vect.transform(annak)\n",
    "\n",
    "# Print the result \n",
    "print(anna_tfidf.toarray())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfIdf on Twitter airline sentiment data\n",
    "=======================================\n",
    "\n",
    "You will now build features using the TfIdf method. You will continue to work with the `tweets` dataset. \n",
    "\n",
    "In this exercise, you will utilize what you have learned in previous lessons and remove stop words, use a token pattern and specify the n-grams.\n",
    "\n",
    "The final output will be a DataFrame, of which the columns are created using the `TfidfVectorizer()`. Such a DataFrame can directly be passed to a supervised learning model, which is what we will tackle in the next chapter.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the required package to build a TfidfVectorizer and the `ENGLISH_STOP_WORDS`.\n",
    "-   Build a TfIdf vectorizer from the `text`column of the `tweets` dataset, specifying uni- and bi-grams as a choice of n-grams, tokens which include only alphanumeric characters using the given token pattern, and the stop words corresponding to the `ENGLISH_STOP_WORDS`. \n",
    "-   Transform the vectorizer, specifying the same column that you fit.\n",
    "-   Specify the column names in the `DataFrame()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required vectorizer package and stop words list\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the vectorizer and specify the arguments\n",
    "my_pattern = r'\\b[^\\d\\W][^\\d\\W]+\\b'\n",
    "vect = TfidfVectorizer(ngram_range=(1, 2), max_features=100, token_pattern=my_pattern, stop_words=ENGLISH_STOP_WORDS).fit(tweets.text)\n",
    "\n",
    "# Transform the vectorizer\n",
    "X_txt = vect.transform(tweets.text)\n",
    "\n",
    "# Transform to a data frame and specify the column names\n",
    "X=pd.DataFrame(X_txt.toarray(), columns=vect.get_feature_names())\n",
    "print('Top 5 rows of the DataFrame: ', X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidf and a BOW on same data\n",
    "============================\n",
    "\n",
    "In this exercise, you will transform the `review`column of the Amazon product `reviews` using both a bag-of-words and a tfidf transformation. \n",
    "\n",
    "Build both vectorizers, specifying only the maximum number of features to be equal to `100`. Create DataFrames after the transformation and print the top 5 rows of each.\n",
    "\n",
    "Be careful how you specify the maximum number of features in the vocabulary. A large vocabulary size can result in your session being disconnected.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the BOW and Tfidf vectorizers.\n",
    "-   Build and fit a BOW and a Tfidf vectorizer from the `review` column and limit the number of created features to 100.\n",
    "-   Create DataFrames from the transformed vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Build a BOW and tfidf vectorizers from the review column and with max of 100 features\n",
    "vect1 = CountVectorizer(max_features=100).fit(reviews.review)\n",
    "vect2 = TfidfVectorizer(max_features=100).fit(reviews.review) \n",
    "\n",
    "# Transform the vectorizers\n",
    "X1 = vect1.transform(reviews.review)\n",
    "X2 = vect2.transform(reviews.review)\n",
    "# Create DataFrames from the vectorizers \n",
    "X_df1 = pd.DataFrame(X1.toarray(), columns=vect1.get_feature_names())\n",
    "X_df2 = pd.DataFrame(X2.toarray(), columns=vect2.get_feature_names())\n",
    "print('Top 5 rows using BOW: \\n', X_df1.head())\n",
    "print('Top 5 rows using tfidf: \\n', X_df2.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
