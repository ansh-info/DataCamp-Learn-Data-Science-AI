{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Stop words\n",
    "--------------\n",
    "\n",
    "00:00 - 00:11\n",
    "\n",
    "In every language, there are words that occur too frequently and are not very informative. Sometimes, it is useful to get rid of them before we build a machine learning model.\n",
    "\n",
    "2\\. What are stop words and how to find them?\n",
    "---------------------------------------------\n",
    "\n",
    "00:11 - 01:00\n",
    "\n",
    "Words that occur too frequently and are not very informative are called stop words. But how do we know which words are not informative? In every language, there is a set of words that most practitioners agree are not useful and should be removed when performing a natural language processing task. For instance, in English the definite and indefinite article (the, a/an), conjunctions ('and','but','for'), propositions('on', 'in', 'at'), etc. are stop words. Secondly, depending on the context, we might want to expand the standard set of stop words. For example, in the movie reviews dataset, we might want to exclude words such as 'film', 'movie', 'cinema', etc.\n",
    "\n",
    "3\\. Stop words with word clouds\n",
    "-------------------------------\n",
    "\n",
    "01:00 - 01:31\n",
    "\n",
    "Maybe you recall from a previous video that we built word clouds using movie reviews. Here is an example of two word clouds using the movie reviews. In the picture on the left, the stop words have not been removed. Words that pop up are 'film' and 'br', which is an indication for a line break. In the cloud on the right side, stop words have been removed and now we see words such as 'character', 'see', 'good', 'story'.\n",
    "\n",
    "4\\. Remove stop words from word clouds\n",
    "--------------------------------------\n",
    "\n",
    "01:31 - 02:21\n",
    "\n",
    "How do we remove stop words when creating a word cloud? Let's start by reviewing how we built a word cloud. First, we import the WordCloud function from wordcloud. We also import the default list of STOPWORDS from wordcloud. To create our list of stop words, we can take a set of the default list. A set is like a list but with unique, not repeating items. We can update the set of stop words by calling update and providing a list to it. We pass our list of stopwords, called my_stopwords to the stopwords argument in the WordCloud function. Then we display it. So, the only new argument we added here is defining the list of stop words. Everything else stays the same.\n",
    "\n",
    "5\\. Stop words with BOW\n",
    "-----------------------\n",
    "\n",
    "02:21 - 03:34\n",
    "\n",
    "Removing non-informative words when we are building a BOW transformation can also be very useful. This can easily be incorporated in the countvectorizer function. First, we need to import the list of default English stop words from the same feature_extraction.text package from sci-kit learn. Let's assume we want to enrich this default list with movie-specific words. To do that, we call the union function on the default list. Remember that a union of two sets A and B consists of all elements of A and all elements of B such that no elements are repeated. In our case, the union will add the new words to the list of default stop words, if that word is not already there. To use the constructed set, we specify the stop_words argument in the CountVectorizer to be equal to our defined set. Everything else stays the same and should look pretty familiar by now. One important thing to note is that using stopwords will reduce the size of the vocabulary we built using a BOW or another approach.\n",
    "\n",
    "6\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:34 - 03:40\n",
    "\n",
    "Let's solve some exercises where you will practice removing stop words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word cloud of tweets\n",
    "====================\n",
    "\n",
    "Your task in this exercise is to plot a word cloud using a sample of Twitter data, expressing customers' sentiments about airlines. A string `text_tweet` has been created for you and it contains the messages of a 1000 customers shared on Twitter. \n",
    "\n",
    "In the first step, your are asked to build the word cloud without removing the stop words, and in the second step to build the same cloud after you have removed the stop words. \n",
    "\n",
    "Feel free to familiarize yourself with the `text_tweet` list.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   -   Import the word cloud function and package.\n",
    "    -   Create and generate the word cloud, using the `text_tweet` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(background_color='white').generate(text_tweet)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   -   Define the default list of stop words and update it.\n",
    "    -   Specify the stop words argument in the `WordCloud` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function and stop words list\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "# Define and update the list of stopwords\n",
    "my_stop_words = STOPWORDS.update(['airline', 'airplane'])\n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(stopwords=my_stop_words).generate(text_tweet)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "# Don't forget to show the final image\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airline sentiment with stop words\n",
    "=================================\n",
    "\n",
    "You are given a dataset, called `tweets`, which contains customers' reviews and sentiments about airlines. It consists of two columns: `airline_sentiment` and `text` where the sentiment can be positive, negative or neutral, and the `text` is the text of the tweet.\n",
    "\n",
    "In this exercise, you will create a BOW representation but will account for the stop words. Remember that stop words are not informative and you might want to remove them. That will result in a smaller vocabulary and eventually, fewer features. Keep in mind that we can enrich a default list of stop words with ones that are specific to our context.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the default list of English stop words.\n",
    "-   Update the default list of stop words with the given list `['airline', 'airlines', '@']` to create `my_stop_words`. \n",
    "-   Specify the stop words argument in the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@'])\n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(stop_words=my_stop_words)\n",
    "vect.fit(tweets.text)\n",
    "\n",
    "# Create the bow representation\n",
    "X_review = vect.transform(tweets.text)\n",
    "# Create the data frame\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple text columns\n",
    "=====================\n",
    "\n",
    "In this exercise, you will continue working with the airline Twitter data. A dataset `tweets` has been imported for you. \n",
    "\n",
    "In some situations, you might have more than one text column in a dataset and you might want to create a numeric representation for each of the text columns. Here, besides the `text` column, which contains the body of the tweet, there is a second text column, called `negativereason`. It contains the reason the customer left a negative review. \n",
    "\n",
    "Your task is to build BOW representations for both columns and specify the required stop words.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the vectorizer package and the default list of English stop words.\n",
    "-   Update the default list of English stop words and create the `my_stop_words` set.\n",
    "-   Specify the stop words argument in the first vectorizer to the updated set, and in the second vectorizer - the default set of English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the vectorizer and default English stop words list\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@', 'am', 'pm'])\n",
    " \n",
    "# Build and fit the vectorizers\n",
    "vect1 = CountVectorizer(stop_words=my_stop_words)\n",
    "vect2 = CountVectorizer(stop_words=ENGLISH_STOP_WORDS) \n",
    "vect1.fit(tweets.text)\n",
    "vect2.fit(tweets.negative_reason)\n",
    "\n",
    "# Print the last 15 features from the first, and all from second vectorizer\n",
    "print(vect1.get_feature_names()[-15:])\n",
    "print(vect2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Capturing a token pattern\n",
    "-----------------------------\n",
    "\n",
    "00:00 - 00:14\n",
    "\n",
    "You may have noticed while working with the airline sentiment data from Twitter that the text contains many digits and other characters. Sometimes we may want to exclude them from our numeric representation.\n",
    "\n",
    "2\\. String operators and comparisons\n",
    "------------------------------------\n",
    "\n",
    "00:14 - 00:49\n",
    "\n",
    "If we work with a string, how can we make sure we extract only certain characters? There are a few useful functionalities we will review here. We can use string comparison operators, such as .isaplha(), which returns true if a string is composed only of letters and false otherwise; .isdigits() returns true if a string is composed only of digits; and finally .isalnum() returns true if a string is composed only of alphanumeric characters, i.e. letters and digits.\n",
    "\n",
    "3\\. String operators with list comprehension\n",
    "--------------------------------------------\n",
    "\n",
    "00:49 - 02:01\n",
    "\n",
    "String operators can improve some of the features we created earlier. As a reminder, in a previous video we used a list comprehension to iterate over each review of the product reviews dataset and create word tokens from each review. We can adjust our original code. If we want to retain only tokens consisting of letters, for example, we can use the .isaplha() operator in a second list comprehension. Since the result of the first list comprehension is a list of lists, we first need to iterate over the items in each inner list, filtering out those tokens that are not letters. This is what happens in the first part of the list comprehension, enclosed in the inner brackets. In the second part, we are iterating over the lists, basically saying that we want to perform this filtering across all lists in the word_tokens list. When we compare the length of the first item of word_tokens and the cleaned_tokens lists, we see that the filtering decreased the number of tokens, as we might expect.\n",
    "\n",
    "4\\. Regular expressions\n",
    "-----------------------\n",
    "\n",
    "02:01 - 02:58\n",
    "\n",
    "Regular expressions are a standard way to extract certain characters from a string. Python has a built-in package, called re, which allows you to work with regular expressions. We will not cover regular expressions in depth here but, a quick reminder on the syntax. We import the re package. Then imagine we have a string #Wonderfulday and we want to extract a hash(#) followed by any letter, capital or small. One standard way to do is by calling the search function on our string, specifying the regular expression. In our case, it starts with a #, and is followed by either an upper or lower case letter. When we print the result, we see that it is a match object, showing how large the match is - in our case, the span is 2, and also the exact characters that were matched.\n",
    "\n",
    "5\\. Token pattern with a BOW\n",
    "----------------------------\n",
    "\n",
    "02:58 - 04:01\n",
    "\n",
    "Our familiar CountVectorizer takes a regular expression as an argument. The default pattern used matches words that consists of at least two letters or numbers (\\w) and which are separated by word boundaries (\\b). It will ignore single-lettered words, and will split words such as 'don't' and 'haven't'. If we are fine with this default pattern, we don't need to change any arguments in the CountVectorizer. If we want to change it, we can specify the token_pattern argument. If we want the vectorizer to ignore digits and other characters and only consider words of two or more letters, we can use the specified token pattern. In fact, there are multiple ways to specify this. It doesn't mean the one specified here is the only correct or best way to accomplish this. Feel free to experiment with this. Note, however, that we need to add an 'r' before the regular expression itself.\n",
    "\n",
    "6\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:01 - 04:05\n",
    "\n",
    "Let's go to the exercises where you can apply the things you learned here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the token pattern\n",
    "=========================\n",
    "\n",
    "In this exercise, you will work with the `text`column of the `tweets` dataset. Your task is to vectorize the object column using `CountVectorizer`. You will apply different patterns of tokens in the vectorizer. Remember that by specifying the token pattern, you can filter out characters. \n",
    "\n",
    "The `CountVectorizer` has been imported for you.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Build a vectorizer from the `text` column, specifying the pattern of tokens to be equal to `r'\\b[^\\d\\W][^\\d\\W]'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(tweets.text)\n",
    "vect_text = vect.transform(tweets.text)\n",
    "print('Length of vectorizer: ', len(vect.get_feature_names()))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Build a vectorizer from the `text` column using the default values of the function's arguments. \n",
    "-   Build a second vectorizer, specifying the pattern of tokens to be equal to `r'\\b[^\\d\\W][^\\d\\W]'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the first vectorizer\n",
    "vect1 = CountVectorizer().fit(tweets.text)\n",
    "vect1.transform(tweets.text)\n",
    "\n",
    "# Build the second vectorizer\n",
    "vect2 = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]').fit(tweets.text)\n",
    "vect2.transform(tweets.text)\n",
    "\n",
    "# Print out the length of each vectorizer\n",
    "print('Length of vectorizer 1: ', len(vect1.get_feature_names()))\n",
    "print('Length of vectorizer 2: ', len(vect2.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String operators with the Twitter data\n",
    "======================================\n",
    "\n",
    "You continue working with the `tweets` data where the `text` column stores the content of each tweet. \n",
    "\n",
    "Your task is to turn the `text` column into a list of tokens. Then, using string operators, remove all non-alphabetic characters from the created list of tokens.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the word tokenizing function.\n",
    "-   Create word tokens from each tweet.\n",
    "-   Filter out all non-alphabetic characters from the created list, i.e. retain only letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenizing package\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize the text column\n",
    "word_tokens = [word_tokenize(review) for review in tweets.text]\n",
    "print('Original tokens: ', word_tokens[0])\n",
    "\n",
    "# Filter out non-letter characters\n",
    "cleaned_tokens = [[word for word in item if word.isalpha()] for item in word_tokens]\n",
    "print('Cleaned tokens: ', cleaned_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More string operators and Twitter\n",
    "=================================\n",
    "\n",
    "In this exercise, you will apply different string operators to three strings, selected from the `tweets` dataset. A `tweets_list` has been created for you.\n",
    "\n",
    "You need to construct three new lists by applying different string operators:\n",
    "\n",
    "-   a list retaining only letters\n",
    "-   a list retaining only characters \n",
    "-   a list retaining only digits \n",
    "\n",
    "The required functions have been imported for you from `nltk`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create a list of the tokens from `tweets_list`.\n",
    "-   In the list `letters` remove all digits and other characters, i.e. keep only letters.\n",
    "-   Retain alphanumeric characters but remove all other characters in `let_digits`.\n",
    "-   Create `digits` by removing letters and characters and keeping only numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists, containing the tokens from list_tweets\n",
    "tokens = [word_tokenize(item) for item in tweets_list]\n",
    "\n",
    "# Remove characters and digits , i.e. retain only letters\n",
    "letters = [[word for word in item if word.isalpha()] for item in tokens]\n",
    "# Remove characters, i.e. retain only letters and digits\n",
    "let_digits = [[word for word in item if word.isalnum()] for item in tokens]\n",
    "# Remove letters and characters, retain only digits\n",
    "digits = [[word for word in item if word.isdigit()] for item in tokens]\n",
    "\n",
    "# Print the last item in each list\n",
    "print('Last item in alphabetic list: ', letters[2])\n",
    "print('Last item in list of alphanumerics: ', let_digits[2])\n",
    "print('Last item in the list of digits: ', digits[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
