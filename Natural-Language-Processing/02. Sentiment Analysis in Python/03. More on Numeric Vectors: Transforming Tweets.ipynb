{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Stop words\n",
    "--------------\n",
    "\n",
    "00:00 - 00:11\n",
    "\n",
    "In every language, there are words that occur too frequently and are not very informative. Sometimes, it is useful to get rid of them before we build a machine learning model.\n",
    "\n",
    "2\\. What are stop words and how to find them?\n",
    "---------------------------------------------\n",
    "\n",
    "00:11 - 01:00\n",
    "\n",
    "Words that occur too frequently and are not very informative are called stop words. But how do we know which words are not informative? In every language, there is a set of words that most practitioners agree are not useful and should be removed when performing a natural language processing task. For instance, in English the definite and indefinite article (the, a/an), conjunctions ('and','but','for'), propositions('on', 'in', 'at'), etc. are stop words. Secondly, depending on the context, we might want to expand the standard set of stop words. For example, in the movie reviews dataset, we might want to exclude words such as 'film', 'movie', 'cinema', etc.\n",
    "\n",
    "3\\. Stop words with word clouds\n",
    "-------------------------------\n",
    "\n",
    "01:00 - 01:31\n",
    "\n",
    "Maybe you recall from a previous video that we built word clouds using movie reviews. Here is an example of two word clouds using the movie reviews. In the picture on the left, the stop words have not been removed. Words that pop up are 'film' and 'br', which is an indication for a line break. In the cloud on the right side, stop words have been removed and now we see words such as 'character', 'see', 'good', 'story'.\n",
    "\n",
    "4\\. Remove stop words from word clouds\n",
    "--------------------------------------\n",
    "\n",
    "01:31 - 02:21\n",
    "\n",
    "How do we remove stop words when creating a word cloud? Let's start by reviewing how we built a word cloud. First, we import the WordCloud function from wordcloud. We also import the default list of STOPWORDS from wordcloud. To create our list of stop words, we can take a set of the default list. A set is like a list but with unique, not repeating items. We can update the set of stop words by calling update and providing a list to it. We pass our list of stopwords, called my_stopwords to the stopwords argument in the WordCloud function. Then we display it. So, the only new argument we added here is defining the list of stop words. Everything else stays the same.\n",
    "\n",
    "5\\. Stop words with BOW\n",
    "-----------------------\n",
    "\n",
    "02:21 - 03:34\n",
    "\n",
    "Removing non-informative words when we are building a BOW transformation can also be very useful. This can easily be incorporated in the countvectorizer function. First, we need to import the list of default English stop words from the same feature_extraction.text package from sci-kit learn. Let's assume we want to enrich this default list with movie-specific words. To do that, we call the union function on the default list. Remember that a union of two sets A and B consists of all elements of A and all elements of B such that no elements are repeated. In our case, the union will add the new words to the list of default stop words, if that word is not already there. To use the constructed set, we specify the stop_words argument in the CountVectorizer to be equal to our defined set. Everything else stays the same and should look pretty familiar by now. One important thing to note is that using stopwords will reduce the size of the vocabulary we built using a BOW or another approach.\n",
    "\n",
    "6\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:34 - 03:40\n",
    "\n",
    "Let's solve some exercises where you will practice removing stop words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word cloud of tweets\n",
    "====================\n",
    "\n",
    "Your task in this exercise is to plot a word cloud using a sample of Twitter data, expressing customers' sentiments about airlines. A string `text_tweet` has been created for you and it contains the messages of a 1000 customers shared on Twitter. \n",
    "\n",
    "In the first step, your are asked to build the word cloud without removing the stop words, and in the second step to build the same cloud after you have removed the stop words. \n",
    "\n",
    "Feel free to familiarize yourself with the `text_tweet` list.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   -   Import the word cloud function and package.\n",
    "    -   Create and generate the word cloud, using the `text_tweet` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(background_color='white').generate(text_tweet)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   -   Define the default list of stop words and update it.\n",
    "    -   Specify the stop words argument in the `WordCloud` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function and stop words list\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "# Define and update the list of stopwords\n",
    "my_stop_words = STOPWORDS.update(['airline', 'airplane'])\n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(stopwords=my_stop_words).generate(text_tweet)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "# Don't forget to show the final image\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airline sentiment with stop words\n",
    "=================================\n",
    "\n",
    "You are given a dataset, called `tweets`, which contains customers' reviews and sentiments about airlines. It consists of two columns: `airline_sentiment` and `text` where the sentiment can be positive, negative or neutral, and the `text` is the text of the tweet.\n",
    "\n",
    "In this exercise, you will create a BOW representation but will account for the stop words. Remember that stop words are not informative and you might want to remove them. That will result in a smaller vocabulary and eventually, fewer features. Keep in mind that we can enrich a default list of stop words with ones that are specific to our context.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the default list of English stop words.\n",
    "-   Update the default list of stop words with the given list `['airline', 'airlines', '@']` to create `my_stop_words`. \n",
    "-   Specify the stop words argument in the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@'])\n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(stop_words=my_stop_words)\n",
    "vect.fit(tweets.text)\n",
    "\n",
    "# Create the bow representation\n",
    "X_review = vect.transform(tweets.text)\n",
    "# Create the data frame\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple text columns\n",
    "=====================\n",
    "\n",
    "In this exercise, you will continue working with the airline Twitter data. A dataset `tweets` has been imported for you. \n",
    "\n",
    "In some situations, you might have more than one text column in a dataset and you might want to create a numeric representation for each of the text columns. Here, besides the `text` column, which contains the body of the tweet, there is a second text column, called `negativereason`. It contains the reason the customer left a negative review. \n",
    "\n",
    "Your task is to build BOW representations for both columns and specify the required stop words.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the vectorizer package and the default list of English stop words.\n",
    "-   Update the default list of English stop words and create the `my_stop_words` set.\n",
    "-   Specify the stop words argument in the first vectorizer to the updated set, and in the second vectorizer - the default set of English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the vectorizer and default English stop words list\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@', 'am', 'pm'])\n",
    " \n",
    "# Build and fit the vectorizers\n",
    "vect1 = CountVectorizer(stop_words=my_stop_words)\n",
    "vect2 = CountVectorizer(stop_words=ENGLISH_STOP_WORDS) \n",
    "vect1.fit(tweets.text)\n",
    "vect2.fit(tweets.negative_reason)\n",
    "\n",
    "# Print the last 15 features from the first, and all from second vectorizer\n",
    "print(vect1.get_feature_names()[-15:])\n",
    "print(vect2.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
