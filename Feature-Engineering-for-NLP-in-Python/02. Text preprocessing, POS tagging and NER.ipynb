{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing, POS tagging and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Tokenization and Lemmatization\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "00:00 - 00:06\n",
    "\n",
    "In NLP, we usually have to deal with texts from a variety of sources. For instance,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Text sources\n",
    "\n",
    "----------------\n",
    "\n",
    "00:06 - 00:22\n",
    "\n",
    "it can be a news article where the text is grammatically correct and proofread. It could be tweets containing shorthands and hashtags. It could also be comments on YouTube where people have a tendency to abuse capital letters and punctuations.\n",
    "\n",
    "- News articles\n",
    "- Tweets\n",
    "- Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Making text machine friendly\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "00:22 - 01:03\n",
    "\n",
    "It is important that we standardize these texts into a machine friendly format. We want our models to treat similar words as the same. Consider the words Dogs and dog. Strictly speaking, they are different strings. However, they connotate the same thing. Similarly, reduction, reducing and reduce should also be standardized to the same string regardless of their form and case usage. Other examples include don't and do not, and won't and will not. In the next couple of lessons, we will learn techniques to achieve this.\n",
    "\n",
    "- `Dogs,dog`\n",
    "- `reduction`, `REDUCING`, `Reduce`\n",
    "- `don't`, `do not`\n",
    "- `won't`, `will not`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Text preprocessing techniques\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "01:03 - 01:31\n",
    "\n",
    "The text processing techniques you use are dependent on the application you're working on. We'll be covering the common ones, including converting words into lowercase removing unnecessary whitespace, removing punctuation, removing commonly occurring words or stopwords, expanding contracted words like don't and removing special characters such as numbers and emojis.\n",
    "\n",
    "- Converting words into lowercase\n",
    "- Removing leading and trailing whitespaces\n",
    "- Removing punctuation\n",
    "- Removing stopwordsExpanding contractions\n",
    "- Removing special characters (numbers, emojis, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Tokenization\n",
    "\n",
    "----------------\n",
    "\n",
    "01:31 - 02:21\n",
    "\n",
    "To do this, we must first understand tokenization. Tokenization is the process of splitting a string into its constituent tokens. These tokens may be sentences, words or punctuations and is specific to a particular language. In this course, we will primarily be focused with word and punctuation tokens. For instance, consider this sentence. Tokenizing it into its constituent words and punctuations will yield the following list of tokens. Tokenization also involves expanding contracted words. Therefore, a word like don't gets decomposed into two tokens: do and n't as can be seen in this example.\n",
    "\n",
    "\"I have a dog. His name is Hachi.\"\n",
    "Tokens:\n",
    "```python\n",
    "[\"I\", \"have\", \"a\", \"dog\", \".\", \"His\", \"name\", \"is\", \"Hachi\", \".\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Tokenization using spaCy\n",
    "\n",
    "----------------------------\n",
    "\n",
    "02:21 - 03:21\n",
    "\n",
    "To perform tokenization in python, we will use the spacy library. We first import the spacy library. Next, we load a pre-trained English model 'en_core_web_sm' using spacy.load(). This will return a Language object that has the know-how to perform tokenization. This is stored in the variable nlp. Let's now define a string we want to tokenize. We pass this string into nlp to generate a spaCy Doc object. We store this in a variable named doc. This Doc object contains the required tokens (and many other things, as we will soon find out). We generate the list of tokens by using list comprehension as shown. This is essentially looping over doc and extracting the text of each token in each iteration. The result is as follows.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "# Generate list of tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "['Hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', \"'\", 'm', 'doing', 'here', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Lemmatization\n",
    "\n",
    "-----------------\n",
    "\n",
    "03:21 - 04:07\n",
    "\n",
    "Lemmatization is the process of converting a word into its lowercased base form or lemma. This is an extremely powerful process of standardization. For instance, the words reducing, reduces, reduced and reduction, when lemmatized, are all converted into the base form reduce. Similarly be verbs such as am, are and is are converted into be. Lemmatization also allows us to convert words with apostrophes into their full forms. Therefore, n't is converted to not and 've is converted to have.\n",
    "\n",
    "- Convert word into its base form\n",
    "  - reducing, reduces, reduced, reduction → reduce\n",
    "  - am, are, is → be\n",
    "  - n't → not\n",
    "  - 've → have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Lemmatization using spaCy\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "04:07 - 04:42\n",
    "\n",
    "When you pass the string into nlp, spaCy automatically performs lemmatization by default. Therefore, generating lemmas is identical to generating tokens except that we extract token.lemma_ in each iteration inside the list comprehension instead of token.text. Also, observe how spaCy converted the Is into -PRON-. This is standard behavior where every pronoun is converted into the string '-PRON-'.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "# Generate list of lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "['hello', '!', '-PRON-', 'do', 'not', 'know', 'what', '-PRON-', 'be', 'do', 'here', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Let's practice!\n",
    "\n",
    "-------------------\n",
    "\n",
    "04:42 - 05:00\n",
    "\n",
    "Once we understand how to perform tokenization and lemmatization, performing the text preprocessing techniques described earlier becomes easier. Before we move to that, let's first practice our understanding of the concepts introduced so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying lemmas\n",
    "==================\n",
    "\n",
    "Identify the list of words from the choices which do not have the same lemma.\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "- [x] He, She, I, They\n",
    "\n",
    "-   Am, Are, Is, Was\n",
    "\n",
    "-   Increase, Increases, Increasing, Increased\n",
    "\n",
    "-   Car, Bike, Truck, Bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the Gettysburg Address\n",
    "=================================\n",
    "\n",
    "In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War.\n",
    "\n",
    "The entire speech is available as a string named `gettysburg`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Load the `en_core_web_sm` model using `spacy.load()`.\n",
    "-   Create a Doc object `doc` for the `gettysburg` string.\n",
    "-   Using list comprehension, loop over `doc` to generate the token texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing the Gettysburg address\n",
    "==================================\n",
    "\n",
    "In this exercise, we will perform lemmatization on the same `gettysburg` address from before. \n",
    "\n",
    "However, this time, we will also take a look at the speech, before and after lemmatization, and try to adjudge the kind of changes that take place to make the piece more machine friendly.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "Print the gettysburg address to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the gettysburg address\n",
    "print(gettysburg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "\n",
    "\n",
    "Loop over doc and extract the lemma for each token of gettysburg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over doc and extract the lemma for each token of gettysburg.\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "\n",
    "Convert lemmas into a string using join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Text cleaning\n",
    "-----------------\n",
    "\n",
    "00:00 - 00:08\n",
    "\n",
    "Now that we know how to convert a string into a list of lemmas, we are now in a good position to perform basic text cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Text cleaning techniques\n",
    "----------------------------\n",
    "\n",
    "00:08 - 00:31\n",
    "\n",
    "Some of the most common text cleaning steps include removing extra whitespaces, escape sequences, punctuations, special characters such as numbers and stopwords. In other words, it is very common to remove non-alphabetic tokens and words that occur so commonly that they are not very useful for analysis.\n",
    "\n",
    "- Unnecessary whitespaces and escape sequences\n",
    "- Punctuations\n",
    "- Special characters (numbers, emojis, etc.)\n",
    "- Stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. isalpha()\n",
    "-------------\n",
    "\n",
    "00:31 - 01:10\n",
    "\n",
    "Every python string has an isalpha() method that returns true if all the characters of the string are alphabets. Therefore, the \"Dog\".isalpha() will return true but \"3dogs\".isalpha() will return false as it has a non-alphabetic character 3. Similarly, numbers, punctuations and emojis will all return false too. This is an extremely convenient method to remove all (lemmatized) tokens that are or contain numbers, punctuation and emojis.\n",
    "\n",
    "```python\n",
    "\"Dog\".isalpha()\n",
    "```\n",
    "Output:\n",
    "```python\n",
    "True\n",
    "```\n",
    "\n",
    "```python\n",
    "\"3dogs\".isalpha()\n",
    "```\n",
    "Output:\n",
    "```python\n",
    "False\n",
    "```\n",
    "\n",
    "```python\n",
    "\"12347\".isalpha()\n",
    "```\n",
    "Output:\n",
    "```python\n",
    "False\n",
    "```\n",
    "\n",
    "```python\n",
    "\"!\".isalpha()\n",
    "```\n",
    "Output:\n",
    "```python\n",
    "False\n",
    "```\n",
    "\n",
    "```python\n",
    "\"?\".isalpha()\n",
    "```\n",
    "Output:\n",
    "```python\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. A word of caution\n",
    "---------------------\n",
    "\n",
    "01:10 - 01:56\n",
    "\n",
    "If isalpha() as a silver bullet that cleans text meticulously seems too good to be true, it's because it is. Remember that isalpha() has a tendency of returning false on words we would not want to remove. Examples include abbreviations such as USA and UK which have periods in them, and proper nouns with numbers in them such as word2vec and xto10x. For such nuanced cases, isalpha() may not be sufficient. It may be advisable to write your own custom functions, typically using regular expressions, to ensure you're not inadvertently removing useful words.\n",
    "\n",
    "```markdown\n",
    "- Abbreviations: U.S.A, U.K, etc.\n",
    "- Proper Nouns: word2vec and xto10x.\n",
    "- Write your own custom function (using regex) for the more nuanced cases.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Removing non-alphabetic characters\n",
    "--------------------------------------\n",
    "\n",
    "01:56 - 02:13\n",
    "\n",
    "Consider the string here. This has a lot of punctuations, unnecessary extra whitespace, escape sequences, numbers and emojis. We will generate the lemmatized tokens like before.\n",
    "\n",
    "```python\n",
    "string = \"\"\"\n",
    "OMG!!!! This is like    the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
    "\"\"\"\n",
    "import spacy\n",
    "\n",
    "# Generate list of tokens\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "['OMG', '!', 'This', 'be', 'like', 'the', 'best', 'thing', 'ever', '\\n', '.', 'Wow', ',', 'such', 'an', 'amazing', 'song', '!', 'I', \"'m\", 'hook', '.', 'Top', '5', 'definitely', '.', '?']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Removing non-alphabetic characters\n",
    "--------------------------------------\n",
    "\n",
    "02:13 - 02:35\n",
    "\n",
    "Next, we loop through the tokens again and choose only those words that are either -PRON- or contain only alphabetic characters. Let's now print out the sanitized string. We see that all the non-alphabetic characters have been removed and each word is separated by a single space.\n",
    "\n",
    "```python\n",
    "# Remove tokens that are not alphabetic\n",
    "a_lemmas = [lemma for lemma in lemmas\n",
    "            if lemma.isalpha() or lemma == '-PRON-']\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "'OMG this be like the good thing ever wow such an amazing song -PRON- be hooked top definitely'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Stopwords\n",
    "-------------\n",
    "\n",
    "02:35 - 02:55\n",
    "\n",
    "There are some words in the English language that occur so commonly that it is often a good idea to just ignore them. Examples include articles such as a and the, be verbs such as is and am and pronouns such as he and she.\n",
    "\n",
    "- Words that occur extremely commonly\n",
    "- Eg. articles, be verbs, pronouns, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Removing stopwords using spaCy\n",
    "----------------------------------\n",
    "\n",
    "02:55 - 03:03\n",
    "\n",
    "spaCy has a built-in list of stopwords which we can access using spacy.lang.en.stop_words.STOP_WORDS..\n",
    "\n",
    "# Get list of stopwords\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "string = \"\"\"\n",
    "OMG!!!! This is like    the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Removing stopwords using spaCy\n",
    "----------------------------------\n",
    "\n",
    "03:03 - 03:49\n",
    "\n",
    "We make a small tweak to a_lemmas generation step. Notice that we have removed the -PRON- condition as pronouns are stopwords anyway and should be removed. Additionally, we have introduced a new condition to check if the word belongs to spacy's list of stopwords. The output is as follows. Notice how the string consists only of base form words. Always exercise caution while using third party stopword lists. It is common that an application find certain words useful that may be considered a stopword by third party lists. It is often advisable to create your custom stopword lists.\n",
    "\n",
    "```python\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas\n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "'OMG like good thing wow amazing song hooked definitely'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Other text preprocessing techniques\n",
    "----------------------------------------\n",
    "\n",
    "03:49 - 04:07\n",
    "\n",
    "There are other preprocessing techniques that are used but have been omitted for the sake of brevity. Some of them include removing HTML or XML tags, replacing accented characters and correcting spelling errors and shorthands\n",
    "\n",
    "- Removing HTML/XML tags\n",
    "- Replacing accented characters (such as é)\n",
    "- Correcting spelling errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. A word of caution\n",
    "----------------------\n",
    "\n",
    "04:07 - 04:42\n",
    "\n",
    "We have covered a lot of text preprocessing techniques in the last couple of lessons. However, a word of caution is in place. The text preprocessing techniques you use is always dependent on the application. There are many applications which may find punctuations, numbers and emojis useful, so it may be wise to not remove them. In other cases, using all caps may be a good indicator of something. Remember to always use only those techniques that are relevant to your particular use case.\n",
    "\n",
    "- Always use only those text preprocessing techniques that are relevant to your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:42 - 04:45\n",
    "\n",
    "It's now time to practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning a blog post\n",
    "====================\n",
    "\n",
    "In this exercise, you have been given an excerpt from a blog post. Your task is to clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters.\n",
    "\n",
    "The excerpt is available as a string `blog` and has been printed to the console. The list of stopwords are available as `stopwords`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Using list comprehension, loop through `doc`to extract the `lemma_` of each token.\n",
    "-   Remove stopwords and non-alphabetic tokens using `stopwords` and `isalpha()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning TED talks in a dataframe\n",
    "=================================\n",
    "\n",
    "In this exercise, we will revisit the TED Talks from the first chapter. You have been a given a dataframe `ted` consisting of 5 TED Talks. Your task is to clean these talks using techniques discussed earlier by writing a function `preprocess` and applying it to the `transcript` feature of the dataframe. \n",
    "\n",
    "The stopwords list is available as `stopwords`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Generate the Doc object for `text`. Ignore the `disable` argument for now.\n",
    "-   Generate lemmas using list comprehension using the `lemma_` attribute.\n",
    "-   Remove non-alphabetic characters using `isalpha()` in the if condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Part-of-speech tagging\n",
    "--------------------------\n",
    "\n",
    "00:00 - 00:08\n",
    "\n",
    "In this lesson, we will cover part-of-speech tagging, which is one of the most popularly used feature engineering techniques in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Applications\n",
    "----------------\n",
    "\n",
    "00:08 - 01:14\n",
    "\n",
    "Part-of speech tagging or POS tagging has an immense number of applications in NLP. It is used in word-sense disambiguation to identify the sense of a word in a sentence. For instance, consider the sentences \"the bear is a majestic animal\" and \"please bear with me\". Both sentences use the word 'bear' but they mean different things. POS tagging helps in identifying this distinction by identifying one bear as a noun and the other as a verb. Consequentially, POS tagging is also used in sentiment analysis, question answering systems and linguistic approaches to detect fake news and opinion spam. For example, one paper discovered that fake news headlines, on average, tend to use lesser common nouns and more proper nouns than mainstream headlines. Generating the POS tags for these words proved extremely useful in detecting false or hyperpartisan news.\n",
    "\n",
    "```markdown\n",
    "- Word-sense disambiguation\n",
    "  - \"The bear is a majestic animal\"\n",
    "  - \"Please bear with me\"\n",
    "- Sentiment analysis\n",
    "- Question answering\n",
    "- Fake news and opinion spam detection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. POS tagging\n",
    "---------------\n",
    "\n",
    "01:14 - 01:45\n",
    "\n",
    "So what is POS tagging? It is the process of assigning every word (or token) in a piece of text, its corresponding part-of-speech. For instance, consider the sentence \"Jane is an amazing guitarist\". A typical POS tagger will label Jane as a proper noun, is as a verb, an as a determiner (or an article), amazing as an adjective and finally, guitarist as a noun.\n",
    "\n",
    "```markdown\n",
    "- Assigning every word, its corresponding part of speech.\n",
    "  - \"Jane is an amazing guitarist.\"\n",
    "- POS Tagging:\n",
    "  - Jane → proper noun\n",
    "  - is → verb\n",
    "  - an → determiner\n",
    "  - amazing → adjective\n",
    "  - guitarist → noun\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. POS tagging using spaCy\n",
    "---------------------------\n",
    "\n",
    "01:45 - 02:14\n",
    "\n",
    "POS Tagging is extremely easy to do using spaCy's models and performing it is almost identical to generating tokens or lemmas. As usual, we import the spacy library and load the en_core_web_sm model as nlp. We will use the same sentence \"Jane is an amazing guitarist\" from before. We will then create a Doc object that will perform POS tagging, by default.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize string\n",
    "string = \"Jane is an amazing guitarist\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. POS tagging using spaCy\n",
    "---------------------------\n",
    "\n",
    "02:14 - 03:08\n",
    "\n",
    "Using list comprehension, we generate a list of tuples pos where the first element of the tuple is the token and is generated using token.text and the second element is its POS tag, which is generated using token.pos_. Printing pos will give us the following output. Note how the tagger correctly identified all the parts-of-speech as we had discussed earlier. That said, remember that POS tagging is not an exact science. spaCy infers the POS tags of these words based on the predictions given by its pre-trained models. In other words, the accuracy of the POS tagging is dependent on the data that the model has been trained on and the data that it is being used on.\n",
    "\n",
    "```python\n",
    "# Generate list of tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)\n",
    "\n",
    "[('Jane', 'PROPN'),\n",
    " ('is', 'VERB'),\n",
    " ('an', 'DET'),\n",
    " ('amazing', 'ADJ'),\n",
    " ('guitarist', 'NOUN')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. POS annotations in spaCy\n",
    "----------------------------\n",
    "\n",
    "03:08 - 03:39\n",
    "\n",
    "spaCy is capable of identifying close to 20 parts-of-speech and as we saw in the previous slide, it uses specific annotations to denote a particular part of speech. For instance, PROPN referred to a proper noun and DET referred to a determinant. You can find the complete list of POS annotations used by spaCy in spaCy's documentation. Here is a snapshot of the web page.\n",
    "\n",
    "```markdown\n",
    "- PROPN → proper noun\n",
    "- DET → determiner\n",
    "- spaCy annotations at [https://spacy.io/api/annotation](https://spacy.io/api/annotation)\n",
    "\n",
    "| POS   | DESCRIPTION          | EXAMPLES                              |\n",
    "|-------|----------------------|---------------------------------------|\n",
    "| ADJ   | adjective            | big, old, green, incomprehensible, first |\n",
    "| ADP   | adposition           | in, to, during                       |\n",
    "| ADV   | adverb               | very, tomorrow, down, where, there   |\n",
    "| AUX   | auxiliary            | is, has (done), will (do), should (do) |\n",
    "| CONJ  | conjunction          | and, or, but                         |\n",
    "| CCONJ | coordinating conjunction | and, or, but                         |\n",
    "| DET   | determiner           | a, an, the                           |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:39 - 03:47\n",
    "\n",
    "Great! Let's now practice our understanding of POS tagging in the next few exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging in Lord of the Flies\n",
    "================================\n",
    "\n",
    "In this exercise, you will perform part-of-speech tagging on a famous passage from one of the most well-known novels of all time, *Lord of the Flies*, authored by William Golding.\n",
    "\n",
    "The passage is available as `lotf` and has already been printed to the console.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Load the `en_core_web_sm` model.\n",
    "-   Create a doc object for `lotf` using `nlp()`.\n",
    "-   Using the `text` and `pos_` attributes, generate tokens and their corresponding POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting nouns in a piece of text\n",
    "=================================\n",
    "\n",
    "In this exercise, we will write two functions, `nouns()` and `proper_nouns()` that will count the number of other nouns and proper nouns in a piece of text respectively.\n",
    "\n",
    "These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news. \n",
    "\n",
    "The `en_core_web_sm` model has already been loaded as `nlp` in this exercise.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Using the list `count` method, count the number of proper nouns (annotated as `PROPN`) in the `pos` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Using the list `count` method, count the number of other nouns (annotated as `NOUN`) in the `pos` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count(\"NOUN\")  # Count the number of NOUN tags\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun usage in fake news\n",
    "=======================\n",
    "\n",
    "In this exercise, you have been given a dataframe `headlines` that contains news headlines that are either fake or real. Your task is to generate two new features `num_propn`and `num_noun` that represent the number of proper nouns and other nouns contained in the `title` feature of `headlines`.\n",
    "\n",
    "Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the `num_propn` and `num_noun` features in fake news detectors will improve its performance.\n",
    "\n",
    "To accomplish this task, the functions `proper_nouns` and `nouns` that you had built in the previous exercise have already been made available to you.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   -   Create a new feature `num_propn` by applying `proper_nouns` to `headlines['title']`.\n",
    "    -   Filter `headlines` to compute the mean number of proper nouns in fake news using the `mean` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   -   Repeat the process for other nous: create a feature `'num_noun'` using `nouns` and compute the mean of other nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Named entity recognition\n",
    "----------------------------\n",
    "\n",
    "00:00 - 00:06\n",
    "\n",
    "The final technique we will learn as part of this chapter is named entity recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Applications\n",
    "----------------\n",
    "\n",
    "00:06 - 00:48\n",
    "\n",
    "Named entity recognition or NER has a host of extremely useful applications. It is used to build efficient search algorithms and question answering systems. For instance, let us say you have a piece of text and you ask your system about the people that are being talked about in the text. NER would help the system in answering this question by identifying all the entities that refer to a person in the text. NER also found application with News Providers who use it to categorize their articles and Customer Service centers who use it to classify and record their complaints efficiently.\n",
    "\n",
    "```markdown\n",
    "- Word-sense disambiguation\n",
    "  - \"The bear is a majestic animal\"\n",
    "  - \"Please bear with me\"\n",
    "- Sentiment analysis\n",
    "- Question answering\n",
    "- Fake news and opinion spam detection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Named entity recognition\n",
    "----------------------------\n",
    "\n",
    "00:48 - 01:38\n",
    "\n",
    "Let us now get down to the definitions. A named entity is anything that can be denoted with a proper name or a proper noun. Named entity recognition or NER, therefore, is the process of identifying such named entities in a piece of text and classifying them into predefined categories such as person, organization, country, etc. For example, consider the text \"John Doe is a software engineer working at Google. He lives in France.\" Performing NER on this text will tell us that there are three named entities: John Doe, who is a person, Google, which is an organization and France, which is a country (or geopolitical entity)\n",
    "\n",
    "```markdown\n",
    "- Identifying and classifying named entities into predefined categories.\n",
    "  - Categories include person, organization, country, etc.\n",
    "- \"John Doe is a software engineer working at Google. He lives in France.\"\n",
    "- Named Entities:\n",
    "  - John Doe → person\n",
    "  - Google → organization\n",
    "  - France → country (geopolitical entity)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. NER using spaCy\n",
    "-------------------\n",
    "\n",
    "01:38 - 02:40\n",
    "\n",
    "Like POS tagging, performing NER is extremely easy using spaCy's pre-trained models. Let's try to find the named entities in the same sentence we used earlier. As usual, we import the spacy library, load the required model and create a Doc object for the string. When we do this, spaCy automatically computes all the named entities and makes it available as the ents attribute of doc. Therefore, to access the named entity and its category, we use list comprehension to loop over doc.ents and create a tuple containing the entity name, which is accessed using ent.text, and entity category, which is accessed using ent.label_. Printing this list out will give the following output. We see that spaCy has correctly identified and classified all the named entities in this string.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "string = \"John Doe is a software engineer working at Google. He lives in France.\"\n",
    "\n",
    "# Load model and create Doc object\n",
    "spacy.Load('en_core_web_sm')\n",
    "nlp(string)\n",
    "\n",
    "nlp\n",
    "\n",
    "doc\n",
    "\n",
    "# Generate named entities\n",
    "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(ne)\n",
    "\n",
    "[('John Doe', 'PERSON'), ('Google', 'ORG'), ('France', 'GPE')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. NER annotations in spaCy\n",
    "----------------------------\n",
    "\n",
    "02:40 - 03:00\n",
    "\n",
    "Currently, spaCy's models are capable of identifying more than 15 different types of named entities. The complete list of categories and their annotations can be found in spaCy's documentatiion. Here is a snapshot of the page.\n",
    "\n",
    "```markdown\n",
    "- More than 15 categories of named entities\n",
    "- NER annotations at [https://spacy.io/api/annotation#named-entities](https://spacy.io/api/annotation#named-entities)\n",
    "\n",
    "| TYPE  | DESCRIPTION                                      |\n",
    "|-------|--------------------------------------------------|\n",
    "| PERSON | People, including fictional.                    |\n",
    "| NORP   | Nationalities or religious or political groups. |\n",
    "| FAC    | Buildings, airports, highways, bridges, etc.    |\n",
    "| ORG    | Companies, agencies, institutions, etc.         |\n",
    "| GPE    | Countries, cities, states.                      |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. A word of caution\n",
    "---------------------\n",
    "\n",
    "03:00 - 03:54\n",
    "\n",
    "In this chapter, we have used spacy's models to accomplish several tasks. However, remember that spacy's models are not perfect and its performance depends on the data it was trained with and the data it is being used on. For instance, if we are trying extract named entities for texts from a heavily technical field, such as medicine, spacy's pretrained models may not perform such a great job. In such nuanced cases, it is better to train your models with your specialized data. Also, remember that spacy's models are language specific. This is understandable considering that each language has its own grammar and nuances. The en_core_web_sm model that we've been using is, as the name suggests, only suitable for English texts.\n",
    "\n",
    "```markdown\n",
    "- Not perfect\n",
    "- Performance dependent on training and test data\n",
    "- Train models with specialized data for nuanced cases\n",
    "- Language specific\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:54 - 04:05\n",
    "\n",
    "This concludes our lesson on named entity recognition. Let us practice our understanding of this technique in the exercises."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
