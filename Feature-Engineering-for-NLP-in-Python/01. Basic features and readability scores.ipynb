{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Introduction to NLP feature engineering\n",
    "-------------------------------------------\n",
    "\n",
    "00:00 - 00:18\n",
    "\n",
    "Welcome to Feature Engineering for NLP in Python! I am Rounak and I will be your instructor for this course. In this course, you will learn to extract useful features out of text and convert them into formats that are suitable for machine learning algorithms.\n",
    "\n",
    "2\\. Numerical data\n",
    "------------------\n",
    "\n",
    "00:18 - 00:44\n",
    "\n",
    "For any ML algorithm, data fed into it must be in tabular form and all the training features must be numerical. Consider the Iris dataset. Every training instance has exactly four numerical features. The ML algorithm uses these four features to train and predict if an instance belongs to class iris-virginica, iris-setosa or iris-versicolor.\n",
    "\n",
    "```markdown\n",
    "# Iris dataset\n",
    "\n",
    "| sepal length | sepal width | petal length | petal width | class           |\n",
    "|--------------|-------------|--------------|-------------|-----------------|\n",
    "| 6.3          | 2.9         | 5.6          | 1.8         | Iris-virginica  |\n",
    "| 4.9          | 3.0         | 1.4          | 0.2         | Iris-setosa     |\n",
    "| 5.6          | 2.9         | 3.6          | 1.3         | Iris-versicolor |\n",
    "| 6.0          | 2.7         | 5.1          | 1.6         | Iris-versicolor |\n",
    "| 7.2          | 3.6         | 6.1          | 2.5         | Iris-virginica  |\n",
    "```\n",
    "\n",
    "3\\. One-hot encoding\n",
    "--------------------\n",
    "\n",
    "00:44 - 01:01\n",
    "\n",
    "ML algorithms can also work with categorical data provided they are converted into numerical form through one-hot encoding. Let's say you have a categorical feature 'sex' with two categories 'male' and 'female'.\n",
    "\n",
    "```markdown\n",
    "| sex    |\n",
    "|--------|\n",
    "| female |\n",
    "| male   |\n",
    "| female |\n",
    "| male   |\n",
    "| female |\n",
    "| ...    |\n",
    "```\n",
    "\n",
    "4\\. One-hot encoding\n",
    "--------------------\n",
    "\n",
    "01:01 - 01:05\n",
    "\n",
    "One-hot encoding will convert this feature into two features,\n",
    "\n",
    "```markdown\n",
    "| sex    | one-hot encoding |\n",
    "|--------|------------------|\n",
    "| female | →                |\n",
    "| male   | →                |\n",
    "| female | →                |\n",
    "| male   | →                |\n",
    "| female | →                |\n",
    "| ...    | ...              |\n",
    "```\n",
    "\n",
    "5\\. One-hot encoding\n",
    "--------------------\n",
    "\n",
    "01:05 - 01:17\n",
    "\n",
    "'sex_male' and 'sex_female' such that each male instance has a 'sex_male' value of 1 and 'sex_female' value of 0. For females, it is the vice versa.\n",
    "\n",
    "| sex | one-hot encoding | sex_female | sex_male |\n",
    "| --- | ---------------- | ---------- | -------- |\n",
    "| female | → | 1 | 0 |\n",
    "| male | → | 0 | 1 |\n",
    "| female | → | 1 | 0 |\n",
    "| male | → | 0 | 1 |\n",
    "| female | → | 1 | 0 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "6\\. One-hot encoding with pandas\n",
    "--------------------------------\n",
    "\n",
    "01:17 - 01:54\n",
    "\n",
    "To do this in code, we use pandas' get_dummies() function. Let's import pandas using the alias pd. We can then pass our dataframe df into the pd.get_dummies() function and pass a list of features to be encoded as the columns argument. Not mentioning columns will lead pandas to automatically encode all non-numerical features. Finally, we overwrite the original dataframe with the encoded version by assigning the dataframe returned by get_dummies() back to df.\n",
    "\n",
    "```python\n",
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Perform one-hot encoding on the 'sex' feature of df\n",
    "df = pd.get_dummies(df, columns=['sex'])\n",
    "```\n",
    "\n",
    "7\\. Textual data\n",
    "----------------\n",
    "\n",
    "01:54 - 02:10\n",
    "\n",
    "Consider a movie reviews dataset. This data cannot be utilized by any machine learning or ML algorithm. The training feature 'review' isn't numerical. Neither is it categorical to perform one-hot encoding on.\n",
    "\n",
    "#### Movie Review Dataset\n",
    "\n",
    "| review | class |\n",
    "| --- | --- |\n",
    "| This movie is for dog lovers. A very poignant... | positive |\n",
    "| The movie is forgettable. The plot lacked... | negative |\n",
    "| A truly amazing movie about dogs. A gripping... | positive |\n",
    "\n",
    "8\\. Text pre-processing\n",
    "-----------------------\n",
    "\n",
    "02:10 - 02:34\n",
    "\n",
    "We need to perform two steps to make this dataset suitable for ML. The first is to standardize the text. This involves steps like converting words to lowercase and their base form. For instance, 'Reduction' gets lowercased and then converted to its base form, reduce. We will cover these concepts in more detail in subsequent lessons.\n",
    "\n",
    "- Converting to lowercase\n",
    "    - Example:`Reduction` to `reduction`\n",
    "- Converting to base-form\n",
    "    - Example:`reduction` to `reduce`\n",
    "\n",
    "\n",
    "9\\. Vectorization\n",
    "-----------------\n",
    "\n",
    "02:34 - 02:48\n",
    "\n",
    "After preprocessing, the reviews are converted into a set of numerical training features through a process known as vectorization. After vectorization, our original review dataset gets converted\n",
    "\n",
    "| review | class |\n",
    "| --- | --- |\n",
    "| This movie is for dog lovers. A very poignant... | positive |\n",
    "| The movie is forgettable. The plot lacked... | negative |\n",
    "| A truly amazing movie about dogs. A gripping... | positive |\n",
    "\n",
    "10\\. Vectorization\n",
    "------------------\n",
    "\n",
    "02:48 - 02:55\n",
    "\n",
    "into something like this. We will learn techniques to achieve this in later lessons.\n",
    "\n",
    "| 0 | 1 | 2 | ... | n | class |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 0.03 | 0.71 | 0.00 | ... | 0.22 | positive |\n",
    "| 0.45 | 0.00 | 0.03 | ... | 0.19 | negative |\n",
    "| 0.14 | 0.18 | 0.00 | ... | 0.45 | positive |\n",
    "\n",
    "11\\. Basic features\n",
    "-------------------\n",
    "\n",
    "02:55 - 03:20\n",
    "\n",
    "We can also extract certain basic features from text. It maybe useful to know the word count, character count and average word length of a particular text. While working with niche data such as tweets, it also maybe useful to know how many hashtags have been used in a tweet. This tweet by Silverado Records,for instance, uses two.\n",
    "\n",
    "- Number of words\n",
    "- Number of characters\n",
    "- Average length of words\n",
    "- Tweets\n",
    "\n",
    "```markdown\n",
    "testbook @books\n",
    "What book are ypu guys reading?\n",
    "\n",
    "#books #reading\n",
    "```\n",
    "\n",
    "12\\. POS tagging\n",
    "----------------\n",
    "\n",
    "03:20 - 03:50\n",
    "\n",
    "So far, we have seen how to extract features out of an entire body of text. Some NLP applications may require you to extract features for individual words. For instance, you may want to do parts-of-speech tagging to know the different parts-of-speech present in your text as shown. As an example, consider the sentence 'I have a dog'. POS tagging will label each word with its corresponding part-of-speech.\n",
    "\n",
    "| Word | POS |\n",
    "| --- | --- |\n",
    "| I | Pronoun |\n",
    "| have | Verb |\n",
    "| a | Article |\n",
    "| dog | Noun |\n",
    "\n",
    "13\\. Named Entity Recognition\n",
    "-----------------------------\n",
    "\n",
    "03:50 - 04:16\n",
    "\n",
    "You may also want to know perform named entity recognition to find out if a particular noun is referring to a person, organization or country. For instance, consider the sentence \"Brian works at DataCamp\". Here, there are two nouns \"Brian\" and \"DataCamp\". Brian refers to a person whereas DataCamp refers to an organization.\n",
    "\n",
    "#### Noun Reference\n",
    "\n",
    "The image asks whether nouns refer to persons, organizations, or countries.\n",
    "\n",
    "| Noun | NER |\n",
    "| --- | --- |\n",
    "| Brian | Person |\n",
    "| DataCamp | Organization |\n",
    "\n",
    "The image contains three main visual elements:\n",
    "1. A photo of a person playing a guitar, which is likely a reference to the \"Brian\" noun.\n",
    "2. A Swiss flag, which could represent a country.\n",
    "3. The TED logo, which represents the organization \"TED\".\n",
    "\n",
    "Based on the table in the image, the nouns \"Brian\" and \"DataCamp\" are classified as referring to a person and an organization, respectively. The image is prompting the viewer to consider whether nouns generally refer to persons, organizations, or countries.\n",
    "\n",
    "14\\. Concepts covered\n",
    "---------------------\n",
    "\n",
    "04:16 - 04:33\n",
    "\n",
    "Therefore, broadly speaking, this course will teach you how to conduct text preprocessing, extract certain basic features, word features and convert documents into a set of numerical features (using a process known as vectorization).\n",
    "\n",
    "- Text Preprocessing\n",
    "- Basic Features\n",
    "- Word Features\n",
    "- Vectorization\n",
    "\n",
    "\n",
    "15\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:33 - 04:36\n",
    "\n",
    "Great! Now, let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format for ML algorithms\n",
    "=============================\n",
    "\n",
    "In this exercise, you have been given four dataframes `df1`, `df2`, `df3` and `df4`. The final column of each dataframe is the predictor variable and the rest of the columns are training features. \n",
    "\n",
    "Using the console, determine which dataframe is in a suitable format to be trained by a classifier.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "`df1`\n",
    "\n",
    "`df2`\n",
    "\n",
    "[/] `df3`\n",
    "\n",
    "`df4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding\n",
    "================\n",
    "\n",
    "In the previous exercise, we encountered a dataframe `df1` which contained categorical features and therefore, was unsuitable for applying ML algorithms to.\n",
    "\n",
    "In this exercise, your task is to convert `df1`into a format that is suitable for machine learning.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "-   Use the `columns` attribute to print the features of `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "----------------\n",
    "\n",
    "-   Use the `pd.get_dummies()` function to perform one-hot encoding on `feature 5` of `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "----------------\n",
    "\n",
    "-   Use the `columns` attribute again to print the new features of `df1`.\n",
    "-   Print the first five rows of `df1` using `head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])\n",
    "\n",
    "# Print the new features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Print first five rows of df1\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Basic feature extraction\n",
    "----------------------------\n",
    "\n",
    "00:00 - 00:11\n",
    "\n",
    "In this video, we will learn to extract certain basic features from text. While not very powerful, they can give us a good idea of the text we are dealing with.\n",
    "\n",
    "2\\. Number of characters\n",
    "------------------------\n",
    "\n",
    "00:11 - 00:58\n",
    "\n",
    "The most basic feature we can extract from text is the number of characters, including whitespaces. For instance, the string \"I don't know.\" has 13 characters. The number of characters is the length of the string. Python gives us a built-in len() function which returns the length of the string passed into it. The output will be 13 here too. If our dataframe df has a textual feature (say 'review'), we can compute the number of characters for each review and store it as a new feature 'num_chars' by using the pandas dataframe apply method. This is done by creating df['num_chars'] and assigning it to df['review'].apply(len).\n",
    "\n",
    "```python\n",
    "\"I don't know.\" # 13 characters\n",
    "# Compute the number of characters \n",
    "text = \"I don't know.\"num_char = len(text)\n",
    "# Print the number of charactersprint\n",
    "(num_char)\n",
    "\n",
    "13\n",
    "\n",
    "# Create a 'num_chars' feature\n",
    "df['num_chars'] = df['review'].apply(len)\n",
    "```\n",
    "\n",
    "3\\. Number of words\n",
    "-------------------\n",
    "\n",
    "00:58 - 01:32\n",
    "\n",
    "Another feature we can compute is the number of words. Assuming that every word is separated by a space, we can use a string's split() method to convert it into a list where every element is a word. In this example, the string Mary had a little lamb is split to create a list containing the words Mary, had, a, little and lamb. We can now compute the number of words by computing the number of elements in this list using len().\n",
    "\n",
    "```python\n",
    "# Split the string into words\n",
    "text = \"Mary had a little lamb.\"\n",
    "words = text.split()\n",
    "# Print the list containing words\n",
    "print(words)\n",
    "\n",
    "['Mary', 'had', 'a', 'little', 'lamb.']\n",
    "\n",
    "# Print number of words\n",
    "print(len(words))5\n",
    "\n",
    "Output :\n",
    "5\n",
    "```\n",
    "\n",
    "4\\. Number of words\n",
    "-------------------\n",
    "\n",
    "01:32 - 01:58\n",
    "\n",
    "To do this for a textual feature in a dataframe, we first define a function that takes in a string as an argument and returns the number of words in it. The steps followed inside the function are similar as before. We then pass this function word_count into apply. We create df['num_words'] and assign it to df['review'].apply(word_count).\n",
    "\n",
    "```python\n",
    "# Function that returns number of words in string\n",
    "def word_count(string):\n",
    "    # Split the string into words    \n",
    "    words = string.split()\n",
    "    # Return length of words list\n",
    "    returnlen(words)\n",
    "# Create num_words feature in df\n",
    "df['num_words'] = df['review'].apply(word_count)\n",
    "```\n",
    "\n",
    "5\\. Average word length\n",
    "-----------------------\n",
    "\n",
    "01:58 - 02:24\n",
    "\n",
    "Let's now compute the average length of words in a string. Let's define a function avg_word_length() which takes in a string and returns the average word length. We first split the string into words and compute the length of each word. Next, we compute the average word length by dividing the sum of the lengths of all words by the number of words.\n",
    "\n",
    "```python\n",
    "#Function that returns average word length\n",
    "def avg_word_length(x):\n",
    "    # Split the string into words    \n",
    "    words = x.split()\n",
    "    # Compute length of each word and store in a separate list \n",
    "    word_lengths = [len(word) for word in words]\n",
    "    # Compute average word length    \n",
    "    avg_word_length = sum(word_lengths)/len(words)\n",
    "    # Return average word length\n",
    "    return(avg_word_length)\n",
    "```\n",
    "\n",
    "6\\. Average word length\n",
    "-----------------------\n",
    "\n",
    "02:24 - 02:31\n",
    "\n",
    "We can now pass this into apply() to generate a average word length feature like before.\n",
    "\n",
    "```python\n",
    "# Create a new feature avg_word_length\n",
    "df['avg_word_length'] = df['review'].apply(doc_density)\n",
    "```\n",
    "\n",
    "7\\. Special features\n",
    "--------------------\n",
    "\n",
    "02:31 - 02:52\n",
    "\n",
    "When working with data such as tweets, it maybe useful to compute the number of hashtags or mentions used. This tweet by DataCamp, for instance, has one mention upendra_35 which begins with an @ and two hashtags, PySpark and Spark which begin with a #.\n",
    "\n",
    "```markdown\n",
    "Tweet: \n",
    "Datacamp @Datacamp\n",
    "\n",
    "Big data Fundamentals via PySpark. #BigData #pySpark\n",
    "```\n",
    "\n",
    "8\\. Hashtags and mentions\n",
    "-------------------------\n",
    "\n",
    "02:52 - 03:44\n",
    "\n",
    "Let's write a function that computes the number of hashtags in a string. We split the string into words. We then use list comprehension to create a list containing only those words that are hashtags. We do this using the startswith method of strings to find out if a word begins with #. The final step is to return the number of elements in this list using len. The procedure to compute number of mentions is identical except that we check if a word starts with @. Let's see this function in action. When we pass a string \"@janedoe This is my first tweet! #FirstTweet #Happy\", the function returns 2 which is indeed the number of hashtags in the string.\n",
    "\n",
    "```python\n",
    "# Function that returns number of hashtags\n",
    "def hashtag_count(string):\n",
    "    # Split the string into words    \n",
    "    words = string.split()\n",
    "    # Create a list of hashtags    \n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    # Return number of hashtags\n",
    "    returnlen(hashtags)\n",
    "    \n",
    "hashtag_count(\"@janedoe This is my first tweet! #FirstTweet #Happy\")\n",
    "\n",
    "Output:\n",
    "2\n",
    "```\n",
    "\n",
    "\n",
    "9\\. Other features\n",
    "------------------\n",
    "\n",
    "03:44 - 04:04\n",
    "\n",
    "There are other basic features we can compute such as number of sentences, number of paragraphs, number of words starting with an uppercase, all-capital words, numeric quantities etc. The procedure to do this is extremely similar to the ones we've already covered.\n",
    "\n",
    "- Number of sentences\n",
    "- Number of paragraphs\n",
    "- Words starting with an uppercase\n",
    "- All-capital words\n",
    "- Numeric quantities\n",
    "\n",
    "\n",
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:04 - 04:09\n",
    "\n",
    "That's enough theory for now. Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character count of Russian tweets\n",
    "=================================\n",
    "\n",
    "In this exercise, you have been given a dataframe `tweets` which contains some tweets associated with Russia's Internet Research Agency and compiled by FiveThirtyEight. \n",
    "\n",
    "Your task is to create a new feature 'char_count' in `tweets` which computes the number of characters for each tweet. Also, compute the average length of each tweet. The tweets are available in the `content`feature of `tweets`.\n",
    "\n",
    "*Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data).*\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create a new feature `char_count` by applying `len` to the 'content' feature of `tweets`.\n",
    "-   Print the average character count of the tweets by computing the mean of the 'char_count' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word count of TED talks\n",
    "=======================\n",
    "\n",
    "`ted` is a dataframe that contains the transcripts of 500 TED talks. Your job is to compute a new feature `word_count` which contains the approximate number of words for each talk. Consequently, you also need to compute the average word count of the talks. The transcripts are available as the `transcript` feature in `ted`.\n",
    "\n",
    "In order to complete this task, you will need to define a function `count_words` that takes in a string as an argument and returns the number of words in the string. You will then need to apply this function to the `transcript` feature of `ted` to create the new feature `word_count`and compute its mean.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Split `string` into a list of words using the `split()` method.\n",
    "-   Return the number of elements in `words`using `len()`.\n",
    "-   Apply your function to the `transcript`column of `ted` to create the new feature `word_count`.\n",
    "-   Compute the average word count of the talks using `mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtags and mentions in Russian tweets\n",
    "=======================================\n",
    "\n",
    "Let's revisit the `tweets` dataframe containing the Russian tweets. In this exercise, you will compute the number of hashtags and mentions in each tweet by defining two functions `count_hashtags()` and `count_mentions()`respectively and applying them to the `content` feature of `tweets`. \n",
    "\n",
    "In case you don't recall, the tweets are contained in the `content` feature of `tweets`.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   In the list comprehension, use `startswith()` to check if a particular `word` starts with `'#'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   In the list comprehension, use `startswith()` to check if a particular `word` starts with '@'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
