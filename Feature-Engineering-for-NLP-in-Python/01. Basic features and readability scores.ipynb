{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Introduction to NLP feature engineering\n",
    "-------------------------------------------\n",
    "\n",
    "00:00 - 00:18\n",
    "\n",
    "Welcome to Feature Engineering for NLP in Python! I am Rounak and I will be your instructor for this course. In this course, you will learn to extract useful features out of text and convert them into formats that are suitable for machine learning algorithms.\n",
    "\n",
    "2\\. Numerical data\n",
    "------------------\n",
    "\n",
    "00:18 - 00:44\n",
    "\n",
    "For any ML algorithm, data fed into it must be in tabular form and all the training features must be numerical. Consider the Iris dataset. Every training instance has exactly four numerical features. The ML algorithm uses these four features to train and predict if an instance belongs to class iris-virginica, iris-setosa or iris-versicolor.\n",
    "\n",
    "#### Iris dataset\n",
    "\n",
    "| sepal length | sepal width | petal length | petal width | class |\n",
    "|--------------|-------------|--------------|-------------|-------|\n",
    "| 6.3 | 2.9 | 5.6 | 1.8 | Iris-virginica |\n",
    "| 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa |\n",
    "| 5.6 | 2.9 | 3.6 | 1.3 | Iris-versicolor |\n",
    "| 6.0 | 2.7 | 5.1 | 1.6 | Iris-versicolor |\n",
    "| 7.2 | 3.6 | 6.1 | 2.5 | Iris-virginica |\n",
    "\n",
    "3\\. One-hot encoding\n",
    "--------------------\n",
    "\n",
    "00:44 - 01:01\n",
    "\n",
    "ML algorithms can also work with categorical data provided they are converted into numerical form through one-hot encoding. Let's say you have a categorical feature 'sex' with two categories 'male' and 'female'.\n",
    "\n",
    "| sex |\n",
    "|-----|\n",
    "| female |\n",
    "| male |\n",
    "| female |\n",
    "| male |\n",
    "| female |\n",
    "| ... |\n",
    "\n",
    "4\\. One-hot encoding\n",
    "--------------------\n",
    "\n",
    "01:01 - 01:05\n",
    "\n",
    "One-hot encoding will convert this feature into two features,\n",
    "\n",
    "| sex | one-hot encoding |\n",
    "|-----|-----------------|\n",
    "| female | → |\n",
    "| male | → |\n",
    "| female | → |\n",
    "| male | → |\n",
    "| female | → |\n",
    "| ... | ... |\n",
    "\n",
    "5\\. One-hot encoding\n",
    "--------------------\n",
    "\n",
    "01:05 - 01:17\n",
    "\n",
    "'sex_male' and 'sex_female' such that each male instance has a 'sex_male' value of 1 and 'sex_female' value of 0. For females, it is the vice versa.\n",
    "\n",
    "| sex | one-hot encoding | sex_female | sex_male |\n",
    "| --- | ---------------- | ---------- | -------- |\n",
    "| female | → | 1 | 0 |\n",
    "| male | → | 0 | 1 |\n",
    "| female | → | 1 | 0 |\n",
    "| male | → | 0 | 1 |\n",
    "| female | → | 1 | 0 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "6\\. One-hot encoding with pandas\n",
    "--------------------------------\n",
    "\n",
    "01:17 - 01:54\n",
    "\n",
    "To do this in code, we use pandas' get_dummies() function. Let's import pandas using the alias pd. We can then pass our dataframe df into the pd.get_dummies() function and pass a list of features to be encoded as the columns argument. Not mentioning columns will lead pandas to automatically encode all non-numerical features. Finally, we overwrite the original dataframe with the encoded version by assigning the dataframe returned by get_dummies() back to df.\n",
    "\n",
    "```python\n",
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Perform one-hot encoding on the 'sex' feature of df\n",
    "df = pd.get_dummies(df, columns=['sex'])\n",
    "```\n",
    "\n",
    "7\\. Textual data\n",
    "----------------\n",
    "\n",
    "01:54 - 02:10\n",
    "\n",
    "Consider a movie reviews dataset. This data cannot be utilized by any machine learning or ML algorithm. The training feature 'review' isn't numerical. Neither is it categorical to perform one-hot encoding on.\n",
    "\n",
    "#### Movie Review Dataset\n",
    "\n",
    "| review | class |\n",
    "| --- | --- |\n",
    "| This movie is for dog lovers. A very poignant... | positive |\n",
    "| The movie is forgettable. The plot lacked... | negative |\n",
    "| A truly amazing movie about dogs. A gripping... | positive |\n",
    "\n",
    "8\\. Text pre-processing\n",
    "-----------------------\n",
    "\n",
    "02:10 - 02:34\n",
    "\n",
    "We need to perform two steps to make this dataset suitable for ML. The first is to standardize the text. This involves steps like converting words to lowercase and their base form. For instance, 'Reduction' gets lowercased and then converted to its base form, reduce. We will cover these concepts in more detail in subsequent lessons.\n",
    "\n",
    "- Converting to lowercase\n",
    "    - Example:`Reduction` to `reduction`\n",
    "- Converting to base-form\n",
    "    - Example:`reduction` to `reduce`\n",
    "\n",
    "\n",
    "9\\. Vectorization\n",
    "-----------------\n",
    "\n",
    "02:34 - 02:48\n",
    "\n",
    "After preprocessing, the reviews are converted into a set of numerical training features through a process known as vectorization. After vectorization, our original review dataset gets converted\n",
    "\n",
    "| review | class |\n",
    "| --- | --- |\n",
    "| This movie is for dog lovers. A very poignant... | positive |\n",
    "| The movie is forgettable. The plot lacked... | negative |\n",
    "| A truly amazing movie about dogs. A gripping... | positive |\n",
    "\n",
    "10\\. Vectorization\n",
    "------------------\n",
    "\n",
    "02:48 - 02:55\n",
    "\n",
    "into something like this. We will learn techniques to achieve this in later lessons.\n",
    "\n",
    "| 0 | 1 | 2 | ... | n | class |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 0.03 | 0.71 | 0.00 | ... | 0.22 | positive |\n",
    "| 0.45 | 0.00 | 0.03 | ... | 0.19 | negative |\n",
    "| 0.14 | 0.18 | 0.00 | ... | 0.45 | positive |\n",
    "\n",
    "11\\. Basic features\n",
    "-------------------\n",
    "\n",
    "02:55 - 03:20\n",
    "\n",
    "We can also extract certain basic features from text. It maybe useful to know the word count, character count and average word length of a particular text. While working with niche data such as tweets, it also maybe useful to know how many hashtags have been used in a tweet. This tweet by Silverado Records,for instance, uses two.\n",
    "\n",
    "- Number of words\n",
    "- Number of characters\n",
    "- Average length of words\n",
    "- Tweets\n",
    "\n",
    "```markdown\n",
    "testbook @books\n",
    "What book are ypu guys reading?\n",
    "\n",
    "#books #reading\n",
    "```\n",
    "\n",
    "12\\. POS tagging\n",
    "----------------\n",
    "\n",
    "03:20 - 03:50\n",
    "\n",
    "So far, we have seen how to extract features out of an entire body of text. Some NLP applications may require you to extract features for individual words. For instance, you may want to do parts-of-speech tagging to know the different parts-of-speech present in your text as shown. As an example, consider the sentence 'I have a dog'. POS tagging will label each word with its corresponding part-of-speech.\n",
    "\n",
    "| Word | POS |\n",
    "| --- | --- |\n",
    "| I | Pronoun |\n",
    "| have | Verb |\n",
    "| a | Article |\n",
    "| dog | Noun |\n",
    "\n",
    "13\\. Named Entity Recognition\n",
    "-----------------------------\n",
    "\n",
    "03:50 - 04:16\n",
    "\n",
    "You may also want to know perform named entity recognition to find out if a particular noun is referring to a person, organization or country. For instance, consider the sentence \"Brian works at DataCamp\". Here, there are two nouns \"Brian\" and \"DataCamp\". Brian refers to a person whereas DataCamp refers to an organization.\n",
    "\n",
    "#### Noun Reference\n",
    "\n",
    "The image asks whether nouns refer to persons, organizations, or countries.\n",
    "\n",
    "| Noun | NER |\n",
    "| --- | --- |\n",
    "| Brian | Person |\n",
    "| DataCamp | Organization |\n",
    "\n",
    "The image contains three main visual elements:\n",
    "1. A photo of a person playing a guitar, which is likely a reference to the \"Brian\" noun.\n",
    "2. A Swiss flag, which could represent a country.\n",
    "3. The TED logo, which represents the organization \"TED\".\n",
    "\n",
    "Based on the table in the image, the nouns \"Brian\" and \"DataCamp\" are classified as referring to a person and an organization, respectively. The image is prompting the viewer to consider whether nouns generally refer to persons, organizations, or countries.\n",
    "\n",
    "14\\. Concepts covered\n",
    "---------------------\n",
    "\n",
    "04:16 - 04:33\n",
    "\n",
    "Therefore, broadly speaking, this course will teach you how to conduct text preprocessing, extract certain basic features, word features and convert documents into a set of numerical features (using a process known as vectorization).\n",
    "\n",
    "- Text Preprocessing\n",
    "- Basic Features\n",
    "- Word Features\n",
    "- Vectorization\n",
    "\n",
    "\n",
    "15\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:33 - 04:36\n",
    "\n",
    "Great! Now, let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format for ML algorithms\n",
    "=============================\n",
    "\n",
    "In this exercise, you have been given four dataframes `df1`, `df2`, `df3` and `df4`. The final column of each dataframe is the predictor variable and the rest of the columns are training features. \n",
    "\n",
    "Using the console, determine which dataframe is in a suitable format to be trained by a classifier.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "`df1`\n",
    "\n",
    "`df2`\n",
    "\n",
    "[/] `df3`\n",
    "\n",
    "`df4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding\n",
    "================\n",
    "\n",
    "In the previous exercise, we encountered a dataframe `df1` which contained categorical features and therefore, was unsuitable for applying ML algorithms to.\n",
    "\n",
    "In this exercise, your task is to convert `df1`into a format that is suitable for machine learning.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "-   Use the `columns` attribute to print the features of `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "----------------\n",
    "\n",
    "-   Use the `pd.get_dummies()` function to perform one-hot encoding on `feature 5` of `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "----------------\n",
    "\n",
    "-   Use the `columns` attribute again to print the new features of `df1`.\n",
    "-   Print the first five rows of `df1` using `head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])\n",
    "\n",
    "# Print the new features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Print first five rows of df1\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Basic feature extraction\n",
    "----------------------------\n",
    "\n",
    "00:00 - 00:11\n",
    "\n",
    "In this video, we will learn to extract certain basic features from text. While not very powerful, they can give us a good idea of the text we are dealing with.\n",
    "\n",
    "2\\. Number of characters\n",
    "------------------------\n",
    "\n",
    "00:11 - 00:58\n",
    "\n",
    "The most basic feature we can extract from text is the number of characters, including whitespaces. For instance, the string \"I don't know.\" has 13 characters. The number of characters is the length of the string. Python gives us a built-in len() function which returns the length of the string passed into it. The output will be 13 here too. If our dataframe df has a textual feature (say 'review'), we can compute the number of characters for each review and store it as a new feature 'num_chars' by using the pandas dataframe apply method. This is done by creating df['num_chars'] and assigning it to df['review'].apply(len).\n",
    "\n",
    "```python\n",
    "\"I don't know.\" # 13 characters\n",
    "# Compute the number of characters \n",
    "text = \"I don't know.\"num_char = len(text)\n",
    "# Print the number of charactersprint\n",
    "(num_char)\n",
    "\n",
    "13\n",
    "\n",
    "# Create a 'num_chars' feature\n",
    "df['num_chars'] = df['review'].apply(len)\n",
    "```\n",
    "\n",
    "3\\. Number of words\n",
    "-------------------\n",
    "\n",
    "00:58 - 01:32\n",
    "\n",
    "Another feature we can compute is the number of words. Assuming that every word is separated by a space, we can use a string's split() method to convert it into a list where every element is a word. In this example, the string Mary had a little lamb is split to create a list containing the words Mary, had, a, little and lamb. We can now compute the number of words by computing the number of elements in this list using len().\n",
    "\n",
    "```python\n",
    "# Split the string into words\n",
    "text = \"Mary had a little lamb.\"\n",
    "words = text.split()\n",
    "# Print the list containing words\n",
    "print(words)\n",
    "\n",
    "['Mary', 'had', 'a', 'little', 'lamb.']\n",
    "\n",
    "# Print number of words\n",
    "print(len(words))5\n",
    "\n",
    "Output :\n",
    "5\n",
    "```\n",
    "\n",
    "4\\. Number of words\n",
    "-------------------\n",
    "\n",
    "01:32 - 01:58\n",
    "\n",
    "To do this for a textual feature in a dataframe, we first define a function that takes in a string as an argument and returns the number of words in it. The steps followed inside the function are similar as before. We then pass this function word_count into apply. We create df['num_words'] and assign it to df['review'].apply(word_count).\n",
    "\n",
    "```python\n",
    "# Function that returns number of words in string\n",
    "def word_count(string):\n",
    "    # Split the string into words    \n",
    "    words = string.split()\n",
    "    # Return length of words list\n",
    "    returnlen(words)\n",
    "# Create num_words feature in df\n",
    "df['num_words'] = df['review'].apply(word_count)\n",
    "```\n",
    "\n",
    "5\\. Average word length\n",
    "-----------------------\n",
    "\n",
    "01:58 - 02:24\n",
    "\n",
    "Let's now compute the average length of words in a string. Let's define a function avg_word_length() which takes in a string and returns the average word length. We first split the string into words and compute the length of each word. Next, we compute the average word length by dividing the sum of the lengths of all words by the number of words.\n",
    "\n",
    "```python\n",
    "#Function that returns average word length\n",
    "def avg_word_length(x):\n",
    "    # Split the string into words    \n",
    "    words = x.split()\n",
    "    # Compute length of each word and store in a separate list \n",
    "    word_lengths = [len(word) for word in words]\n",
    "    # Compute average word length    \n",
    "    avg_word_length = sum(word_lengths)/len(words)\n",
    "    # Return average word length\n",
    "    return(avg_word_length)\n",
    "```\n",
    "\n",
    "6\\. Average word length\n",
    "-----------------------\n",
    "\n",
    "02:24 - 02:31\n",
    "\n",
    "We can now pass this into apply() to generate a average word length feature like before.\n",
    "\n",
    "```python\n",
    "# Create a new feature avg_word_length\n",
    "df['avg_word_length'] = df['review'].apply(doc_density)\n",
    "```\n",
    "\n",
    "7\\. Special features\n",
    "--------------------\n",
    "\n",
    "02:31 - 02:52\n",
    "\n",
    "When working with data such as tweets, it maybe useful to compute the number of hashtags or mentions used. This tweet by DataCamp, for instance, has one mention upendra_35 which begins with an @ and two hashtags, PySpark and Spark which begin with a #.\n",
    "\n",
    "```markdown\n",
    "Tweet: \n",
    "Datacamp @Datacamp\n",
    "\n",
    "Big data Fundamentals via PySpark. #BigData #pySpark\n",
    "```\n",
    "\n",
    "8\\. Hashtags and mentions\n",
    "-------------------------\n",
    "\n",
    "02:52 - 03:44\n",
    "\n",
    "Let's write a function that computes the number of hashtags in a string. We split the string into words. We then use list comprehension to create a list containing only those words that are hashtags. We do this using the startswith method of strings to find out if a word begins with #. The final step is to return the number of elements in this list using len. The procedure to compute number of mentions is identical except that we check if a word starts with @. Let's see this function in action. When we pass a string \"@janedoe This is my first tweet! #FirstTweet #Happy\", the function returns 2 which is indeed the number of hashtags in the string.\n",
    "\n",
    "```python\n",
    "# Function that returns number of hashtags\n",
    "def hashtag_count(string):\n",
    "    # Split the string into words    \n",
    "    words = string.split()\n",
    "    # Create a list of hashtags    \n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    # Return number of hashtags\n",
    "    returnlen(hashtags)\n",
    "    \n",
    "hashtag_count(\"@janedoe This is my first tweet! #FirstTweet #Happy\")\n",
    "\n",
    "Output:\n",
    "2\n",
    "```\n",
    "\n",
    "\n",
    "9\\. Other features\n",
    "------------------\n",
    "\n",
    "03:44 - 04:04\n",
    "\n",
    "There are other basic features we can compute such as number of sentences, number of paragraphs, number of words starting with an uppercase, all-capital words, numeric quantities etc. The procedure to do this is extremely similar to the ones we've already covered.\n",
    "\n",
    "- Number of sentences\n",
    "- Number of paragraphs\n",
    "- Words starting with an uppercase\n",
    "- All-capital words\n",
    "- Numeric quantities\n",
    "\n",
    "\n",
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:04 - 04:09\n",
    "\n",
    "That's enough theory for now. Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character count of Russian tweets\n",
    "=================================\n",
    "\n",
    "In this exercise, you have been given a dataframe `tweets` which contains some tweets associated with Russia's Internet Research Agency and compiled by FiveThirtyEight. \n",
    "\n",
    "Your task is to create a new feature 'char_count' in `tweets` which computes the number of characters for each tweet. Also, compute the average length of each tweet. The tweets are available in the `content`feature of `tweets`.\n",
    "\n",
    "*Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data).*\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create a new feature `char_count` by applying `len` to the 'content' feature of `tweets`.\n",
    "-   Print the average character count of the tweets by computing the mean of the 'char_count' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word count of TED talks\n",
    "=======================\n",
    "\n",
    "`ted` is a dataframe that contains the transcripts of 500 TED talks. Your job is to compute a new feature `word_count` which contains the approximate number of words for each talk. Consequently, you also need to compute the average word count of the talks. The transcripts are available as the `transcript` feature in `ted`.\n",
    "\n",
    "In order to complete this task, you will need to define a function `count_words` that takes in a string as an argument and returns the number of words in the string. You will then need to apply this function to the `transcript` feature of `ted` to create the new feature `word_count`and compute its mean.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Split `string` into a list of words using the `split()` method.\n",
    "-   Return the number of elements in `words`using `len()`.\n",
    "-   Apply your function to the `transcript`column of `ted` to create the new feature `word_count`.\n",
    "-   Compute the average word count of the talks using `mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtags and mentions in Russian tweets\n",
    "=======================================\n",
    "\n",
    "Let's revisit the `tweets` dataframe containing the Russian tweets. In this exercise, you will compute the number of hashtags and mentions in each tweet by defining two functions `count_hashtags()` and `count_mentions()`respectively and applying them to the `content` feature of `tweets`. \n",
    "\n",
    "In case you don't recall, the tweets are contained in the `content` feature of `tweets`.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   In the list comprehension, use `startswith()` to check if a particular `word` starts with `'#'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   In the list comprehension, use `startswith()` to check if a particular `word` starts with '@'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Readability tests\n",
    "---------------------\n",
    "\n",
    "00:00 - 00:06\n",
    "\n",
    "In this lesson, we will look at a set of interesting features known as readability tests.\n",
    "\n",
    "2\\. Overview of readability tests\n",
    "---------------------------------\n",
    "\n",
    "00:06 - 00:56\n",
    "\n",
    "These tests are used to determine the readability of a particular passage. In other words, it indicates at what educational level a person needs to be in, in order to comprehend a particular piece of text. The scale usually ranges from primary school up to college graduate level and is in context of the American education system. Readability tests are done using a mathematical formula that utilizes the word, syllable and sentence count of the passage. They are routinely used by organizations to determine how easy their publications are to understand. They have also found applications in domains such as fake news and opinion spam detection.\n",
    "\n",
    "- Determine readability of an English passage\n",
    "- Scale ranging from primary school up to college graduate level\n",
    "- A mathematical formula utilizing word, syllable and sentence count\n",
    "- Used in fake news and opinion spam detection\n",
    "\n",
    "\n",
    "3\\. Readability text examples\n",
    "-----------------------------\n",
    "\n",
    "00:56 - 01:25\n",
    "\n",
    "There are a variety of readability tests in use. Some of the common ones include the Flesch reading ease, the Gunning fog index, the simple measure of gobbledygook or SMOG and the Dale-Chall score. Note that these tests are used for texts in English. Tests for other languages also exist that take into consideration, the nuances of that particular language. For the sake of brevity, we will cover only the\n",
    "\n",
    "- Flesch reading ease\n",
    "- Gunning fog index\n",
    "- Simple Measure of Gobbledygook (SMOG)\n",
    "- Dale-Chall score\n",
    "\n",
    "\n",
    "4\\. Readability test examples\n",
    "-----------------------------\n",
    "\n",
    "01:25 - 01:36\n",
    "\n",
    "first two scores in detail. However, once you understand them, you will be in a good position to understand and use the other scores too.\n",
    "\n",
    "- Flesch reading ease\n",
    "- Gunning fog index\n",
    "- Simple Measure of Gobbledygook (SMOG)\n",
    "- Dale-Chall score\n",
    "\n",
    "\n",
    "5\\. Flesch reading ease\n",
    "-----------------------\n",
    "\n",
    "01:36 - 02:30\n",
    "\n",
    "The Flesch Reading Ease is one of the oldest and most widely used readability tests. The score is based on two ideas: the first is that the greater the average sentence length, harder the text is to read. Consider these two sentences. The first is easier to follow than the second. The second is that the greater the average number of syllables in a word, the harder the text is to read. Therefore, I live in my home is considered easier to read than I reside in my domicile on account of its usage of lesser syllables per word. The higher the Flesch Reading Ease score, the greater is the readability. Therefore, a higher score indicates that the text is easier to understand.\n",
    "\n",
    "- One of the oldest and most widely used tests\n",
    "- **Greater the average sentence length, harder the text is to read**\n",
    "    - \"This is a short sentence.\"\n",
    "    - \"This is longer sentence with more words and it is harder to follow than the first sentence.\"\n",
    "- **Greater the average number of syllables in a word, harder the text is to read**\n",
    "    - \"I live in my home.\"\n",
    "    - \"I reside in my domicile.\"\n",
    "- Higher the score, greater the readability\n",
    "\n",
    "\n",
    "6\\. Flesch reading ease score interpretation\n",
    "--------------------------------------------\n",
    "\n",
    "02:30 - 02:49\n",
    "\n",
    "This table shows how to interpret the Flesch Reading Ease scores. A score above 90 would imply that the text is comprehensible to a 5th grader whereas a score below 30 would imply the text can only be understood by college graduates.\n",
    "\n",
    "| Reading ease score | Grade Level |\n",
    "|-------------------|-------------|\n",
    "| 90-100           | 5           |\n",
    "| 80-90            | 6           |\n",
    "| 70-80            | 7           |\n",
    "| 60-70            | 8-9         |\n",
    "| 50-60            | 10-12       |\n",
    "| 30-50            | College     |\n",
    "| 0-30             | College Graduate |\n",
    "\n",
    "7\\. Gunning fog index\n",
    "---------------------\n",
    "\n",
    "02:49 - 03:23\n",
    "\n",
    "The Gunning fog index was developed in 1954. Like Flesch, this score is also dependent on the average sentence length. However, it uses percentage of complex words in place of average syllables per word to compute its score. Here, complex words refer to all words that have three or more syllables. Unlike Flesch, the formula for Gunning fog index is such that the higher the score, the more difficult the passage is to understand.\n",
    "\n",
    "- Developed in 1954\n",
    "- Also dependent on average sentence length\n",
    "- Greater the percentage of complex words, harder the text is to read\n",
    "- Higher the index, lesser the readability\n",
    "\n",
    "8\\. Gunning fog index interpretation\n",
    "------------------------------------\n",
    "\n",
    "03:23 - 03:39\n",
    "\n",
    "The index can be interpreted using this table. A score of 6 would indicate 6th grade reading difficulty whereas a score of 17 would indicate college graduate level reading difficulty.\n",
    "\n",
    "| Fog index | Grade level |\n",
    "|-----------|-------------|\n",
    "| 17 | College graduate |\n",
    "| 16 | College senior |\n",
    "| 15 | College junior |\n",
    "| 14 | College sophomore |\n",
    "| 13 | College freshman |\n",
    "| 12 | High school senior |\n",
    "| 11 | High school junior |\n",
    "\n",
    "| Fog index | Grade level |\n",
    "|-----------|-------------|\n",
    "| 10 | High school sophomore |\n",
    "| 9 | High school freshman |\n",
    "| 8 | Eighth grade |\n",
    "| 7 | Seventh grade |\n",
    "| 6 | Sixth grade |\n",
    "\n",
    "9\\. The readability library\n",
    "---------------------------\n",
    "\n",
    "03:39 - 04:28\n",
    "\n",
    "We can conduct these tests in Python using the readability metrics library. In order to use this package, we first need to download the punkt module from nltk. We then import the Readability class from readability. Next, we create a Readability object and pass in the passage or text we're evaluating. To compute a readability score, we call a method that computes the score of our interest, for instance, gunning fog. We store this variable in a variable named gf. Next, we access the score using gf.score. In this example, the text that was passed is between the reading level of a college senior and that of a college graduate.\n",
    "\n",
    "```python\n",
    "# Download nltk punkt module\n",
    "import nltknltk.download('punkt_tab')\n",
    "# Import the Readability class\n",
    "from readability import Readability\n",
    "# Create a Readability Object\n",
    "readability_scores = Readability(text)\n",
    "# Generate scoresgf = readability_scores.gunning_fog()\n",
    "print(gf.score())\n",
    "\n",
    "Output: \n",
    "16.26\n",
    "```\n",
    "\n",
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:28 - 04:38\n",
    "\n",
    "Let's now practice computing readability scores using the readability library in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability of 'The Myth of Sisyphus'\n",
    "=====================================\n",
    "\n",
    "In this exercise, you will compute the Flesch reading ease score for Albert Camus' famous essay *The Myth of Sisyphus*. We will then interpret the value of this score as explained in the video and try to determine the reading level of the essay.\n",
    "\n",
    "The entire essay is in the form of a string and is available as `sisyphus_essay`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the `Readability` class from `readability`.\n",
    "-   Compute the `readability_scores` object for `sisyphus_essay` using `Readability`.\n",
    "-   Print the Flesch reading ease score using the `flesch` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# Compute the readability scores \n",
    "readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability of various publications\n",
    "===================================\n",
    "\n",
    "In this exercise, you have been given excerpts of articles from four publications. Your task is to compute the readability of these excerpts using the Gunning fog score and consequently, determine the relative difficulty of reading these publications.\n",
    "\n",
    "The excerpts are available as the following strings:\n",
    "\n",
    "-   `forbes`- An excerpt from an article from *Forbes* magazine on the Chinese social credit score system.\n",
    "-   `harvard_law`- An excerpt from a book review published in *Harvard Law Review*.\n",
    "-   `r_digest`- An excerpt from a *Reader's Digest* article on flight turbulence.\n",
    "-   `time_kids` - An excerpt from an article on the ill effects of salt consumption published in *TIME for Kids*.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the `Readability` class from `readability`.\n",
    "-   Compute the `gf` object for each `excerpt`using the `gunning_fog()` method on `Readability`.\n",
    "-   Compute the Gunning fog score using the the `score` attribute.\n",
    "-   Print the list of Gunning fog scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing, POS tagging and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Tokenization and Lemmatization\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "00:00 - 00:06\n",
    "\n",
    "In NLP, we usually have to deal with texts from a variety of sources. For instance,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Text sources\n",
    "\n",
    "----------------\n",
    "\n",
    "00:06 - 00:22\n",
    "\n",
    "it can be a news article where the text is grammatically correct and proofread. It could be tweets containing shorthands and hashtags. It could also be comments on YouTube where people have a tendency to abuse capital letters and punctuations.\n",
    "\n",
    "- News articles\n",
    "- Tweets\n",
    "- Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Making text machine friendly\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "00:22 - 01:03\n",
    "\n",
    "It is important that we standardize these texts into a machine friendly format. We want our models to treat similar words as the same. Consider the words Dogs and dog. Strictly speaking, they are different strings. However, they connotate the same thing. Similarly, reduction, reducing and reduce should also be standardized to the same string regardless of their form and case usage. Other examples include don't and do not, and won't and will not. In the next couple of lessons, we will learn techniques to achieve this.\n",
    "\n",
    "- `Dogs,dog`\n",
    "- `reduction`, `REDUCING`, `Reduce`\n",
    "- `don't`, `do not`\n",
    "- `won't`, `will not`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Text preprocessing techniques\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "01:03 - 01:31\n",
    "\n",
    "The text processing techniques you use are dependent on the application you're working on. We'll be covering the common ones, including converting words into lowercase removing unnecessary whitespace, removing punctuation, removing commonly occurring words or stopwords, expanding contracted words like don't and removing special characters such as numbers and emojis.\n",
    "\n",
    "- Converting words into lowercase\n",
    "- Removing leading and trailing whitespaces\n",
    "- Removing punctuation\n",
    "- Removing stopwordsExpanding contractions\n",
    "- Removing special characters (numbers, emojis, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Tokenization\n",
    "\n",
    "----------------\n",
    "\n",
    "01:31 - 02:21\n",
    "\n",
    "To do this, we must first understand tokenization. Tokenization is the process of splitting a string into its constituent tokens. These tokens may be sentences, words or punctuations and is specific to a particular language. In this course, we will primarily be focused with word and punctuation tokens. For instance, consider this sentence. Tokenizing it into its constituent words and punctuations will yield the following list of tokens. Tokenization also involves expanding contracted words. Therefore, a word like don't gets decomposed into two tokens: do and n't as can be seen in this example.\n",
    "\n",
    "\"I have a dog. His name is Hachi.\"\n",
    "Tokens:\n",
    "```python\n",
    "[\"I\", \"have\", \"a\", \"dog\", \".\", \"His\", \"name\", \"is\", \"Hachi\", \".\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Tokenization using spaCy\n",
    "\n",
    "----------------------------\n",
    "\n",
    "02:21 - 03:21\n",
    "\n",
    "To perform tokenization in python, we will use the spacy library. We first import the spacy library. Next, we load a pre-trained English model 'en_core_web_sm' using spacy.load(). This will return a Language object that has the know-how to perform tokenization. This is stored in the variable nlp. Let's now define a string we want to tokenize. We pass this string into nlp to generate a spaCy Doc object. We store this in a variable named doc. This Doc object contains the required tokens (and many other things, as we will soon find out). We generate the list of tokens by using list comprehension as shown. This is essentially looping over doc and extracting the text of each token in each iteration. The result is as follows.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "# Generate list of tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "['Hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', \"'\", 'm', 'doing', 'here', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Lemmatization\n",
    "\n",
    "-----------------\n",
    "\n",
    "03:21 - 04:07\n",
    "\n",
    "Lemmatization is the process of converting a word into its lowercased base form or lemma. This is an extremely powerful process of standardization. For instance, the words reducing, reduces, reduced and reduction, when lemmatized, are all converted into the base form reduce. Similarly be verbs such as am, are and is are converted into be. Lemmatization also allows us to convert words with apostrophes into their full forms. Therefore, n't is converted to not and 've is converted to have.\n",
    "\n",
    "- Convert word into its base form\n",
    "  - reducing, reduces, reduced, reduction → reduce\n",
    "  - am, are, is → be\n",
    "  - n't → not\n",
    "  - 've → have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Lemmatization using spaCy\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "04:07 - 04:42\n",
    "\n",
    "When you pass the string into nlp, spaCy automatically performs lemmatization by default. Therefore, generating lemmas is identical to generating tokens except that we extract token.lemma_ in each iteration inside the list comprehension instead of token.text. Also, observe how spaCy converted the Is into -PRON-. This is standard behavior where every pronoun is converted into the string '-PRON-'.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "# Generate list of lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "['hello', '!', '-PRON-', 'do', 'not', 'know', 'what', '-PRON-', 'be', 'do', 'here', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Let's practice!\n",
    "\n",
    "-------------------\n",
    "\n",
    "04:42 - 05:00\n",
    "\n",
    "Once we understand how to perform tokenization and lemmatization, performing the text preprocessing techniques described earlier becomes easier. Before we move to that, let's first practice our understanding of the concepts introduced so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying lemmas\n",
    "==================\n",
    "\n",
    "Identify the list of words from the choices which do not have the same lemma.\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "- [x] He, She, I, They\n",
    "\n",
    "-   Am, Are, Is, Was\n",
    "\n",
    "-   Increase, Increases, Increasing, Increased\n",
    "\n",
    "-   Car, Bike, Truck, Bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the Gettysburg Address\n",
    "=================================\n",
    "\n",
    "In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War.\n",
    "\n",
    "The entire speech is available as a string named `gettysburg`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Load the `en_core_web_sm` model using `spacy.load()`.\n",
    "-   Create a Doc object `doc` for the `gettysburg` string.\n",
    "-   Using list comprehension, loop over `doc` to generate the token texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing the Gettysburg address\n",
    "==================================\n",
    "\n",
    "In this exercise, we will perform lemmatization on the same `gettysburg` address from before. \n",
    "\n",
    "However, this time, we will also take a look at the speech, before and after lemmatization, and try to adjudge the kind of changes that take place to make the piece more machine friendly.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "Print the gettysburg address to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the gettysburg address\n",
    "print(gettysburg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "\n",
    "\n",
    "Loop over doc and extract the lemma for each token of gettysburg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over doc and extract the lemma for each token of gettysburg.\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "\n",
    "Convert lemmas into a string using join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
