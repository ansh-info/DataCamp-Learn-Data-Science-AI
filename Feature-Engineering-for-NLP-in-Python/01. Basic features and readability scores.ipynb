{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Introduction to NLP feature engineering\n",
    "-------------------------------------------\n",
    "\n",
    "00:00 - 00:18\n",
    "\n",
    "Welcome to Feature Engineering for NLP in Python! I am Rounak and I will be your instructor for this course. In this course, you will learn to extract useful features out of text and convert them into formats that are suitable for machine learning algorithms.\n",
    "\n",
    "2\\. Numerical data\n",
    "------------------\n",
    "\n",
    "00:18 - 00:44\n",
    "\n",
    "For any ML algorithm, data fed into it must be in tabular form and all the training features must be numerical. Consider the Iris dataset. Every training instance has exactly four numerical features. The ML algorithm uses these four features to train and predict if an instance belongs to class iris-virginica, iris-setosa or iris-versicolor.\n",
    "\n",
    "#### Iris dataset\n",
    "\n",
    "| sepal length | sepal width | petal length | petal width | class |\n",
    "|--------------|-------------|--------------|-------------|-------|\n",
    "| 6.3 | 2.9 | 5.6 | 1.8 | Iris-virginica |\n",
    "| 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa |\n",
    "| 5.6 | 2.9 | 3.6 | 1.3 | Iris-versicolor |\n",
    "| 6.0 | 2.7 | 5.1 | 1.6 | Iris-versicolor |\n",
    "| 7.2 | 3.6 | 6.1 | 2.5 | Iris-virginica |\n",
    "\n",
    "3\\. One-hot encoding\n",
    "--------------------\n",
    "\n",
    "00:44 - 01:01\n",
    "\n",
    "ML algorithms can also work with categorical data provided they are converted into numerical form through one-hot encoding. Let's say you have a categorical feature 'sex' with two categories 'male' and 'female'.\n",
    "\n",
    "| sex |\n",
    "|-----|\n",
    "| female |\n",
    "| male |\n",
    "| female |\n",
    "| male |\n",
    "| female |\n",
    "| ... |\n",
    "\n",
    "4\\. One-hot encoding\n",
    "--------------------\n",
    "\n",
    "01:01 - 01:05\n",
    "\n",
    "One-hot encoding will convert this feature into two features,\n",
    "\n",
    "| sex | one-hot encoding |\n",
    "|-----|-----------------|\n",
    "| female | → |\n",
    "| male | → |\n",
    "| female | → |\n",
    "| male | → |\n",
    "| female | → |\n",
    "| ... | ... |\n",
    "\n",
    "5\\. One-hot encoding\n",
    "--------------------\n",
    "\n",
    "01:05 - 01:17\n",
    "\n",
    "'sex_male' and 'sex_female' such that each male instance has a 'sex_male' value of 1 and 'sex_female' value of 0. For females, it is the vice versa.\n",
    "\n",
    "| sex | one-hot encoding | sex_female | sex_male |\n",
    "| --- | ---------------- | ---------- | -------- |\n",
    "| female | → | 1 | 0 |\n",
    "| male | → | 0 | 1 |\n",
    "| female | → | 1 | 0 |\n",
    "| male | → | 0 | 1 |\n",
    "| female | → | 1 | 0 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "6\\. One-hot encoding with pandas\n",
    "--------------------------------\n",
    "\n",
    "01:17 - 01:54\n",
    "\n",
    "To do this in code, we use pandas' get_dummies() function. Let's import pandas using the alias pd. We can then pass our dataframe df into the pd.get_dummies() function and pass a list of features to be encoded as the columns argument. Not mentioning columns will lead pandas to automatically encode all non-numerical features. Finally, we overwrite the original dataframe with the encoded version by assigning the dataframe returned by get_dummies() back to df.\n",
    "\n",
    "```python\n",
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Perform one-hot encoding on the 'sex' feature of df\n",
    "df = pd.get_dummies(df, columns=['sex'])\n",
    "```\n",
    "\n",
    "7\\. Textual data\n",
    "----------------\n",
    "\n",
    "01:54 - 02:10\n",
    "\n",
    "Consider a movie reviews dataset. This data cannot be utilized by any machine learning or ML algorithm. The training feature 'review' isn't numerical. Neither is it categorical to perform one-hot encoding on.\n",
    "\n",
    "#### Movie Review Dataset\n",
    "\n",
    "| review | class |\n",
    "| --- | --- |\n",
    "| This movie is for dog lovers. A very poignant... | positive |\n",
    "| The movie is forgettable. The plot lacked... | negative |\n",
    "| A truly amazing movie about dogs. A gripping... | positive |\n",
    "\n",
    "8\\. Text pre-processing\n",
    "-----------------------\n",
    "\n",
    "02:10 - 02:34\n",
    "\n",
    "We need to perform two steps to make this dataset suitable for ML. The first is to standardize the text. This involves steps like converting words to lowercase and their base form. For instance, 'Reduction' gets lowercased and then converted to its base form, reduce. We will cover these concepts in more detail in subsequent lessons.\n",
    "\n",
    "- Converting to lowercase\n",
    "    - Example:`Reduction` to `reduction`\n",
    "- Converting to base-form\n",
    "    - Example:`reduction` to `reduce`\n",
    "\n",
    "\n",
    "9\\. Vectorization\n",
    "-----------------\n",
    "\n",
    "02:34 - 02:48\n",
    "\n",
    "After preprocessing, the reviews are converted into a set of numerical training features through a process known as vectorization. After vectorization, our original review dataset gets converted\n",
    "\n",
    "| review | class |\n",
    "| --- | --- |\n",
    "| This movie is for dog lovers. A very poignant... | positive |\n",
    "| The movie is forgettable. The plot lacked... | negative |\n",
    "| A truly amazing movie about dogs. A gripping... | positive |\n",
    "\n",
    "10\\. Vectorization\n",
    "------------------\n",
    "\n",
    "02:48 - 02:55\n",
    "\n",
    "into something like this. We will learn techniques to achieve this in later lessons.\n",
    "\n",
    "| 0 | 1 | 2 | ... | n | class |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 0.03 | 0.71 | 0.00 | ... | 0.22 | positive |\n",
    "| 0.45 | 0.00 | 0.03 | ... | 0.19 | negative |\n",
    "| 0.14 | 0.18 | 0.00 | ... | 0.45 | positive |\n",
    "\n",
    "11\\. Basic features\n",
    "-------------------\n",
    "\n",
    "02:55 - 03:20\n",
    "\n",
    "We can also extract certain basic features from text. It maybe useful to know the word count, character count and average word length of a particular text. While working with niche data such as tweets, it also maybe useful to know how many hashtags have been used in a tweet. This tweet by Silverado Records,for instance, uses two.\n",
    "\n",
    "- Number of words\n",
    "- Number of characters\n",
    "- Average length of words\n",
    "- Tweets\n",
    "\n",
    "```markdown\n",
    "testbook @books\n",
    "What book are ypu guys reading?\n",
    "\n",
    "#books #reading\n",
    "```\n",
    "\n",
    "12\\. POS tagging\n",
    "----------------\n",
    "\n",
    "03:20 - 03:50\n",
    "\n",
    "So far, we have seen how to extract features out of an entire body of text. Some NLP applications may require you to extract features for individual words. For instance, you may want to do parts-of-speech tagging to know the different parts-of-speech present in your text as shown. As an example, consider the sentence 'I have a dog'. POS tagging will label each word with its corresponding part-of-speech.\n",
    "\n",
    "| Word | POS |\n",
    "| --- | --- |\n",
    "| I | Pronoun |\n",
    "| have | Verb |\n",
    "| a | Article |\n",
    "| dog | Noun |\n",
    "\n",
    "13\\. Named Entity Recognition\n",
    "-----------------------------\n",
    "\n",
    "03:50 - 04:16\n",
    "\n",
    "You may also want to know perform named entity recognition to find out if a particular noun is referring to a person, organization or country. For instance, consider the sentence \"Brian works at DataCamp\". Here, there are two nouns \"Brian\" and \"DataCamp\". Brian refers to a person whereas DataCamp refers to an organization.\n",
    "\n",
    "#### Noun Reference\n",
    "\n",
    "The image asks whether nouns refer to persons, organizations, or countries.\n",
    "\n",
    "| Noun | NER |\n",
    "| --- | --- |\n",
    "| Brian | Person |\n",
    "| DataCamp | Organization |\n",
    "\n",
    "The image contains three main visual elements:\n",
    "1. A photo of a person playing a guitar, which is likely a reference to the \"Brian\" noun.\n",
    "2. A Swiss flag, which could represent a country.\n",
    "3. The TED logo, which represents the organization \"TED\".\n",
    "\n",
    "Based on the table in the image, the nouns \"Brian\" and \"DataCamp\" are classified as referring to a person and an organization, respectively. The image is prompting the viewer to consider whether nouns generally refer to persons, organizations, or countries.\n",
    "\n",
    "14\\. Concepts covered\n",
    "---------------------\n",
    "\n",
    "04:16 - 04:33\n",
    "\n",
    "Therefore, broadly speaking, this course will teach you how to conduct text preprocessing, extract certain basic features, word features and convert documents into a set of numerical features (using a process known as vectorization).\n",
    "\n",
    "- Text Preprocessing\n",
    "- Basic Features\n",
    "- Word Features\n",
    "- Vectorization\n",
    "\n",
    "\n",
    "15\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:33 - 04:36\n",
    "\n",
    "Great! Now, let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format for ML algorithms\n",
    "=============================\n",
    "\n",
    "In this exercise, you have been given four dataframes `df1`, `df2`, `df3` and `df4`. The final column of each dataframe is the predictor variable and the rest of the columns are training features. \n",
    "\n",
    "Using the console, determine which dataframe is in a suitable format to be trained by a classifier.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "`df1`\n",
    "\n",
    "`df2`\n",
    "\n",
    "[/] `df3`\n",
    "\n",
    "`df4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding\n",
    "================\n",
    "\n",
    "In the previous exercise, we encountered a dataframe `df1` which contained categorical features and therefore, was unsuitable for applying ML algorithms to.\n",
    "\n",
    "In this exercise, your task is to convert `df1`into a format that is suitable for machine learning.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "-   Use the `columns` attribute to print the features of `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "----------------\n",
    "\n",
    "-   Use the `pd.get_dummies()` function to perform one-hot encoding on `feature 5` of `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "----------------\n",
    "\n",
    "-   Use the `columns` attribute again to print the new features of `df1`.\n",
    "-   Print the first five rows of `df1` using `head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])\n",
    "\n",
    "# Print the new features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Print first five rows of df1\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Basic feature extraction\n",
    "----------------------------\n",
    "\n",
    "00:00 - 00:11\n",
    "\n",
    "In this video, we will learn to extract certain basic features from text. While not very powerful, they can give us a good idea of the text we are dealing with.\n",
    "\n",
    "2\\. Number of characters\n",
    "------------------------\n",
    "\n",
    "00:11 - 00:58\n",
    "\n",
    "The most basic feature we can extract from text is the number of characters, including whitespaces. For instance, the string \"I don't know.\" has 13 characters. The number of characters is the length of the string. Python gives us a built-in len() function which returns the length of the string passed into it. The output will be 13 here too. If our dataframe df has a textual feature (say 'review'), we can compute the number of characters for each review and store it as a new feature 'num_chars' by using the pandas dataframe apply method. This is done by creating df['num_chars'] and assigning it to df['review'].apply(len).\n",
    "\n",
    "```python\n",
    "\"I don't know.\" # 13 characters\n",
    "# Compute the number of characters \n",
    "text = \"I don't know.\"num_char = len(text)\n",
    "# Print the number of charactersprint\n",
    "(num_char)\n",
    "\n",
    "13\n",
    "\n",
    "# Create a 'num_chars' feature\n",
    "df['num_chars'] = df['review'].apply(len)\n",
    "```\n",
    "\n",
    "3\\. Number of words\n",
    "-------------------\n",
    "\n",
    "00:58 - 01:32\n",
    "\n",
    "Another feature we can compute is the number of words. Assuming that every word is separated by a space, we can use a string's split() method to convert it into a list where every element is a word. In this example, the string Mary had a little lamb is split to create a list containing the words Mary, had, a, little and lamb. We can now compute the number of words by computing the number of elements in this list using len().\n",
    "\n",
    "```python\n",
    "# Split the string into words\n",
    "text = \"Mary had a little lamb.\"\n",
    "words = text.split()\n",
    "# Print the list containing words\n",
    "print(words)\n",
    "\n",
    "['Mary', 'had', 'a', 'little', 'lamb.']\n",
    "\n",
    "# Print number of words\n",
    "print(len(words))5\n",
    "\n",
    "Output :\n",
    "5\n",
    "```\n",
    "\n",
    "4\\. Number of words\n",
    "-------------------\n",
    "\n",
    "01:32 - 01:58\n",
    "\n",
    "To do this for a textual feature in a dataframe, we first define a function that takes in a string as an argument and returns the number of words in it. The steps followed inside the function are similar as before. We then pass this function word_count into apply. We create df['num_words'] and assign it to df['review'].apply(word_count).\n",
    "\n",
    "```python\n",
    "# Function that returns number of words in string\n",
    "def word_count(string):\n",
    "    # Split the string into words    \n",
    "    words = string.split()\n",
    "    # Return length of words list\n",
    "    returnlen(words)\n",
    "# Create num_words feature in df\n",
    "df['num_words'] = df['review'].apply(word_count)\n",
    "```\n",
    "\n",
    "5\\. Average word length\n",
    "-----------------------\n",
    "\n",
    "01:58 - 02:24\n",
    "\n",
    "Let's now compute the average length of words in a string. Let's define a function avg_word_length() which takes in a string and returns the average word length. We first split the string into words and compute the length of each word. Next, we compute the average word length by dividing the sum of the lengths of all words by the number of words.\n",
    "\n",
    "```python\n",
    "#Function that returns average word length\n",
    "def avg_word_length(x):\n",
    "    # Split the string into words    \n",
    "    words = x.split()\n",
    "    # Compute length of each word and store in a separate list \n",
    "    word_lengths = [len(word) for word in words]\n",
    "    # Compute average word length    \n",
    "    avg_word_length = sum(word_lengths)/len(words)\n",
    "    # Return average word length\n",
    "    return(avg_word_length)\n",
    "```\n",
    "\n",
    "6\\. Average word length\n",
    "-----------------------\n",
    "\n",
    "02:24 - 02:31\n",
    "\n",
    "We can now pass this into apply() to generate a average word length feature like before.\n",
    "\n",
    "```python\n",
    "# Create a new feature avg_word_length\n",
    "df['avg_word_length'] = df['review'].apply(doc_density)\n",
    "```\n",
    "\n",
    "7\\. Special features\n",
    "--------------------\n",
    "\n",
    "02:31 - 02:52\n",
    "\n",
    "When working with data such as tweets, it maybe useful to compute the number of hashtags or mentions used. This tweet by DataCamp, for instance, has one mention upendra_35 which begins with an @ and two hashtags, PySpark and Spark which begin with a #.\n",
    "\n",
    "```markdown\n",
    "Tweet: \n",
    "Datacamp @Datacamp\n",
    "\n",
    "Big data Fundamentals via PySpark. #BigData #pySpark\n",
    "```\n",
    "\n",
    "8\\. Hashtags and mentions\n",
    "-------------------------\n",
    "\n",
    "02:52 - 03:44\n",
    "\n",
    "Let's write a function that computes the number of hashtags in a string. We split the string into words. We then use list comprehension to create a list containing only those words that are hashtags. We do this using the startswith method of strings to find out if a word begins with #. The final step is to return the number of elements in this list using len. The procedure to compute number of mentions is identical except that we check if a word starts with @. Let's see this function in action. When we pass a string \"@janedoe This is my first tweet! #FirstTweet #Happy\", the function returns 2 which is indeed the number of hashtags in the string.\n",
    "\n",
    "```python\n",
    "# Function that returns number of hashtags\n",
    "def hashtag_count(string):\n",
    "    # Split the string into words    \n",
    "    words = string.split()\n",
    "    # Create a list of hashtags    \n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    # Return number of hashtags\n",
    "    returnlen(hashtags)\n",
    "    \n",
    "hashtag_count(\"@janedoe This is my first tweet! #FirstTweet #Happy\")\n",
    "\n",
    "Output:\n",
    "2\n",
    "```\n",
    "\n",
    "\n",
    "9\\. Other features\n",
    "------------------\n",
    "\n",
    "03:44 - 04:04\n",
    "\n",
    "There are other basic features we can compute such as number of sentences, number of paragraphs, number of words starting with an uppercase, all-capital words, numeric quantities etc. The procedure to do this is extremely similar to the ones we've already covered.\n",
    "\n",
    "- Number of sentences\n",
    "- Number of paragraphs\n",
    "- Words starting with an uppercase\n",
    "- All-capital words\n",
    "- Numeric quantities\n",
    "\n",
    "\n",
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:04 - 04:09\n",
    "\n",
    "That's enough theory for now. Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character count of Russian tweets\n",
    "=================================\n",
    "\n",
    "In this exercise, you have been given a dataframe `tweets` which contains some tweets associated with Russia's Internet Research Agency and compiled by FiveThirtyEight. \n",
    "\n",
    "Your task is to create a new feature 'char_count' in `tweets` which computes the number of characters for each tweet. Also, compute the average length of each tweet. The tweets are available in the `content`feature of `tweets`.\n",
    "\n",
    "*Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data).*\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create a new feature `char_count` by applying `len` to the 'content' feature of `tweets`.\n",
    "-   Print the average character count of the tweets by computing the mean of the 'char_count' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word count of TED talks\n",
    "=======================\n",
    "\n",
    "`ted` is a dataframe that contains the transcripts of 500 TED talks. Your job is to compute a new feature `word_count` which contains the approximate number of words for each talk. Consequently, you also need to compute the average word count of the talks. The transcripts are available as the `transcript` feature in `ted`.\n",
    "\n",
    "In order to complete this task, you will need to define a function `count_words` that takes in a string as an argument and returns the number of words in the string. You will then need to apply this function to the `transcript` feature of `ted` to create the new feature `word_count`and compute its mean.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Split `string` into a list of words using the `split()` method.\n",
    "-   Return the number of elements in `words`using `len()`.\n",
    "-   Apply your function to the `transcript`column of `ted` to create the new feature `word_count`.\n",
    "-   Compute the average word count of the talks using `mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtags and mentions in Russian tweets\n",
    "=======================================\n",
    "\n",
    "Let's revisit the `tweets` dataframe containing the Russian tweets. In this exercise, you will compute the number of hashtags and mentions in each tweet by defining two functions `count_hashtags()` and `count_mentions()`respectively and applying them to the `content` feature of `tweets`. \n",
    "\n",
    "In case you don't recall, the tweets are contained in the `content` feature of `tweets`.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   In the list comprehension, use `startswith()` to check if a particular `word` starts with `'#'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   In the list comprehension, use `startswith()` to check if a particular `word` starts with '@'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Readability tests\n",
    "---------------------\n",
    "\n",
    "00:00 - 00:06\n",
    "\n",
    "In this lesson, we will look at a set of interesting features known as readability tests.\n",
    "\n",
    "2\\. Overview of readability tests\n",
    "---------------------------------\n",
    "\n",
    "00:06 - 00:56\n",
    "\n",
    "These tests are used to determine the readability of a particular passage. In other words, it indicates at what educational level a person needs to be in, in order to comprehend a particular piece of text. The scale usually ranges from primary school up to college graduate level and is in context of the American education system. Readability tests are done using a mathematical formula that utilizes the word, syllable and sentence count of the passage. They are routinely used by organizations to determine how easy their publications are to understand. They have also found applications in domains such as fake news and opinion spam detection.\n",
    "\n",
    "- Determine readability of an English passage\n",
    "- Scale ranging from primary school up to college graduate level\n",
    "- A mathematical formula utilizing word, syllable and sentence count\n",
    "- Used in fake news and opinion spam detection\n",
    "\n",
    "\n",
    "3\\. Readability text examples\n",
    "-----------------------------\n",
    "\n",
    "00:56 - 01:25\n",
    "\n",
    "There are a variety of readability tests in use. Some of the common ones include the Flesch reading ease, the Gunning fog index, the simple measure of gobbledygook or SMOG and the Dale-Chall score. Note that these tests are used for texts in English. Tests for other languages also exist that take into consideration, the nuances of that particular language. For the sake of brevity, we will cover only the\n",
    "\n",
    "- Flesch reading ease\n",
    "- Gunning fog index\n",
    "- Simple Measure of Gobbledygook (SMOG)\n",
    "- Dale-Chall score\n",
    "\n",
    "\n",
    "4\\. Readability test examples\n",
    "-----------------------------\n",
    "\n",
    "01:25 - 01:36\n",
    "\n",
    "first two scores in detail. However, once you understand them, you will be in a good position to understand and use the other scores too.\n",
    "\n",
    "- Flesch reading ease\n",
    "- Gunning fog index\n",
    "- Simple Measure of Gobbledygook (SMOG)\n",
    "- Dale-Chall score\n",
    "\n",
    "\n",
    "5\\. Flesch reading ease\n",
    "-----------------------\n",
    "\n",
    "01:36 - 02:30\n",
    "\n",
    "The Flesch Reading Ease is one of the oldest and most widely used readability tests. The score is based on two ideas: the first is that the greater the average sentence length, harder the text is to read. Consider these two sentences. The first is easier to follow than the second. The second is that the greater the average number of syllables in a word, the harder the text is to read. Therefore, I live in my home is considered easier to read than I reside in my domicile on account of its usage of lesser syllables per word. The higher the Flesch Reading Ease score, the greater is the readability. Therefore, a higher score indicates that the text is easier to understand.\n",
    "\n",
    "- One of the oldest and most widely used tests\n",
    "- **Greater the average sentence length, harder the text is to read**\n",
    "    - \"This is a short sentence.\"\n",
    "    - \"This is longer sentence with more words and it is harder to follow than the first sentence.\"\n",
    "- **Greater the average number of syllables in a word, harder the text is to read**\n",
    "    - \"I live in my home.\"\n",
    "    - \"I reside in my domicile.\"\n",
    "- Higher the score, greater the readability\n",
    "\n",
    "\n",
    "6\\. Flesch reading ease score interpretation\n",
    "--------------------------------------------\n",
    "\n",
    "02:30 - 02:49\n",
    "\n",
    "This table shows how to interpret the Flesch Reading Ease scores. A score above 90 would imply that the text is comprehensible to a 5th grader whereas a score below 30 would imply the text can only be understood by college graduates.\n",
    "\n",
    "| Reading ease score | Grade Level |\n",
    "|-------------------|-------------|\n",
    "| 90-100           | 5           |\n",
    "| 80-90            | 6           |\n",
    "| 70-80            | 7           |\n",
    "| 60-70            | 8-9         |\n",
    "| 50-60            | 10-12       |\n",
    "| 30-50            | College     |\n",
    "| 0-30             | College Graduate |\n",
    "\n",
    "7\\. Gunning fog index\n",
    "---------------------\n",
    "\n",
    "02:49 - 03:23\n",
    "\n",
    "The Gunning fog index was developed in 1954. Like Flesch, this score is also dependent on the average sentence length. However, it uses percentage of complex words in place of average syllables per word to compute its score. Here, complex words refer to all words that have three or more syllables. Unlike Flesch, the formula for Gunning fog index is such that the higher the score, the more difficult the passage is to understand.\n",
    "\n",
    "- Developed in 1954\n",
    "- Also dependent on average sentence length\n",
    "- Greater the percentage of complex words, harder the text is to read\n",
    "- Higher the index, lesser the readability\n",
    "\n",
    "8\\. Gunning fog index interpretation\n",
    "------------------------------------\n",
    "\n",
    "03:23 - 03:39\n",
    "\n",
    "The index can be interpreted using this table. A score of 6 would indicate 6th grade reading difficulty whereas a score of 17 would indicate college graduate level reading difficulty.\n",
    "\n",
    "| Fog index | Grade level |\n",
    "|-----------|-------------|\n",
    "| 17 | College graduate |\n",
    "| 16 | College senior |\n",
    "| 15 | College junior |\n",
    "| 14 | College sophomore |\n",
    "| 13 | College freshman |\n",
    "| 12 | High school senior |\n",
    "| 11 | High school junior |\n",
    "\n",
    "| Fog index | Grade level |\n",
    "|-----------|-------------|\n",
    "| 10 | High school sophomore |\n",
    "| 9 | High school freshman |\n",
    "| 8 | Eighth grade |\n",
    "| 7 | Seventh grade |\n",
    "| 6 | Sixth grade |\n",
    "\n",
    "9\\. The readability library\n",
    "---------------------------\n",
    "\n",
    "03:39 - 04:28\n",
    "\n",
    "We can conduct these tests in Python using the readability metrics library. In order to use this package, we first need to download the punkt module from nltk. We then import the Readability class from readability. Next, we create a Readability object and pass in the passage or text we're evaluating. To compute a readability score, we call a method that computes the score of our interest, for instance, gunning fog. We store this variable in a variable named gf. Next, we access the score using gf.score. In this example, the text that was passed is between the reading level of a college senior and that of a college graduate.\n",
    "\n",
    "```python\n",
    "# Download nltk punkt module\n",
    "import nltknltk.download('punkt_tab')\n",
    "# Import the Readability class\n",
    "from readability import Readability\n",
    "# Create a Readability Object\n",
    "readability_scores = Readability(text)\n",
    "# Generate scoresgf = readability_scores.gunning_fog()\n",
    "print(gf.score())\n",
    "\n",
    "Output: \n",
    "16.26\n",
    "```\n",
    "\n",
    "10\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:28 - 04:38\n",
    "\n",
    "Let's now practice computing readability scores using the readability library in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability of 'The Myth of Sisyphus'\n",
    "=====================================\n",
    "\n",
    "In this exercise, you will compute the Flesch reading ease score for Albert Camus' famous essay *The Myth of Sisyphus*. We will then interpret the value of this score as explained in the video and try to determine the reading level of the essay.\n",
    "\n",
    "The entire essay is in the form of a string and is available as `sisyphus_essay`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the `Readability` class from `readability`.\n",
    "-   Compute the `readability_scores` object for `sisyphus_essay` using `Readability`.\n",
    "-   Print the Flesch reading ease score using the `flesch` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# Compute the readability scores \n",
    "readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability of various publications\n",
    "===================================\n",
    "\n",
    "In this exercise, you have been given excerpts of articles from four publications. Your task is to compute the readability of these excerpts using the Gunning fog score and consequently, determine the relative difficulty of reading these publications.\n",
    "\n",
    "The excerpts are available as the following strings:\n",
    "\n",
    "-   `forbes`- An excerpt from an article from *Forbes* magazine on the Chinese social credit score system.\n",
    "-   `harvard_law`- An excerpt from a book review published in *Harvard Law Review*.\n",
    "-   `r_digest`- An excerpt from a *Reader's Digest* article on flight turbulence.\n",
    "-   `time_kids` - An excerpt from an article on the ill effects of salt consumption published in *TIME for Kids*.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the `Readability` class from `readability`.\n",
    "-   Compute the `gf` object for each `excerpt`using the `gunning_fog()` method on `Readability`.\n",
    "-   Compute the Gunning fog score using the the `score` attribute.\n",
    "-   Print the list of Gunning fog scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing, POS tagging and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Tokenization and Lemmatization\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "00:00 - 00:06\n",
    "\n",
    "In NLP, we usually have to deal with texts from a variety of sources. For instance,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Text sources\n",
    "\n",
    "----------------\n",
    "\n",
    "00:06 - 00:22\n",
    "\n",
    "it can be a news article where the text is grammatically correct and proofread. It could be tweets containing shorthands and hashtags. It could also be comments on YouTube where people have a tendency to abuse capital letters and punctuations.\n",
    "\n",
    "- News articles\n",
    "- Tweets\n",
    "- Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Making text machine friendly\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "00:22 - 01:03\n",
    "\n",
    "It is important that we standardize these texts into a machine friendly format. We want our models to treat similar words as the same. Consider the words Dogs and dog. Strictly speaking, they are different strings. However, they connotate the same thing. Similarly, reduction, reducing and reduce should also be standardized to the same string regardless of their form and case usage. Other examples include don't and do not, and won't and will not. In the next couple of lessons, we will learn techniques to achieve this.\n",
    "\n",
    "- `Dogs,dog`\n",
    "- `reduction`, `REDUCING`, `Reduce`\n",
    "- `don't`, `do not`\n",
    "- `won't`, `will not`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Text preprocessing techniques\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "01:03 - 01:31\n",
    "\n",
    "The text processing techniques you use are dependent on the application you're working on. We'll be covering the common ones, including converting words into lowercase removing unnecessary whitespace, removing punctuation, removing commonly occurring words or stopwords, expanding contracted words like don't and removing special characters such as numbers and emojis.\n",
    "\n",
    "- Converting words into lowercase\n",
    "- Removing leading and trailing whitespaces\n",
    "- Removing punctuation\n",
    "- Removing stopwordsExpanding contractions\n",
    "- Removing special characters (numbers, emojis, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Tokenization\n",
    "\n",
    "----------------\n",
    "\n",
    "01:31 - 02:21\n",
    "\n",
    "To do this, we must first understand tokenization. Tokenization is the process of splitting a string into its constituent tokens. These tokens may be sentences, words or punctuations and is specific to a particular language. In this course, we will primarily be focused with word and punctuation tokens. For instance, consider this sentence. Tokenizing it into its constituent words and punctuations will yield the following list of tokens. Tokenization also involves expanding contracted words. Therefore, a word like don't gets decomposed into two tokens: do and n't as can be seen in this example.\n",
    "\n",
    "\"I have a dog. His name is Hachi.\"\n",
    "Tokens:\n",
    "```python\n",
    "[\"I\", \"have\", \"a\", \"dog\", \".\", \"His\", \"name\", \"is\", \"Hachi\", \".\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Tokenization using spaCy\n",
    "\n",
    "----------------------------\n",
    "\n",
    "02:21 - 03:21\n",
    "\n",
    "To perform tokenization in python, we will use the spacy library. We first import the spacy library. Next, we load a pre-trained English model 'en_core_web_sm' using spacy.load(). This will return a Language object that has the know-how to perform tokenization. This is stored in the variable nlp. Let's now define a string we want to tokenize. We pass this string into nlp to generate a spaCy Doc object. We store this in a variable named doc. This Doc object contains the required tokens (and many other things, as we will soon find out). We generate the list of tokens by using list comprehension as shown. This is essentially looping over doc and extracting the text of each token in each iteration. The result is as follows.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "# Generate list of tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "['Hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', \"'\", 'm', 'doing', 'here', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Lemmatization\n",
    "\n",
    "-----------------\n",
    "\n",
    "03:21 - 04:07\n",
    "\n",
    "Lemmatization is the process of converting a word into its lowercased base form or lemma. This is an extremely powerful process of standardization. For instance, the words reducing, reduces, reduced and reduction, when lemmatized, are all converted into the base form reduce. Similarly be verbs such as am, are and is are converted into be. Lemmatization also allows us to convert words with apostrophes into their full forms. Therefore, n't is converted to not and 've is converted to have.\n",
    "\n",
    "- Convert word into its base form\n",
    "  - reducing, reduces, reduced, reduction → reduce\n",
    "  - am, are, is → be\n",
    "  - n't → not\n",
    "  - 've → have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Lemmatization using spaCy\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "04:07 - 04:42\n",
    "\n",
    "When you pass the string into nlp, spaCy automatically performs lemmatization by default. Therefore, generating lemmas is identical to generating tokens except that we extract token.lemma_ in each iteration inside the list comprehension instead of token.text. Also, observe how spaCy converted the Is into -PRON-. This is standard behavior where every pronoun is converted into the string '-PRON-'.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "# Generate list of lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "['hello', '!', '-PRON-', 'do', 'not', 'know', 'what', '-PRON-', 'be', 'do', 'here', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Let's practice!\n",
    "\n",
    "-------------------\n",
    "\n",
    "04:42 - 05:00\n",
    "\n",
    "Once we understand how to perform tokenization and lemmatization, performing the text preprocessing techniques described earlier becomes easier. Before we move to that, let's first practice our understanding of the concepts introduced so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying lemmas\n",
    "==================\n",
    "\n",
    "Identify the list of words from the choices which do not have the same lemma.\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "- [x] He, She, I, They\n",
    "\n",
    "-   Am, Are, Is, Was\n",
    "\n",
    "-   Increase, Increases, Increasing, Increased\n",
    "\n",
    "-   Car, Bike, Truck, Bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the Gettysburg Address\n",
    "=================================\n",
    "\n",
    "In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War.\n",
    "\n",
    "The entire speech is available as a string named `gettysburg`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Load the `en_core_web_sm` model using `spacy.load()`.\n",
    "-   Create a Doc object `doc` for the `gettysburg` string.\n",
    "-   Using list comprehension, loop over `doc` to generate the token texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing the Gettysburg address\n",
    "==================================\n",
    "\n",
    "In this exercise, we will perform lemmatization on the same `gettysburg` address from before. \n",
    "\n",
    "However, this time, we will also take a look at the speech, before and after lemmatization, and try to adjudge the kind of changes that take place to make the piece more machine friendly.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "Print the gettysburg address to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the gettysburg address\n",
    "print(gettysburg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "\n",
    "\n",
    "Loop over doc and extract the lemma for each token of gettysburg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over doc and extract the lemma for each token of gettysburg.\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "\n",
    "Convert lemmas into a string using join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Text cleaning\n",
    "-----------------\n",
    "\n",
    "00:00 - 00:08\n",
    "\n",
    "Now that we know how to convert a string into a list of lemmas, we are now in a good position to perform basic text cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Text cleaning techniques\n",
    "----------------------------\n",
    "\n",
    "00:08 - 00:31\n",
    "\n",
    "Some of the most common text cleaning steps include removing extra whitespaces, escape sequences, punctuations, special characters such as numbers and stopwords. In other words, it is very common to remove non-alphabetic tokens and words that occur so commonly that they are not very useful for analysis.\n",
    "\n",
    "- Unnecessary whitespaces and escape sequences\n",
    "- Punctuations\n",
    "- Special characters (numbers, emojis, etc.)\n",
    "- Stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. isalpha()\n",
    "-------------\n",
    "\n",
    "00:31 - 01:10\n",
    "\n",
    "Every python string has an isalpha() method that returns true if all the characters of the string are alphabets. Therefore, the \"Dog\".isalpha() will return true but \"3dogs\".isalpha() will return false as it has a non-alphabetic character 3. Similarly, numbers, punctuations and emojis will all return false too. This is an extremely convenient method to remove all (lemmatized) tokens that are or contain numbers, punctuation and emojis.\n",
    "\n",
    "```python\n",
    "\"Dog\".isalpha()\n",
    "```\n",
    "Output:\n",
    "```python\n",
    "True\n",
    "```\n",
    "\n",
    "```python\n",
    "\"3dogs\".isalpha()\n",
    "```\n",
    "Output:\n",
    "```python\n",
    "False\n",
    "```\n",
    "\n",
    "```python\n",
    "\"12347\".isalpha()\n",
    "```\n",
    "Output:\n",
    "```python\n",
    "False\n",
    "```\n",
    "\n",
    "```python\n",
    "\"!\".isalpha()\n",
    "```\n",
    "Output:\n",
    "```python\n",
    "False\n",
    "```\n",
    "\n",
    "```python\n",
    "\"?\".isalpha()\n",
    "```\n",
    "Output:\n",
    "```python\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. A word of caution\n",
    "---------------------\n",
    "\n",
    "01:10 - 01:56\n",
    "\n",
    "If isalpha() as a silver bullet that cleans text meticulously seems too good to be true, it's because it is. Remember that isalpha() has a tendency of returning false on words we would not want to remove. Examples include abbreviations such as USA and UK which have periods in them, and proper nouns with numbers in them such as word2vec and xto10x. For such nuanced cases, isalpha() may not be sufficient. It may be advisable to write your own custom functions, typically using regular expressions, to ensure you're not inadvertently removing useful words.\n",
    "\n",
    "```markdown\n",
    "- Abbreviations: U.S.A, U.K, etc.\n",
    "- Proper Nouns: word2vec and xto10x.\n",
    "- Write your own custom function (using regex) for the more nuanced cases.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Removing non-alphabetic characters\n",
    "--------------------------------------\n",
    "\n",
    "01:56 - 02:13\n",
    "\n",
    "Consider the string here. This has a lot of punctuations, unnecessary extra whitespace, escape sequences, numbers and emojis. We will generate the lemmatized tokens like before.\n",
    "\n",
    "```python\n",
    "string = \"\"\"\n",
    "OMG!!!! This is like    the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
    "\"\"\"\n",
    "import spacy\n",
    "\n",
    "# Generate list of tokens\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "['OMG', '!', 'This', 'be', 'like', 'the', 'best', 'thing', 'ever', '\\n', '.', 'Wow', ',', 'such', 'an', 'amazing', 'song', '!', 'I', \"'m\", 'hook', '.', 'Top', '5', 'definitely', '.', '?']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Removing non-alphabetic characters\n",
    "--------------------------------------\n",
    "\n",
    "02:13 - 02:35\n",
    "\n",
    "Next, we loop through the tokens again and choose only those words that are either -PRON- or contain only alphabetic characters. Let's now print out the sanitized string. We see that all the non-alphabetic characters have been removed and each word is separated by a single space.\n",
    "\n",
    "```python\n",
    "# Remove tokens that are not alphabetic\n",
    "a_lemmas = [lemma for lemma in lemmas\n",
    "            if lemma.isalpha() or lemma == '-PRON-']\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "'OMG this be like the good thing ever wow such an amazing song -PRON- be hooked top definitely'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Stopwords\n",
    "-------------\n",
    "\n",
    "02:35 - 02:55\n",
    "\n",
    "There are some words in the English language that occur so commonly that it is often a good idea to just ignore them. Examples include articles such as a and the, be verbs such as is and am and pronouns such as he and she.\n",
    "\n",
    "- Words that occur extremely commonly\n",
    "- Eg. articles, be verbs, pronouns, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Removing stopwords using spaCy\n",
    "----------------------------------\n",
    "\n",
    "02:55 - 03:03\n",
    "\n",
    "spaCy has a built-in list of stopwords which we can access using spacy.lang.en.stop_words.STOP_WORDS..\n",
    "\n",
    "# Get list of stopwords\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "string = \"\"\"\n",
    "OMG!!!! This is like    the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Removing stopwords using spaCy\n",
    "----------------------------------\n",
    "\n",
    "03:03 - 03:49\n",
    "\n",
    "We make a small tweak to a_lemmas generation step. Notice that we have removed the -PRON- condition as pronouns are stopwords anyway and should be removed. Additionally, we have introduced a new condition to check if the word belongs to spacy's list of stopwords. The output is as follows. Notice how the string consists only of base form words. Always exercise caution while using third party stopword lists. It is common that an application find certain words useful that may be considered a stopword by third party lists. It is often advisable to create your custom stopword lists.\n",
    "\n",
    "```python\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas\n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "'OMG like good thing wow amazing song hooked definitely'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Other text preprocessing techniques\n",
    "----------------------------------------\n",
    "\n",
    "03:49 - 04:07\n",
    "\n",
    "There are other preprocessing techniques that are used but have been omitted for the sake of brevity. Some of them include removing HTML or XML tags, replacing accented characters and correcting spelling errors and shorthands\n",
    "\n",
    "- Removing HTML/XML tags\n",
    "- Replacing accented characters (such as é)\n",
    "- Correcting spelling errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. A word of caution\n",
    "----------------------\n",
    "\n",
    "04:07 - 04:42\n",
    "\n",
    "We have covered a lot of text preprocessing techniques in the last couple of lessons. However, a word of caution is in place. The text preprocessing techniques you use is always dependent on the application. There are many applications which may find punctuations, numbers and emojis useful, so it may be wise to not remove them. In other cases, using all caps may be a good indicator of something. Remember to always use only those techniques that are relevant to your particular use case.\n",
    "\n",
    "- Always use only those text preprocessing techniques that are relevant to your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:42 - 04:45\n",
    "\n",
    "It's now time to practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning a blog post\n",
    "====================\n",
    "\n",
    "In this exercise, you have been given an excerpt from a blog post. Your task is to clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters.\n",
    "\n",
    "The excerpt is available as a string `blog` and has been printed to the console. The list of stopwords are available as `stopwords`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Using list comprehension, loop through `doc`to extract the `lemma_` of each token.\n",
    "-   Remove stopwords and non-alphabetic tokens using `stopwords` and `isalpha()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning TED talks in a dataframe\n",
    "=================================\n",
    "\n",
    "In this exercise, we will revisit the TED Talks from the first chapter. You have been a given a dataframe `ted` consisting of 5 TED Talks. Your task is to clean these talks using techniques discussed earlier by writing a function `preprocess` and applying it to the `transcript` feature of the dataframe. \n",
    "\n",
    "The stopwords list is available as `stopwords`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Generate the Doc object for `text`. Ignore the `disable` argument for now.\n",
    "-   Generate lemmas using list comprehension using the `lemma_` attribute.\n",
    "-   Remove non-alphabetic characters using `isalpha()` in the if condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Part-of-speech tagging\n",
    "--------------------------\n",
    "\n",
    "00:00 - 00:08\n",
    "\n",
    "In this lesson, we will cover part-of-speech tagging, which is one of the most popularly used feature engineering techniques in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Applications\n",
    "----------------\n",
    "\n",
    "00:08 - 01:14\n",
    "\n",
    "Part-of speech tagging or POS tagging has an immense number of applications in NLP. It is used in word-sense disambiguation to identify the sense of a word in a sentence. For instance, consider the sentences \"the bear is a majestic animal\" and \"please bear with me\". Both sentences use the word 'bear' but they mean different things. POS tagging helps in identifying this distinction by identifying one bear as a noun and the other as a verb. Consequentially, POS tagging is also used in sentiment analysis, question answering systems and linguistic approaches to detect fake news and opinion spam. For example, one paper discovered that fake news headlines, on average, tend to use lesser common nouns and more proper nouns than mainstream headlines. Generating the POS tags for these words proved extremely useful in detecting false or hyperpartisan news.\n",
    "\n",
    "```markdown\n",
    "- Word-sense disambiguation\n",
    "  - \"The bear is a majestic animal\"\n",
    "  - \"Please bear with me\"\n",
    "- Sentiment analysis\n",
    "- Question answering\n",
    "- Fake news and opinion spam detection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. POS tagging\n",
    "---------------\n",
    "\n",
    "01:14 - 01:45\n",
    "\n",
    "So what is POS tagging? It is the process of assigning every word (or token) in a piece of text, its corresponding part-of-speech. For instance, consider the sentence \"Jane is an amazing guitarist\". A typical POS tagger will label Jane as a proper noun, is as a verb, an as a determiner (or an article), amazing as an adjective and finally, guitarist as a noun.\n",
    "\n",
    "```markdown\n",
    "- Assigning every word, its corresponding part of speech.\n",
    "  - \"Jane is an amazing guitarist.\"\n",
    "- POS Tagging:\n",
    "  - Jane → proper noun\n",
    "  - is → verb\n",
    "  - an → determiner\n",
    "  - amazing → adjective\n",
    "  - guitarist → noun\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. POS tagging using spaCy\n",
    "---------------------------\n",
    "\n",
    "01:45 - 02:14\n",
    "\n",
    "POS Tagging is extremely easy to do using spaCy's models and performing it is almost identical to generating tokens or lemmas. As usual, we import the spacy library and load the en_core_web_sm model as nlp. We will use the same sentence \"Jane is an amazing guitarist\" from before. We will then create a Doc object that will perform POS tagging, by default.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize string\n",
    "string = \"Jane is an amazing guitarist\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. POS tagging using spaCy\n",
    "---------------------------\n",
    "\n",
    "02:14 - 03:08\n",
    "\n",
    "Using list comprehension, we generate a list of tuples pos where the first element of the tuple is the token and is generated using token.text and the second element is its POS tag, which is generated using token.pos_. Printing pos will give us the following output. Note how the tagger correctly identified all the parts-of-speech as we had discussed earlier. That said, remember that POS tagging is not an exact science. spaCy infers the POS tags of these words based on the predictions given by its pre-trained models. In other words, the accuracy of the POS tagging is dependent on the data that the model has been trained on and the data that it is being used on.\n",
    "\n",
    "```python\n",
    "# Generate list of tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)\n",
    "\n",
    "[('Jane', 'PROPN'),\n",
    " ('is', 'VERB'),\n",
    " ('an', 'DET'),\n",
    " ('amazing', 'ADJ'),\n",
    " ('guitarist', 'NOUN')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. POS annotations in spaCy\n",
    "----------------------------\n",
    "\n",
    "03:08 - 03:39\n",
    "\n",
    "spaCy is capable of identifying close to 20 parts-of-speech and as we saw in the previous slide, it uses specific annotations to denote a particular part of speech. For instance, PROPN referred to a proper noun and DET referred to a determinant. You can find the complete list of POS annotations used by spaCy in spaCy's documentation. Here is a snapshot of the web page.\n",
    "\n",
    "```markdown\n",
    "- PROPN → proper noun\n",
    "- DET → determiner\n",
    "- spaCy annotations at [https://spacy.io/api/annotation](https://spacy.io/api/annotation)\n",
    "\n",
    "| POS   | DESCRIPTION          | EXAMPLES                              |\n",
    "|-------|----------------------|---------------------------------------|\n",
    "| ADJ   | adjective            | big, old, green, incomprehensible, first |\n",
    "| ADP   | adposition           | in, to, during                       |\n",
    "| ADV   | adverb               | very, tomorrow, down, where, there   |\n",
    "| AUX   | auxiliary            | is, has (done), will (do), should (do) |\n",
    "| CONJ  | conjunction          | and, or, but                         |\n",
    "| CCONJ | coordinating conjunction | and, or, but                         |\n",
    "| DET   | determiner           | a, an, the                           |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:39 - 03:47\n",
    "\n",
    "Great! Let's now practice our understanding of POS tagging in the next few exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging in Lord of the Flies\n",
    "================================\n",
    "\n",
    "In this exercise, you will perform part-of-speech tagging on a famous passage from one of the most well-known novels of all time, *Lord of the Flies*, authored by William Golding.\n",
    "\n",
    "The passage is available as `lotf` and has already been printed to the console.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Load the `en_core_web_sm` model.\n",
    "-   Create a doc object for `lotf` using `nlp()`.\n",
    "-   Using the `text` and `pos_` attributes, generate tokens and their corresponding POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting nouns in a piece of text\n",
    "=================================\n",
    "\n",
    "In this exercise, we will write two functions, `nouns()` and `proper_nouns()` that will count the number of other nouns and proper nouns in a piece of text respectively.\n",
    "\n",
    "These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news. \n",
    "\n",
    "The `en_core_web_sm` model has already been loaded as `nlp` in this exercise.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Using the list `count` method, count the number of proper nouns (annotated as `PROPN`) in the `pos` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Using the list `count` method, count the number of other nouns (annotated as `NOUN`) in the `pos` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count(\"NOUN\")  # Count the number of NOUN tags\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun usage in fake news\n",
    "=======================\n",
    "\n",
    "In this exercise, you have been given a dataframe `headlines` that contains news headlines that are either fake or real. Your task is to generate two new features `num_propn`and `num_noun` that represent the number of proper nouns and other nouns contained in the `title` feature of `headlines`.\n",
    "\n",
    "Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the `num_propn` and `num_noun` features in fake news detectors will improve its performance.\n",
    "\n",
    "To accomplish this task, the functions `proper_nouns` and `nouns` that you had built in the previous exercise have already been made available to you.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   -   Create a new feature `num_propn` by applying `proper_nouns` to `headlines['title']`.\n",
    "    -   Filter `headlines` to compute the mean number of proper nouns in fake news using the `mean` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   -   Repeat the process for other nous: create a feature `'num_noun'` using `nouns` and compute the mean of other nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
