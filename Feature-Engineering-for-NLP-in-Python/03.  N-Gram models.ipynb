{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Building a bag of words model\n",
    "---------------------------------\n",
    "\n",
    "00:00 - 00:09\n",
    "\n",
    "In this chapter, we will cover vectorization which is, as you may recall, the process of converting text into vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Recap of data format for ML algorithms\n",
    "------------------------------------------\n",
    "\n",
    "00:09 - 00:31\n",
    "\n",
    "Recall that for any ML algorithm to run properly, data fed into it must be in tabular form and all the training features must be numerical. This is clearly not the case for textual data. In this lesson, we will learn a technique called bag of words that converts text documents into vectors.\n",
    "\n",
    "For any ML algorithm,\n",
    "- Data must be in tabular form\n",
    "- Training features must be numerical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Bag of words model\n",
    "----------------------\n",
    "\n",
    "00:31 - 00:56\n",
    "\n",
    "The bag of words model is a procedure of extracting word tokens from a text document (henceforth, we will refer to this as just document), computing the frequency of these word tokens and constructing a word vector based on these frequencies and the vocabulary of the entire corpus of documents. This is best explained with the help of an example.\n",
    "\n",
    "- Extract word tokens\n",
    "- Compute frequency of word tokens\n",
    "- Construct a word vector out of these frequencies and vocabulary of corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Bag of words model example\n",
    "------------------------------\n",
    "\n",
    "00:56 - 01:12\n",
    "\n",
    "Consider a corpus of three documents. The lion is the king of the jungle. Lions have an average lifespan of 15 years. And, the lion is an endangered species.\n",
    "\n",
    "```markdown\n",
    "Corpus\n",
    "- \"The lion is the king of the jungle\"\n",
    "- \"Lions have lifespans of a decade\"\n",
    "- \"The lion is an endangered species\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Bag of words model example\n",
    "------------------------------\n",
    "\n",
    "01:12 - 02:11\n",
    "\n",
    "We now extract the unique word tokens that occur in this corpus of documents. This will be the vocabulary of our model. In this example, the following 15 word tokens will constitute our vocabulary. Since there are 15 words in our vocabulary, our word vectors will have 15 dimensions and each dimension's value will correspond to the frequency of the word token corresponding to that dimension. For instance, the second dimension will correspond to the number of times the second word in the vocabulary, an, occurs in the document. Let's now convert our documents into word vectors using this bag of words model. The lion is the king of the jungle is converted to the following vector. Similarly, the other two sentences have the following word vector representations.\n",
    "\n",
    "```markdown\n",
    "Vocabulary → a, an, decade, endangered, have, is, jungle, king, lifespans, lion, Lions, of, species, the, The\n",
    "\n",
    "[0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 1]\n",
    "\n",
    "[1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0]\n",
    "\n",
    "[0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Text preprocessing\n",
    "----------------------\n",
    "\n",
    "02:11 - 03:01\n",
    "\n",
    "As we were constructing this model, you may have noticed how text preprocessing would have been extremely useful in creating arguably better models. We would usually want Lions and lion to mean the same thing and therefore, counted as the same thing. The same applies to 'the' with different cases. We would also want to remove punctuations and stopwords as they are extremely common and don't really contribute much to the character of the document. Performing text preprocessing usually leads to smaller vocabularies, which is a good thing. While working with vectorization, it is routine to form word vectors running into thousands of dimensions and keeping this to a minimum helps improve performance.\n",
    "\n",
    "```markdown\n",
    "- Lions, Lion → lion\n",
    "- The, the → the\n",
    "- No punctuations\n",
    "- No stopwords\n",
    "- Leads to smaller vocabularies\n",
    "- Reducing number of dimensions helps improve performance\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Bag of words model using sklearn\n",
    "------------------------------------\n",
    "\n",
    "03:01 - 03:16\n",
    "\n",
    "To construct the bag of words model in Python, we will use the scikit-learn library. We will use the corpus from before, consisting of the three sentences on lions. Let's ignore text preprocessing for now.\n",
    "\n",
    "```python\n",
    "corpus = pd.Series([\n",
    "    'The lion is the king of the jungle',\n",
    "    'Lions have lifespans of a decade',\n",
    "    'The lion is an endangered species'\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Bag of words model using sklearn\n",
    "------------------------------------\n",
    "\n",
    "03:16 - 04:29\n",
    "\n",
    "We import the CountVectorizer class from sklearn.feature_extraction.text. This is the class that will help us build our bag of words model. Next, we instantiate a CountVectorizer object vectorizer. We finally create our matrix of word vectors by passing corpus to the fit_transform method of vectorizer. This is stored in bow_matrix. This bow_matrix is a sparse matrix and we can print out its 2D array form using bow matrix dot toarray(). This gives us the following output. Notice how this is different from the word vectors we generated. This is because CountVectorizer automatically lowercases words and ignores single character tokens such as 'a'. Also, it doesn't necessarily index the vocabulary in alphabetical order. We will learn how to map the vocabulary to the indices in the exercises. We can now use this bow_matrix as our training features in ML models.\n",
    "\n",
    "```python\n",
    "# Import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "array([[0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 3],\n",
    "       [0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n",
    "       [1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1]], dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:29 - 04:36\n",
    "\n",
    "We've covered a lot of theory in this lesson. Let us practice this in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors with a given vocabulary\n",
    "====================================\n",
    "\n",
    "You have been given a corpus of documents and you have computed the vocabulary of the corpus to be the following: ***V***: *a, an, and, but, can, come, evening, forever, go, i, men, may, on, the, women*\n",
    "\n",
    "Which of the following corresponds to the bag of words vector for the document \"men may come and men may go but i go on forever\"?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[x] -   `(0, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 2, 1, 0, 0)`\n",
    "\n",
    "    PRESS1\n",
    "\n",
    "-   `(0, 1, 0, 1, 1, 1, 2, 0, 2, 1, 0, 0, 0, 2, 0)`\n",
    "\n",
    "    PRESS2\n",
    "\n",
    "-   `(2, 1, 0, 0, 2, 1, 0, 0, 0, 1)`\n",
    "\n",
    "    PRESS3\n",
    "\n",
    "-   `(0, 0, 1, 2, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1)`\n",
    "\n",
    "    PRESS4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW model for movie taglines\n",
    "============================\n",
    "\n",
    "In this exercise, you have been provided with a `corpus` of more than 7000 movie tag lines. Your job is to generate the bag of words representation `bow_matrix` for these taglines. For this exercise, we will ignore the text preprocessing step and generate `bow_matrix`directly.\n",
    "\n",
    "We will also investigate the shape of the resultant `bow_matrix`. The first five taglines in `corpus` have been printed to the console for you to examine.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the `CountVectorizer` class from `sklearn`.\n",
    "-   Instantiate a `CountVectorizer` object. Name it `vectorizer`.\n",
    "-   Using `fit_transform()`, generate `bow_matrix` for `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing dimensionality and preprocessing\n",
    "==========================================\n",
    "\n",
    "In this exercise, you have been provided with a `lem_corpus` which contains the pre-processed versions of the movie taglines from the previous exercise. In other words, the taglines have been lowercased and lemmatized, and stopwords have been removed. \n",
    "\n",
    "Your job is to generate the bag of words representation `bow_lem_matrix` for these lemmatized taglines and compare its shape with that of `bow_matrix` obtained in the previous exercise. The first five lemmatized taglines in `lem_corpus` have been printed to the console for you to examine.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import the `CountVectorizer` class from `sklearn`.\n",
    "-   Instantiate a `CountVectorizer` object. Name it `vectorizer`.\n",
    "-   Using `fit_transform()`, generate `bow_lem_matrix` for `lem_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n",
    "\n",
    "# Print the shape of bow_lem_matrix\n",
    "print(bow_lem_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping feature indices with feature names\n",
    "==========================================\n",
    "\n",
    "In the lesson video, we had seen that `CountVectorizer` doesn't necessarily index the vocabulary in alphabetical order. In this exercise, we will learn to map each feature index to its corresponding feature name from the vocabulary.\n",
    "\n",
    "We will use the same three sentences on lions from the video. The sentences are available in a list named `corpus` and has already been printed to the console.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Instantiate a `CountVectorizer` object. Name it `vectorizer`.\n",
    "-   Using `fit_transform()`, generate `bow_matrix` for `corpus`.\n",
    "-   Using the `get_feature_names()` method, map the column names to the corresponding word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Building a BoW Naive Bayes classifier\n",
    "-----------------------------------------\n",
    "\n",
    "00:00 - 00:09\n",
    "\n",
    "In this lesson, we will walk through a machine learning problem that utilizes feature engineering techniques we've learned, to arrive at a desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Spam filtering\n",
    "------------------\n",
    "\n",
    "00:09 - 00:28\n",
    "\n",
    "Let's take a look at the spam filtering problem. We're given a dataset of messages that have been labelled as spam or ham. Here, you can see a typical spam and ham message. Our task is to train an ML model that can predict the label given a particular text.\n",
    "\n",
    "```markdown\n",
    "| message                                                                                         | label |\n",
    "|-------------------------------------------------------------------------------------------------|-------|\n",
    "| WINNER!! As a valued network customer you have been selected to receive a $900 prize reward! To claim call 09061701461 | spam  |\n",
    "| Ah, work. I vaguely remember that. What does it feel like?                                        | ham   |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Steps\n",
    "---------\n",
    "\n",
    "00:28 - 00:51\n",
    "\n",
    "There are 3 steps involved. The first is to preprocess the text. Next, we proceed to build the bag-of-words model. Finally, we conduct predictive modeling using the generated BoW vectors. Note that although we use the term 'modeling' in the context of both BoW and machine learning, they mean two different things.\n",
    "\n",
    "1. Text preprocessing\n",
    "2. Building a bag-of-words model (or representation)\n",
    "3. Machine learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Text preprocessing using CountVectorizer\n",
    "--------------------------------------------\n",
    "\n",
    "00:51 - 02:17\n",
    "\n",
    "We've already learned how to conduct text preprocessing using spaCy. However, it is also possible to do this using CountVectorizer. CountVectorizer takes in a number of arguments to perform preprocessing. The lowercase argument, when set to True, converts words to lowercase. The strip_accents argument can convert accented characters according to unicode or ASCII mapping. Passing in a stopwords argument will lead to CountVectorizer ignoring stopwords. You can pass in a custom list or the string 'english' to use scikit-learn's list of English stopwords. You can specify tokenization using a regular expression as the value of the token_pattern argument. Tokenization can also be specified using a tokenizer argument. Here, you can pass a function that takes a string as an argument and returns a list of tokens. This way, CountVectorizer allows usage of spaCy's tokenization techniques. CountVectorizer cannot perform certain steps such as lemmatization automatically. This is where spaCy is useful. Although it performs tokenization and preprocessing, CountVectorizer's main job is to convert a corpus into a matrix of numerical vectors.\n",
    "\n",
    "```markdown\n",
    "CountVectorizer arguments:\n",
    "\n",
    "- lowercase: False, True\n",
    "- strip_accents: 'unicode', 'ascii', None\n",
    "- stop_words: 'english', list, None\n",
    "- token_pattern: regex\n",
    "- tokenizer: function\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Building the BoW model\n",
    "--------------------------\n",
    "\n",
    "02:17 - 02:57\n",
    "\n",
    "As usual, we import CountVectorizer from scikit-learn. We then instantiate a CountVectorizer object called vectorizer. We perform accent stripping using ASCII mapping and remove English stopwords. We also set the lowercase argument to False. This is because spam messages usually tend to abuse all-capital words and we might want to preserve this information for the ML step. The dataset has been already been loaded into the dataframe df. We split this dataset into training and test sets using scikit-learn's train test split function.\n",
    "\n",
    "```python\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', lowercase=False)\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.25)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Building the BoW model\n",
    "--------------------------\n",
    "\n",
    "02:57 - 03:32\n",
    "\n",
    "We now fit the vectorizer on the training set and transform it into its bag-of-words representation. We can perform both these steps together using the fit transform method. Next, we transform the test set into its BoW representation. Note, that we do not fit the vectorizer with the test data. It is possible that there are some words in the test data that is not in the vocabulary of the vectorizer. In such cases, CountVectorizer simply ignores these words.\n",
    "\n",
    "```python\n",
    "# Generate training Bow vectors\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Generate test BoW vectors\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Training the Naive Bayes classifier\n",
    "---------------------------------------\n",
    "\n",
    "03:32 - 04:11\n",
    "\n",
    "We're now in a good position to train an ML model. We will use the Multinomial Naive Bayes classifier for this task. We import the Multinomial NB class from scikit-learn and create an object named clf. We then fit the training BoW vectors and their corresponding labels to clf. We can now test the performance of our model. We compute the accuracy of the model on the test set using clf dot score. In this case, our model registered an accuracy of 76% on the test set.\n",
    "\n",
    "```python\n",
    "# Import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train clf\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Compute accuracy on test set\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(accuracy)\n",
    "\n",
    "0.760051\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:11 - 04:26\n",
    "\n",
    "We've covered a lot of ground in building a spam filter in this lesson. In the exercises, we will perform similar steps to perform sentiment analysis on movie reviews. Let's practice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
