{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9464b220",
   "metadata": {},
   "source": [
    "1\\. Building tf-idf document vectors\n",
    "------------------------------------\n",
    "\n",
    "00:00 - 00:04\n",
    "\n",
    "In the last chapter, we learned about n-gram modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12695c4",
   "metadata": {},
   "source": [
    "2\\. n-gram modeling\n",
    "-------------------\n",
    "\n",
    "00:04 - 00:29\n",
    "\n",
    "In n-gram modeling, the weight of a dimension for the vector representation of a document is dependent on the number of times the word corresponding to the dimension occurs in the document. Let's say we have a document that has the word 'human' occurring 5 times. Then, the dimension of its vector representation corresponding to 'human' would have the value 5.\n",
    "\n",
    "- Weight of dimension dependent on the frequency of the word corresponding to the dimension.\n",
    "- Document contains the word *human* in five places.\n",
    "- Dimension corresponding to *human* has weight 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b9e7a",
   "metadata": {},
   "source": [
    "3\\. Motivation\n",
    "--------------\n",
    "\n",
    "00:29 - 01:17\n",
    "\n",
    "However, some words occur very commonly across all the documents in the corpus. As a result, the vector representations get more characterized by these dimensions. Consider a corpus of documents on the Universe. Let's say there is a particular document on Jupiter where the word 'jupiter' and 'universe' both occur about 20 times. However, 'jupiter' rarely figures in the other documents whereas 'universe' is just as common. We could argue that although both *jupiter* and *universe* occur 20 times, *jupiter* should be given a larger weight on account of its exclusivity. In other words, the word 'jupiter' characterizes the document more than 'universe'.\n",
    "\n",
    "- Some words occur very commonly across all documents\n",
    "- Corpus of documents on the universe:\n",
    "  - One document has *jupiter* and *universe* occurring 20 times each.\n",
    "  - *jupiter* rarely occurs in the other documents. *universe* is common.\n",
    "  - Give more weight to *jupiter* on account of exclusivity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00b174",
   "metadata": {},
   "source": [
    "4\\. Applications\n",
    "----------------\n",
    "\n",
    "01:17 - 01:48\n",
    "\n",
    "Weighting words this way has a huge number of applications. They can be used to automatically detect stopwords for the corpus instead of relying on a generic list. They're used in search algorithms to determine the ranking of pages containing the search query and in recommender systems as we will soon find out. In a lot of cases, this kind of weighting also generates better performance during predictive modeling.\n",
    "\n",
    "- Automatically detect stopwords\n",
    "- Search\n",
    "- Recommender systems\n",
    "- Better performance in predictive modeling for some cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047ac50",
   "metadata": {},
   "source": [
    "5\\. Term frequency-inverse document frequency\n",
    "---------------------------------------------\n",
    "\n",
    "01:48 - 02:09\n",
    "\n",
    "The weighting mechanism we've described is known as term frequency-inverse document frequency or tf-idf for short. It is based on the idea that the weight of a term in a document should be proportional to its frequency and an inverse function of the number of documents in which it occurs.\n",
    "\n",
    "- Proportional to term frequency\n",
    "- Inverse function of the number of documents in which it occurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e68c4",
   "metadata": {},
   "source": [
    "6\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:09 - 02:16\n",
    "\n",
    "Mathematically, the weight of a term i in document j is computed as\n",
    "\n",
    "\n",
    "\\( w_{i,j} = tf_{i,j} \\cdot \\log \\left( \\frac{N}{df_i} \\right) \\)\n",
    "\n",
    "\\( w_{i,j} \\rightarrow \\) weight of term \\( i \\) in document \\( j \\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293c9a0",
   "metadata": {},
   "source": [
    "7\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:16 - 02:20\n",
    "\n",
    "term frequency of the term i in document j\n",
    "\n",
    "\n",
    "$$\n",
    "w_{i,j} = tf_{i,j} \\cdot \\log\\left(\\frac{N}{df_i}\\right)\n",
    "$$\n",
    "\n",
    "- \\(w_{i,j}\\) → weight of term \\(i\\) in document \\(j\\)\n",
    "- \\(tf_{i,j}\\) → term frequency of term \\(i\\) in document \\(j\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe308f",
   "metadata": {},
   "source": [
    "8\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:20 - 02:32\n",
    "\n",
    "multiplied by the log of the ratio of the number of documents in the corpus and the number of documents in which the term i occurs or dfi.\n",
    "\n",
    "\n",
    "$$\n",
    "w_{i,j} = tf_{i,j} \\cdot \\log\\left(\\frac{N}{df_i}\\right)\n",
    "$$\n",
    "\n",
    "- \\(w_{i,j}\\) → weight of term \\(i\\) in document \\(j\\)\n",
    "- \\(tf_{i,j}\\) → term frequency of term \\(i\\) in document \\(j\\)\n",
    "- \\(N\\) → number of documents in the corpus\n",
    "- \\(df_i\\) → number of documents containing term \\(i\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869af3c7",
   "metadata": {},
   "source": [
    "9\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:32 - 03:18\n",
    "\n",
    "Therefore, let's say the word 'library' occurs in a document 5 times. There are 20 documents in the corpus and 'library' occurs in 8 of them. Then, the tf-idf weight of 'library' in the vector representation of this document will be 5 times log of 20 by 8 which is approximately 2. In general, higher the tf-idf weight, more important is the word in characterizing the document. A high tf-idf weight for a word in a document may imply that the word is relatively exclusive to that particular document or that the word occurs extremely commonly in the document, or both.\n",
    "\n",
    "\n",
    "$$\n",
    "w_{i,j} = tf_{i,j} \\cdot \\log\\left(\\frac{N}{df_i}\\right)\n",
    "$$\n",
    "\n",
    "- \\(w_{i,j}\\) → weight of term \\(i\\) in document \\(j\\)\n",
    "- \\(tf_{i,j}\\) → term frequency of term \\(i\\) in document \\(j\\)\n",
    "- \\(N\\) → number of documents in the corpus\n",
    "- \\(df_i\\) → number of documents containing term \\(i\\)\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "w_{library, document} = 5 \\cdot log\\left(\\frac{20}{8}\\right) \\approx 2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57022628",
   "metadata": {},
   "source": [
    "10\\. tf-idf using scikit-learn\n",
    "------------------------------\n",
    "\n",
    "03:18 - 04:10\n",
    "\n",
    "Generating vectors that use tf-idf weighting is almost identical to what we've already done so far. Instead of using CountVectorizer, we use the TfidfVectorizer class of scikit-learn. The parameters and methods it has is almost identical to CountVectorizer. The only difference is that TfidfVectorizer assigns weights using the tf-idf formula from before and has extra parameters related to inverse document frequency which we will not cover in this course. Here, we can see how using TfidfVectorizer is almost identical to using CountVectorizer for a corpus. However, notice that the weights are non-integer and reflect values calculated by the tf-idf formula.\n",
    "\n",
    "```python\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "print(tfidf_matrix.toarray())\n",
    "```\n",
    "\n",
    "```python\n",
    "[[0.         0.         0.         0.25434658 0.33443519 0.33443519\n",
    "  0.25434658 0.         0.25434658 0.         0.76303975]\n",
    " [0.         0.46735098 0.         0.         0.46735098 0.\n",
    "  0.         0.46735098 0.35543247 0.         0.        ]\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7828250",
   "metadata": {},
   "source": [
    "11\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:10 - 04:14\n",
    "\n",
    "That's enough theory for now. Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5f7a8",
   "metadata": {},
   "source": [
    "tf-idf weight of commonly occurring words\n",
    "=========================================\n",
    "\n",
    "The word `bottle` occurs 5 times in a particular document `D` and also occurs in every document of the corpus. What is the tf-idf weight of `bottle` in `D`?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[x] -   0\n",
    "\n",
    "    PRESS1\n",
    "\n",
    "-   1\n",
    "\n",
    "    PRESS2\n",
    "\n",
    "-   Not defined\n",
    "\n",
    "    PRESS3\n",
    "\n",
    "-   5\n",
    "\n",
    "    PRESS4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42931799",
   "metadata": {},
   "source": [
    "tf-idf vectors for TED talks\n",
    "============================\n",
    "\n",
    "In this exercise, you have been given a corpus `ted` which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks.\n",
    "\n",
    "In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import `TfidfVectorizer` from `sklearn`.\n",
    "-   Create a `TfidfVectorizer` object. Name it `vectorizer`.\n",
    "-   Generate `tfidf_matrix` for `ted` using the `fit_transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer= TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423fd766",
   "metadata": {},
   "source": [
    "1\\. Cosine similarity\n",
    "---------------------\n",
    "\n",
    "00:00 - 00:25\n",
    "\n",
    "We now know how to compute vectors out of text documents. With this representation in mind, let us now explore techniques that will allow us to determine how similar two vectors and consequentially two documents, are to each other. More specifically, we will learn about the cosine similarity score which is one of the most popularly used similarity metrics in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae1defa",
   "metadata": {},
   "source": [
    "2\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "00:25 - 00:45\n",
    "\n",
    "Very simply put, the cosine similarity score of two vectors is the cosine of the angle between the vectors. Mathematically, it is the ratio of the dot product of the vectors and the product of the magnitude of the two vectors. Let's walk through what this formula really means.\n",
    "\n",
    "\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "\\[\n",
    "sim(A, B) = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\|\\|B\\|}\n",
    "\\]\n",
    "\n",
    "\n",
    "```plaintext\n",
    "          y\n",
    "          ^\n",
    "          |\n",
    "      10 -|        🐼 (A)\n",
    "          |      /\n",
    "          |     /\n",
    "       5 -|    /θ\n",
    "          |   /\n",
    "          |  /_______ 🐯 (B)\n",
    "          | /          \n",
    "          |/____________________> x\n",
    "            5    10    15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b005b",
   "metadata": {},
   "source": [
    "3\\. The dot product\n",
    "-------------------\n",
    "\n",
    "00:45 - 01:21\n",
    "\n",
    "The dot product is computed by summing the product of values across corresponding dimensions of the vectors. Let's say we have two n-dimensional vectors V and W as shown. Then, the dot product here would be v1 times w1 plus v2 times w2 and so on until vn times wn. As an example, consider two vectors A and B. By applying the formula above, we see that the dot product comes to 37.\n",
    "\n",
    "\n",
    "Consider two vectors,\n",
    "\n",
    "\\[\n",
    "V = (v_1, v_2, \\dots, v_n), W = (w_1, w_2, \\dots, w_n)\n",
    "\\]\n",
    "\n",
    "Then the dot product of \\( V \\) and \\( W \\) is,\n",
    "\n",
    "\\[\n",
    "V \\cdot W = (v_1 \\times w_1) + (v_2 \\times w_2) + \\dots + (v_n \\times w_n)\n",
    "\\]\n",
    "\n",
    "**Example:**\n",
    "\n",
    "\\[\n",
    "A = (4, 7, 1), B = (5, 2, 3)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "A \\cdot B = (4 \\times 5) + (7 \\times 2) + (1 \\times 3)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 20 + 14 + 3 = 37\n",
    "\\]\n",
    "\n",
    "```plaintext\n",
    "          y\n",
    "          ^\n",
    "          |\n",
    "      10 -|        A (4, 7, 1)\n",
    "          |       /\n",
    "          |      /\n",
    "       7 -|     /\n",
    "          |    /\n",
    "       5 -|   /\n",
    "          |  /\n",
    "       3 -| /  \n",
    "          |/____________________> x\n",
    "             5   7   1   3\n",
    "          B (5, 2, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00224085",
   "metadata": {},
   "source": [
    "4\\. Magnitude of a vector\n",
    "-------------------------\n",
    "\n",
    "01:21 - 01:57\n",
    "\n",
    "The magnitude of a vector is essentially the length of the vector. Mathematically, it is defined as the square root of the sum of the squares of values across all the dimensions of a vector. Therefore, for an n-dimensional vector V, the magnitude,mod V, is computed as the square root of v1 square plus v2 square and so on until vn square. Consider the vector A from before. Using the above formula, we compute its magnitude to be root 66.\n",
    "\n",
    "\n",
    "For any vector,\n",
    "\n",
    "\\[\n",
    "V = (v_1, v_2, \\dots, v_n)\n",
    "\\]\n",
    "\n",
    "The magnitude is defined as,\n",
    "\n",
    "\\[\n",
    "\\|V\\| = \\sqrt{(v_1)^2 + (v_2)^2 + \\dots + (v_n)^2}\n",
    "\\]\n",
    "\n",
    "**Example:**\n",
    "\n",
    "\\[\n",
    "A = (4, 7, 1), B = (5, 2, 3)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\|A\\| = \\sqrt{(4)^2 + (7)^2 + (1)^2}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= \\sqrt{16 + 49 + 1} = \\sqrt{66}\n",
    "\\]\n",
    "\n",
    "```plaintext\n",
    "                    A (4, 7, 1)\n",
    "                       |\n",
    "                       |\\\n",
    "                       | \\\n",
    "                       |  \\   Magnitude of A = √66\n",
    "                       |   \\\n",
    "                       |    \\\n",
    "                       |     \\\n",
    "                       |______\\__________________________> B (5, 2, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40ebeb",
   "metadata": {},
   "source": [
    "5\\. The cosine score\n",
    "--------------------\n",
    "\n",
    "01:57 - 02:23\n",
    "\n",
    "We are now in a position to compute the cosine similarity score of A and B. It is the dot product, which is 37, divided by the product of the magnitudes of A and B, which are root 66 and root 38 respectively. The value comes out to be approximately 0.738, which is the value of the cosine of the angle theta between the two vectors.\n",
    "\n",
    "\n",
    "For vectors,\n",
    "\n",
    "\\[\n",
    "A : (4, 7, 1), B : (5, 2, 3)\n",
    "\\]\n",
    "\n",
    "The cosine score,\n",
    "\n",
    "\\[\n",
    "\\cos(A, B) = \\frac{A \\cdot B}{|A| \\cdot |B|}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= \\frac{37}{\\sqrt{66} \\times \\sqrt{38}} = 0.7388\n",
    "\\]\n",
    "\n",
    "```plaintext\n",
    "          y\n",
    "          ^\n",
    "          |\n",
    "      10 -|         B (5, 2, 3)\n",
    "          |        /\n",
    "          |       /\n",
    "       7 -|      /θ\n",
    "          |     /\n",
    "          |    /\n",
    "       5 -|   /\n",
    "          |  /\n",
    "       3 -| /  \n",
    "          |/____________________> x\n",
    "             5   7   1   3\n",
    "          A (4, 7, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4d0e1c",
   "metadata": {},
   "source": [
    "6\\. Cosine Score: points to remember\n",
    "------------------------------------\n",
    "\n",
    "02:23 - 03:03\n",
    "\n",
    "Since the cosine score is simply the cosine of the angle between two vectors, its value is bounded between -1 and 1. However, in NLP, document vectors almost always use non-negative weights. Therefore, cosine scores vary between 0 and 1 where 0 indicates no similarity and 1 indicates that the documents are identical. Finally, since the cosine score ignores the magnitude of the vectors, it is fairly robust to document length. This may be an advantage or a disadvantage depending on the use case.\n",
    "\n",
    "- Value between -1 and 1.\n",
    "- In NLP, value between 0 and 1.\n",
    "- Robust to document length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b011b79",
   "metadata": {},
   "source": [
    "7\\. Implementation using scikit-learn\n",
    "-------------------------------------\n",
    "\n",
    "03:03 - 03:42\n",
    "\n",
    "Scikit-learn offers a cosine_similarity function that outputs a similarity matrix containing the pairwise cosine scores for a set of vectors. You can import cosine_similarity from sklearn dot metrics dot pairwise. However, remember that cosine_similarity takes in 2-D arrays as arguments. Passing in 1-D arrays will throw an error. Let us compute the cosine similarity scores of vectors A and B from before. We see that we get the same answer of 0.738 from before.\n",
    "\n",
    "```python\n",
    "# Import the cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define two 3-dimensional vectors A and B\n",
    "A = (4, 7, 1)\n",
    "B = (5, 2, 3)\n",
    "\n",
    "# Compute the cosine score of A and B\n",
    "score = cosine_similarity([A], [B])\n",
    "\n",
    "# Print the cosine score\n",
    "print(score)\n",
    "```\n",
    "```plaintext\n",
    "array([[0.73881883]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53314397",
   "metadata": {},
   "source": [
    "8\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:42 - 03:46\n",
    "\n",
    "That's enough theory for now. Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0586745",
   "metadata": {},
   "source": [
    "Range of cosine scores\n",
    "======================\n",
    "\n",
    "Which of the following is a possible cosine score for a pair of document vectors?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[x] -   0.86\n",
    "\n",
    "    PRESS1\n",
    "\n",
    "-   -0.52\n",
    "\n",
    "    PRESS2\n",
    "\n",
    "-   2.36\n",
    "\n",
    "    PRESS3\n",
    "\n",
    "-   -1.32\n",
    "\n",
    "    PRESS4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798ba35",
   "metadata": {},
   "source": [
    "Computing dot product\n",
    "=====================\n",
    "\n",
    "In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the `numpy` library. More specifically, we will use the `np.dot()` function to compute the dot product of two numpy arrays.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Initialize `A` (1,3) and `B` (-2,2) as `numpy`arrays using `np.array()`.\n",
    "-   Compute the dot product using `np.dot()`and passing `A` and `B` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize numpy vectors\n",
    "A = np.array([1,3])\n",
    "B = np.array([-2,2])\n",
    "\n",
    "# Compute dot product\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "# Print dot product\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020629a7",
   "metadata": {},
   "source": [
    "Cosine similarity matrix of a corpus\n",
    "====================================\n",
    "\n",
    "In this exercise, you have been given a `corpus`, which is a list containing five sentences. The `corpus` is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf). \n",
    "\n",
    "Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Initialize an instance of `TfidfVectorizer`. Name it `tfidf_vectorizer`.\n",
    "-   Using `fit_transform()`, generate the tf-idf vectors for `corpus`. Name it `tfidf_matrix`.\n",
    "-   Use `cosine_similarity()` and pass `tfidf_matrix` to compute the cosine similarity matrix `cosine_sim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity( tfidf_matrix,tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1dee73",
   "metadata": {},
   "source": [
    "1\\. Building a plot line based recommender\n",
    "------------------------------------------\n",
    "\n",
    "00:00 - 00:09\n",
    "\n",
    "In this lesson, we will use tf-idf vectors and cosine scores to build a recommender system that suggests movies based on overviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8447bff",
   "metadata": {},
   "source": [
    "2\\. Movie recommender\n",
    "---------------------\n",
    "\n",
    "00:09 - 00:19\n",
    "\n",
    "We've a dataset containing movie overviews. Here, we can see two movies, Shanghai Triad and Cry, the Beloved Country and their overviews.\n",
    "\n",
    "```markdown\n",
    "| Title               | Overview                                                                 |\n",
    "|---------------------|--------------------------------------------------------------------------|\n",
    "| Shanghai Triad       | A provincial boy related to a Shanghai crime family is recruited by his uncle into cosmopolitan Shanghai in the 1930s to be a servant to a ganglord's mistress. |\n",
    "| Cry, the Beloved Country | A South-African preacher goes to search for his wayward son who has committed a crime in the big city. |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d623c1",
   "metadata": {},
   "source": [
    "3\\. Movie recommender\n",
    "---------------------\n",
    "\n",
    "00:19 - 00:40\n",
    "\n",
    "Our task is to build a system that takes in a movie title and outputs a list of movies that has similar plot lines. For instance, if we passed in 'The Godfather', we could expect output like this. Notice how a lot of the movies listed here have to do with crime and gangsters, just like The Godfather.\n",
    "\n",
    "```python\n",
    "get_recommendations(\"The Godfather\")\n",
    "```\n",
    "```plaintext\n",
    "1178     The Godfather: Part II\n",
    "44030    The Godfather Trilogy: 1972–1990\n",
    "1914     The Godfather: Part III\n",
    "23126    Blood Ties\n",
    "11297    Household Saints\n",
    "34717    Start Liquidation\n",
    "10821    Election\n",
    "38030    Goodfellas\n",
    "17729    Short Sharp Shock\n",
    "26293    Beck 28 - Familjen\n",
    "Name: title, dtype: object\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6919cb",
   "metadata": {},
   "source": [
    "4\\. Steps\n",
    "---------\n",
    "\n",
    "00:40 - 01:09\n",
    "\n",
    "Following are the steps involved. The first step, as always, is to preprocess movie overviews. The next step is to generate the tf-idf vectors for our overviews. Finally, we generate a cosine similarity matrix which contains the pairwise similarity scores of every movie with every other movie. Once the cosine similarity matrix is computed, we can proceed to build the recommender function.\n",
    "\n",
    "1. Text preprocessing\n",
    "2. Generate tf-idf vectors\n",
    "3. Generate cosine similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c640a",
   "metadata": {},
   "source": [
    "5\\. The recommender function\n",
    "----------------------------\n",
    "\n",
    "01:09 - 01:59\n",
    "\n",
    "We will build a recommender function as part of this course. Let's take a look at how it works. The recommender function takes a movie title, the cosine similarity matrix and an indices series as arguments. The indices series is a reverse mapping of movie titles with their indices in the original dataframe. The function extracts the pairwise cosine similarity scores of the movie passed in with every other movie. Next, it sorts these scores in descending order. Finally, it outputs the titles of movies corresponding to the highest similarity scores. Note that the function ignores the highest similarity score of 1. This is because the movie most similar to a given movie is the movie itself!\n",
    "\n",
    "1. Take a movie title, cosine similarity matrix and indices series as arguments.\n",
    "2. Extract pairwise cosine similarity scores for the movie.\n",
    "3. Sort the scores in descending order.\n",
    "4. Output titles corresponding to the highest scores.\n",
    "5. Ignore the highest similarity score (of 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c6fc14",
   "metadata": {},
   "source": [
    "6\\. Generating tf-idf vectors\n",
    "-----------------------------\n",
    "\n",
    "01:59 - 02:10\n",
    "\n",
    "Let's say we already have the preprocessed movie overviews as 'movie_plots'. We already know how to generate the tf-idf vectors.\n",
    "\n",
    "```python\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of tf-idf vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(movie_plots)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a55699",
   "metadata": {},
   "source": [
    "7\\. Generating cosine similarity matrix\n",
    "---------------------------------------\n",
    "\n",
    "02:10 - 02:51\n",
    "\n",
    "Generating the cosine similarity matrix is also extremely simple. We simply pass in tfidf_matrix as both the first and second argument of cosine_similarity. This generates a matrix that contains the pairwise similarity score of every movie with every other movie. The value corresponding to the ith row and the jth column is the cosine similarity score of movie i with movie j. Notice that the diagonal elements of this matrix is 1. This is because, as stated earlier, the cosine similarity score of movie k with itself is 1.\n",
    "\n",
    "```python\n",
    "# Import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Generate cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "```\n",
    "```plaintext\n",
    "array([[1.        , 0.27435345, 0.23092036, ..., 0.00758112],\n",
    "       [0.27435345, 1.        , 0.1246955 , ..., 0.00740494],\n",
    "       ...,\n",
    "       [0.00758112, 0.00740494, 0.        , ..., 1.        ]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39780114",
   "metadata": {},
   "source": [
    "8\\. The linear_kernel function\n",
    "------------------------------\n",
    "\n",
    "02:51 - 03:37\n",
    "\n",
    "The magnitude of a tf-idf vector is always 1. Recall from the previous lesson that the cosine score is computed as the ratio of the dot product and the product of the magnitude of the vectors. Since the magnitude is 1, the cosine score of two tf-idf vectors is equal to their dot product! This fact can help us greatly improve the speed of computation of our cosine similarity matrix as we do not need to compute the magnitudes while working with tf-idf vectors. Therefore, while working with tf-idf vectors, we can use the linear_kernel function which computes the pairwise dot product of every vector with every other vector.\n",
    "\n",
    "```markdown\n",
    "- Magnitude of a tf-idf vector is 1\n",
    "- Cosine score between two tf-idf vectors is their dot product.\n",
    "- Can significantly improve computation time.\n",
    "- Use `linear_kernel` instead of `cosine_similarity`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5fa81",
   "metadata": {},
   "source": [
    "9\\. Generating cosine similarity matrix\n",
    "---------------------------------------\n",
    "\n",
    "03:37 - 03:48\n",
    "\n",
    "Let us replace the cosine_similarity function with linear_kernel. As you can see, the output remains the same but it takes significantly lesser time to compute.\n",
    "\n",
    "```python\n",
    "# Import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Generate cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "```\n",
    "```plaintext\n",
    "array([[1.        , 0.27435345, 0.23092036, ..., 0.00758112],\n",
    "       [0.27435345, 1.        , 0.1246955 , ..., 0.00740494],\n",
    "       ...,\n",
    "       [0.00758112, 0.00740494, 0.        , ..., 1.        ]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cd89b",
   "metadata": {},
   "source": [
    "10\\. The get_recommendations function\n",
    "-------------------------------------\n",
    "\n",
    "03:48 - 04:01\n",
    "\n",
    "The recommender function and the indices series described earlier will be built in the exercises. You can use this function to generate recommendations using the cosine similarity matrix.\n",
    "\n",
    "```python\n",
    "get_recommendations('The Lion King', cosine_sim, indices)\n",
    "```\n",
    "```plaintext\n",
    "7782     African Cats\n",
    "5877     The Lion King 2: Simba's Pride\n",
    "4524     Born Free\n",
    "2719     The Bear\n",
    "4770     Once Upon a Time in China III\n",
    "7070     Crows Zero\n",
    "739      The Wizard of Oz\n",
    "8926     The Jungle Book\n",
    "1749     Shadow of a Doubt\n",
    "7993     October Baby\n",
    "Name: title, dtype: object\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8fbd4d",
   "metadata": {},
   "source": [
    "11\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:01 - 04:10\n",
    "\n",
    "In the exercises, you will build recommendation systems of your own and see them in action. Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e148a",
   "metadata": {},
   "source": [
    "Comparing linear_kernel and cosine_similarity\n",
    "=============================================\n",
    "\n",
    "In this exercise, you have been given `tfidf_matrix` which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using `cosine_similarity`and then, using `linear_kernel`. \n",
    "\n",
    "We will then compare the computation times for both functions.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Compute the cosine similarity matrix for `tfidf_matrix` using `cosine_similarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5daa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d1660",
   "metadata": {},
   "source": [
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Compute the cosine similarity matrix for `tfidf_matrix` using `cosine_similarity`.\n",
    "\n",
    "-   Compute the cosine similarity matrix for `tfidf_matrix` using `linear_kernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d97fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1283a",
   "metadata": {},
   "source": [
    "Plot recommendation engine\n",
    "==========================\n",
    "\n",
    "In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a `get_recommendations()` function that takes in the title of a movie, a similarity matrix and an `indices` series as its arguments and outputs a list of most similar movies. `indices`has already been provided to you.\n",
    "\n",
    "You have also been given a `movie_plots`Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots.\n",
    "\n",
    "Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Initialize a `TfidfVectorizer` with English `stop_words`. Name it `tfidf`.\n",
    "-   Construct `tfidf_matrix` by fitting and transforming the movie plot data using `fit_transform()`.\n",
    "-   Generate the cosine similarity matrix `cosine_sim` using `tfidf_matrix`. Don't use `cosine_similarity()`!\n",
    "-   Use `get_recommendations()` to generate recommendations for `'The Dark Knight Rises'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84880760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7f614",
   "metadata": {},
   "source": [
    "The recommender function\n",
    "========================\n",
    "\n",
    "In this exercise, we will build a recommender function `get_recommendations()`, as discussed in the lesson and the previous exercise. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself).\n",
    "\n",
    "You have been given a dataset `metadata` that consists of the movie titles and overviews. The head of this dataset has been printed to console.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Get index of the movie that matches the title by using the `title` key of `indices`.\n",
    "-   Extract the ten most similar movies from `sim_scores` and store it back in `sim_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba019a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78facfc5",
   "metadata": {},
   "source": [
    "TED talk recommender\n",
    "====================\n",
    "\n",
    "In this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts. You have been given a `get_recommendations()` function that takes in the title of a talk, a similarity matrix and an `indices` series as its arguments, and outputs a list of most similar talks. `indices`has already been provided to you.\n",
    "\n",
    "You have also been given a `transcripts`series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts.\n",
    "\n",
    "Consequently, we will generate recommendations for a talk titled '5 ways to kill your dreams' by Brazilian entrepreneur Bel Pesce.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Initialize a `TfidfVectorizer` with English stopwords. Name it `tfidf`.\n",
    "-   Construct `tfidf_matrix` by fitting and transforming `transcripts`.\n",
    "-   Generate the cosine similarity matrix `cosine_sim` using `tfidf_matrix`.\n",
    "-   Use `get_recommendations()` to generate recommendations for '5 ways to kill your dreams'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739ba1c",
   "metadata": {},
   "source": [
    "1\\. Beyond n-grams: word embeddings\n",
    "-----------------------------------\n",
    "\n",
    "00:00 - 00:12\n",
    "\n",
    "We have covered a lot of ground in the last 4 chapters. However, before we bid adieu, we will cover one advanced topic that has a large number of applications in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bef00c",
   "metadata": {},
   "source": [
    "2\\. The problem with BoW and tf-idf\n",
    "-----------------------------------\n",
    "\n",
    "00:12 - 00:50\n",
    "\n",
    "Consider the three sentences, I am happy, I am joyous and I am sad. Now if we were to compute the similarities, I am happy and I am joyous would have the same score as I am happy and I am sad, regardless of how we vectorize it. This is because 'happy', 'joyous' and 'sad' are considered to be completely different words. However, we know that happy and joyous are more similar to each other than sad. This is something that the vectorization techniques we've covered so far simply cannot capture.\n",
    "\n",
    "```markdown\n",
    "'I am happy'\n",
    "'I am joyous'\n",
    "'I am sad'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a781a0c",
   "metadata": {},
   "source": [
    "3\\. Word embeddings\n",
    "-------------------\n",
    "\n",
    "00:50 - 01:53\n",
    "\n",
    "Word embedding is the process of mapping words into an n-dimensional vector space. These vectors are usually produced using deep learning models and huge amounts of data. The techniques used are beyond the scope of this course. However, once generated, these vectors can be used to discern how similar two words are to each other. Consequently, they can also be used to detect synonyms and antonyms. Word embeddings are also capable of capturing complex relationships. For instance, it can be used to detect that the words king and queen relate to each other the same way as man and woman. Or that France and Paris are related in the same way as Russia and Moscow. One last thing to note is that word embeddings are not trained on user data; they are dependent on the pre-trained spacy model you're using and are independent of the size of your dataset.\n",
    "\n",
    "```markdown\n",
    "- Mapping words into an n-dimensional vector space\n",
    "- Produced using deep learning and huge amounts of data\n",
    "- Discern how similar two words are to each other\n",
    "- Used to detect synonyms and antonyms\n",
    "- Captures complex relationships\n",
    "  - King - Queen → Man - Woman\n",
    "  - France - Paris → Russia - Moscow\n",
    "- Dependent on spacy model; independent of dataset you use\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e735368",
   "metadata": {},
   "source": [
    "4\\. Word embeddings using spaCy\n",
    "-------------------------------\n",
    "\n",
    "01:53 - 02:34\n",
    "\n",
    "Generating word embeddings is easy using spaCy's pre-trained models. As usual, we load the spacy model and create the doc object for our string. Note that it is advisable to load larger spacy models while working with word vectors. This is because the en_core_web_sm model does not technically ship with word vectors but context specific tensors, which tend to give relatively poorer results. We generate word vectors for each word by looping through the tokens and accessing the vector attribute. The truncated output is as shown.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp('I am happy')\n",
    "\n",
    "# Generate word vectors for each token\n",
    "for token in doc:\n",
    "    print(token.vector)\n",
    "```\n",
    "```plaintext\n",
    "[-1.0747459e+00  4.8677087e-02  5.6630421e+00  1.6680446e+00 -1.3194644e+00 -1.5142369e+00  1.1940931e+00 -3.0168812e+00 ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9c157",
   "metadata": {},
   "source": [
    "5\\. Word similarities\n",
    "---------------------\n",
    "\n",
    "02:34 - 03:02\n",
    "\n",
    "We can compute how similar two words are to each other by using the similarity method of a spacy token. Let's say we want to compute how similar happy, joyous and sad are to each other. We define a doc containing the three words. We then use a nested loop to calculate the similarity scores between each pair of words. As expected, happy and joyous are more similar to each other than they are to sad.\n",
    "\n",
    "```python\n",
    "doc = nlp(\"happy joyous sad\")\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n",
    "```\n",
    "```plaintext\n",
    "happy happy 1.0\n",
    "happy joyous 0.63244456\n",
    "happy sad 0.37338886\n",
    "joyous happy 0.63244456\n",
    "joyous joyous 1.0\n",
    "joyous sad 0.5340932\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5931c7",
   "metadata": {},
   "source": [
    "6\\. Document similarities\n",
    "-------------------------\n",
    "\n",
    "03:02 - 03:45\n",
    "\n",
    "Spacy also allows us to directly compute the similarity between two documents by using the average of the word vectors of all the words in a particular document. Let's consider the three sentences from before. We create doc objects for the sentences. Like spacy tokens, docs also have a similarity method. Therefore, we can compute the similarity between two docs as follows. As expected, I am happy is more similar to I am joyous than it is to I am sad. Note that the similarity scores are high in both cases because all sentences share 2 out of their three words, I and am.\n",
    "\n",
    "```python\n",
    "# Generate doc objects\n",
    "sent1 = nlp(\"I am happy\")\n",
    "sent2 = nlp(\"I am sad\")\n",
    "sent3 = nlp(\"I am joyous\")\n",
    "\n",
    "# Compute similarity between sent1 and sent2\n",
    "sent1.similarity(sent2)\n",
    "```\n",
    "```plaintext\n",
    "0.9273363837282105\n",
    "```\n",
    "\n",
    "```python\n",
    "# Compute similarity between sent1 and sent3\n",
    "sent1.similarity(sent3)\n",
    "```\n",
    "```plaintext\n",
    "0.9403554938594568\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b50f6",
   "metadata": {},
   "source": [
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:45 - 03:55\n",
    "\n",
    "With this, we come to an end of this lesson. Let's now practice our new found skills in the last set of exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ba993",
   "metadata": {},
   "source": [
    "Generating word vectors\n",
    "=======================\n",
    "\n",
    "In this exercise, we will generate the pairwise similarity scores of all the words in a sentence. The sentence is available as `sent` and has been printed to the console for your convenience.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create a `Doc` object `doc` for `sent`.\n",
    "-   In the nested loop, compute the similarity between `token1` and `token2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b825c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "  for token2 in doc:\n",
    "    print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff79d48",
   "metadata": {},
   "source": [
    "Computing similarity of Pink Floyd songs\n",
    "========================================\n",
    "\n",
    "In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely 'High Hopes', 'Hey You' and 'Mother'. The lyrics to these songs are available as `hopes`, `hey` and `mother` respectively.\n",
    "\n",
    "Your task is to compute the pairwise similarity between `mother` and `hopes`, and `mother`and `hey`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create `Doc` objects for `mother`, `hopes`and `hey`.\n",
    "-   Compute the similarity between `mother`and `hopes`.\n",
    "-   Compute the similarity between `mother`and `hey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd981c6e",
   "metadata": {},
   "source": [
    "1\\. Congratulations!\n",
    "--------------------\n",
    "\n",
    "00:00 - 00:04\n",
    "\n",
    "Congratulations on making it to the end of the course!\n",
    "\n",
    "2\\. Review\n",
    "----------\n",
    "\n",
    "00:04 - 01:16\n",
    "\n",
    "In this course, we learned about various feature engineering techniques for natural language processing in python. We started off by computing basic features such as character length and word length of documents. We then moved on to readability scores and learned various metrics that could help us deduce the amount of education required to comprehend a piece of text fully. Next, we were introduced to the spacy library and learned to perform tokenization and lemmatization. Building on these techniques, we proceeded to explore text cleaning. We also learned how to perform part of speech tagging and named entity recognition using spacy models and had a sneak peek at their applications. The third chapter was dedicated to n-gram modeling. We also explored an application of it in sentiment analysis of movie reviews. The final chapter saw us covering tf-idf vectors and cosine similarity. Using these concepts, we built a movie and a TED Talk recommender. The final lesson gave you a sneak peek into word embeddings and their use cases.\n",
    "\n",
    "3\\. Further resources\n",
    "---------------------\n",
    "\n",
    "01:16 - 01:28\n",
    "\n",
    "This, by no means, is the end of the road. Once you're done with this course, it is highly recommended that you take the following courses, also offered by DataCamp to muscle up your skills further.\n",
    "\n",
    "4\\. Thank you!\n",
    "--------------\n",
    "\n",
    "01:28 - 01:37\n",
    "\n",
    "We hope you have enjoyed taking this course as much as we did developing it. Thank you and all the best with your data science journey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
