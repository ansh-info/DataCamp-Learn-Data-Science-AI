{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9464b220",
   "metadata": {},
   "source": [
    "1\\. Building tf-idf document vectors\n",
    "------------------------------------\n",
    "\n",
    "00:00 - 00:04\n",
    "\n",
    "In the last chapter, we learned about n-gram modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12695c4",
   "metadata": {},
   "source": [
    "2\\. n-gram modeling\n",
    "-------------------\n",
    "\n",
    "00:04 - 00:29\n",
    "\n",
    "In n-gram modeling, the weight of a dimension for the vector representation of a document is dependent on the number of times the word corresponding to the dimension occurs in the document. Let's say we have a document that has the word 'human' occurring 5 times. Then, the dimension of its vector representation corresponding to 'human' would have the value 5.\n",
    "\n",
    "- Weight of dimension dependent on the frequency of the word corresponding to the dimension.\n",
    "- Document contains the word *human* in five places.\n",
    "- Dimension corresponding to *human* has weight 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b9e7a",
   "metadata": {},
   "source": [
    "3\\. Motivation\n",
    "--------------\n",
    "\n",
    "00:29 - 01:17\n",
    "\n",
    "However, some words occur very commonly across all the documents in the corpus. As a result, the vector representations get more characterized by these dimensions. Consider a corpus of documents on the Universe. Let's say there is a particular document on Jupiter where the word 'jupiter' and 'universe' both occur about 20 times. However, 'jupiter' rarely figures in the other documents whereas 'universe' is just as common. We could argue that although both *jupiter* and *universe* occur 20 times, *jupiter* should be given a larger weight on account of its exclusivity. In other words, the word 'jupiter' characterizes the document more than 'universe'.\n",
    "\n",
    "- Some words occur very commonly across all documents\n",
    "- Corpus of documents on the universe:\n",
    "  - One document has *jupiter* and *universe* occurring 20 times each.\n",
    "  - *jupiter* rarely occurs in the other documents. *universe* is common.\n",
    "  - Give more weight to *jupiter* on account of exclusivity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00b174",
   "metadata": {},
   "source": [
    "4\\. Applications\n",
    "----------------\n",
    "\n",
    "01:17 - 01:48\n",
    "\n",
    "Weighting words this way has a huge number of applications. They can be used to automatically detect stopwords for the corpus instead of relying on a generic list. They're used in search algorithms to determine the ranking of pages containing the search query and in recommender systems as we will soon find out. In a lot of cases, this kind of weighting also generates better performance during predictive modeling.\n",
    "\n",
    "- Automatically detect stopwords\n",
    "- Search\n",
    "- Recommender systems\n",
    "- Better performance in predictive modeling for some cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047ac50",
   "metadata": {},
   "source": [
    "5\\. Term frequency-inverse document frequency\n",
    "---------------------------------------------\n",
    "\n",
    "01:48 - 02:09\n",
    "\n",
    "The weighting mechanism we've described is known as term frequency-inverse document frequency or tf-idf for short. It is based on the idea that the weight of a term in a document should be proportional to its frequency and an inverse function of the number of documents in which it occurs.\n",
    "\n",
    "- Proportional to term frequency\n",
    "- Inverse function of the number of documents in which it occurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e68c4",
   "metadata": {},
   "source": [
    "6\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:09 - 02:16\n",
    "\n",
    "Mathematically, the weight of a term i in document j is computed as\n",
    "\n",
    "```markdown\n",
    "\\( w_{i,j} = tf_{i,j} \\cdot \\log \\left( \\frac{N}{df_i} \\right) \\)\n",
    "\n",
    "\\( w_{i,j} \\rightarrow \\) weight of term \\( i \\) in document \\( j \\)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293c9a0",
   "metadata": {},
   "source": [
    "7\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:16 - 02:20\n",
    "\n",
    "term frequency of the term i in document j\n",
    "\n",
    "```markdown\n",
    "$$\n",
    "w_{i,j} = tf_{i,j} \\cdot \\log\\left(\\frac{N}{df_i}\\right)\n",
    "$$\n",
    "\n",
    "- \\(w_{i,j}\\) → weight of term \\(i\\) in document \\(j\\)\n",
    "- \\(tf_{i,j}\\) → term frequency of term \\(i\\) in document \\(j\\)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe308f",
   "metadata": {},
   "source": [
    "8\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:20 - 02:32\n",
    "\n",
    "multiplied by the log of the ratio of the number of documents in the corpus and the number of documents in which the term i occurs or dfi.\n",
    "\n",
    "```markdown\n",
    "$$\n",
    "w_{i,j} = tf_{i,j} \\cdot \\log\\left(\\frac{N}{df_i}\\right)\n",
    "$$\n",
    "\n",
    "- \\(w_{i,j}\\) → weight of term \\(i\\) in document \\(j\\)\n",
    "- \\(tf_{i,j}\\) → term frequency of term \\(i\\) in document \\(j\\)\n",
    "- \\(N\\) → number of documents in the corpus\n",
    "- \\(df_i\\) → number of documents containing term \\(i\\)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869af3c7",
   "metadata": {},
   "source": [
    "9\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:32 - 03:18\n",
    "\n",
    "Therefore, let's say the word 'library' occurs in a document 5 times. There are 20 documents in the corpus and 'library' occurs in 8 of them. Then, the tf-idf weight of 'library' in the vector representation of this document will be 5 times log of 20 by 8 which is approximately 2. In general, higher the tf-idf weight, more important is the word in characterizing the document. A high tf-idf weight for a word in a document may imply that the word is relatively exclusive to that particular document or that the word occurs extremely commonly in the document, or both.\n",
    "\n",
    "```markdown\n",
    "$$\n",
    "w_{i,j} = tf_{i,j} \\cdot \\log\\left(\\frac{N}{df_i}\\right)\n",
    "$$\n",
    "\n",
    "- \\(w_{i,j}\\) → weight of term \\(i\\) in document \\(j\\)\n",
    "- \\(tf_{i,j}\\) → term frequency of term \\(i\\) in document \\(j\\)\n",
    "- \\(N\\) → number of documents in the corpus\n",
    "- \\(df_i\\) → number of documents containing term \\(i\\)\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "w_{library, document} = 5 \\cdot log\\left(\\frac{20}{8}\\right) \\approx 2\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57022628",
   "metadata": {},
   "source": [
    "10\\. tf-idf using scikit-learn\n",
    "------------------------------\n",
    "\n",
    "03:18 - 04:10\n",
    "\n",
    "Generating vectors that use tf-idf weighting is almost identical to what we've already done so far. Instead of using CountVectorizer, we use the TfidfVectorizer class of scikit-learn. The parameters and methods it has is almost identical to CountVectorizer. The only difference is that TfidfVectorizer assigns weights using the tf-idf formula from before and has extra parameters related to inverse document frequency which we will not cover in this course. Here, we can see how using TfidfVectorizer is almost identical to using CountVectorizer for a corpus. However, notice that the weights are non-integer and reflect values calculated by the tf-idf formula.\n",
    "\n",
    "```python\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "print(tfidf_matrix.toarray())\n",
    "```\n",
    "\n",
    "```python\n",
    "[[0.         0.         0.         0.25434658 0.33443519 0.33443519\n",
    "  0.25434658 0.         0.25434658 0.         0.76303975]\n",
    " [0.         0.46735098 0.         0.         0.46735098 0.\n",
    "  0.         0.46735098 0.35543247 0.         0.        ]\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7828250",
   "metadata": {},
   "source": [
    "11\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:10 - 04:14\n",
    "\n",
    "That's enough theory for now. Let's practice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
