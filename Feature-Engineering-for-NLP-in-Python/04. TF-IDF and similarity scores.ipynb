{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9464b220",
   "metadata": {},
   "source": [
    "1\\. Building tf-idf document vectors\n",
    "------------------------------------\n",
    "\n",
    "00:00 - 00:04\n",
    "\n",
    "In the last chapter, we learned about n-gram modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12695c4",
   "metadata": {},
   "source": [
    "2\\. n-gram modeling\n",
    "-------------------\n",
    "\n",
    "00:04 - 00:29\n",
    "\n",
    "In n-gram modeling, the weight of a dimension for the vector representation of a document is dependent on the number of times the word corresponding to the dimension occurs in the document. Let's say we have a document that has the word 'human' occurring 5 times. Then, the dimension of its vector representation corresponding to 'human' would have the value 5.\n",
    "\n",
    "- Weight of dimension dependent on the frequency of the word corresponding to the dimension.\n",
    "- Document contains the word *human* in five places.\n",
    "- Dimension corresponding to *human* has weight 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b9e7a",
   "metadata": {},
   "source": [
    "3\\. Motivation\n",
    "--------------\n",
    "\n",
    "00:29 - 01:17\n",
    "\n",
    "However, some words occur very commonly across all the documents in the corpus. As a result, the vector representations get more characterized by these dimensions. Consider a corpus of documents on the Universe. Let's say there is a particular document on Jupiter where the word 'jupiter' and 'universe' both occur about 20 times. However, 'jupiter' rarely figures in the other documents whereas 'universe' is just as common. We could argue that although both *jupiter* and *universe* occur 20 times, *jupiter* should be given a larger weight on account of its exclusivity. In other words, the word 'jupiter' characterizes the document more than 'universe'.\n",
    "\n",
    "- Some words occur very commonly across all documents\n",
    "- Corpus of documents on the universe:\n",
    "  - One document has *jupiter* and *universe* occurring 20 times each.\n",
    "  - *jupiter* rarely occurs in the other documents. *universe* is common.\n",
    "  - Give more weight to *jupiter* on account of exclusivity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00b174",
   "metadata": {},
   "source": [
    "4\\. Applications\n",
    "----------------\n",
    "\n",
    "01:17 - 01:48\n",
    "\n",
    "Weighting words this way has a huge number of applications. They can be used to automatically detect stopwords for the corpus instead of relying on a generic list. They're used in search algorithms to determine the ranking of pages containing the search query and in recommender systems as we will soon find out. In a lot of cases, this kind of weighting also generates better performance during predictive modeling.\n",
    "\n",
    "- Automatically detect stopwords\n",
    "- Search\n",
    "- Recommender systems\n",
    "- Better performance in predictive modeling for some cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047ac50",
   "metadata": {},
   "source": [
    "5\\. Term frequency-inverse document frequency\n",
    "---------------------------------------------\n",
    "\n",
    "01:48 - 02:09\n",
    "\n",
    "The weighting mechanism we've described is known as term frequency-inverse document frequency or tf-idf for short. It is based on the idea that the weight of a term in a document should be proportional to its frequency and an inverse function of the number of documents in which it occurs.\n",
    "\n",
    "- Proportional to term frequency\n",
    "- Inverse function of the number of documents in which it occurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e68c4",
   "metadata": {},
   "source": [
    "6\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:09 - 02:16\n",
    "\n",
    "Mathematically, the weight of a term i in document j is computed as\n",
    "\n",
    "```markdown\n",
    "\\( w_{i,j} = tf_{i,j} \\cdot \\log \\left( \\frac{N}{df_i} \\right) \\)\n",
    "\n",
    "\\( w_{i,j} \\rightarrow \\) weight of term \\( i \\) in document \\( j \\)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293c9a0",
   "metadata": {},
   "source": [
    "7\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:16 - 02:20\n",
    "\n",
    "term frequency of the term i in document j\n",
    "\n",
    "```markdown\n",
    "$$\n",
    "w_{i,j} = tf_{i,j} \\cdot \\log\\left(\\frac{N}{df_i}\\right)\n",
    "$$\n",
    "\n",
    "- \\(w_{i,j}\\) → weight of term \\(i\\) in document \\(j\\)\n",
    "- \\(tf_{i,j}\\) → term frequency of term \\(i\\) in document \\(j\\)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe308f",
   "metadata": {},
   "source": [
    "8\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:20 - 02:32\n",
    "\n",
    "multiplied by the log of the ratio of the number of documents in the corpus and the number of documents in which the term i occurs or dfi.\n",
    "\n",
    "```markdown\n",
    "$$\n",
    "w_{i,j} = tf_{i,j} \\cdot \\log\\left(\\frac{N}{df_i}\\right)\n",
    "$$\n",
    "\n",
    "- \\(w_{i,j}\\) → weight of term \\(i\\) in document \\(j\\)\n",
    "- \\(tf_{i,j}\\) → term frequency of term \\(i\\) in document \\(j\\)\n",
    "- \\(N\\) → number of documents in the corpus\n",
    "- \\(df_i\\) → number of documents containing term \\(i\\)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869af3c7",
   "metadata": {},
   "source": [
    "9\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "02:32 - 03:18\n",
    "\n",
    "Therefore, let's say the word 'library' occurs in a document 5 times. There are 20 documents in the corpus and 'library' occurs in 8 of them. Then, the tf-idf weight of 'library' in the vector representation of this document will be 5 times log of 20 by 8 which is approximately 2. In general, higher the tf-idf weight, more important is the word in characterizing the document. A high tf-idf weight for a word in a document may imply that the word is relatively exclusive to that particular document or that the word occurs extremely commonly in the document, or both.\n",
    "\n",
    "```markdown\n",
    "$$\n",
    "w_{i,j} = tf_{i,j} \\cdot \\log\\left(\\frac{N}{df_i}\\right)\n",
    "$$\n",
    "\n",
    "- \\(w_{i,j}\\) → weight of term \\(i\\) in document \\(j\\)\n",
    "- \\(tf_{i,j}\\) → term frequency of term \\(i\\) in document \\(j\\)\n",
    "- \\(N\\) → number of documents in the corpus\n",
    "- \\(df_i\\) → number of documents containing term \\(i\\)\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "w_{library, document} = 5 \\cdot log\\left(\\frac{20}{8}\\right) \\approx 2\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57022628",
   "metadata": {},
   "source": [
    "10\\. tf-idf using scikit-learn\n",
    "------------------------------\n",
    "\n",
    "03:18 - 04:10\n",
    "\n",
    "Generating vectors that use tf-idf weighting is almost identical to what we've already done so far. Instead of using CountVectorizer, we use the TfidfVectorizer class of scikit-learn. The parameters and methods it has is almost identical to CountVectorizer. The only difference is that TfidfVectorizer assigns weights using the tf-idf formula from before and has extra parameters related to inverse document frequency which we will not cover in this course. Here, we can see how using TfidfVectorizer is almost identical to using CountVectorizer for a corpus. However, notice that the weights are non-integer and reflect values calculated by the tf-idf formula.\n",
    "\n",
    "```python\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "print(tfidf_matrix.toarray())\n",
    "```\n",
    "\n",
    "```python\n",
    "[[0.         0.         0.         0.25434658 0.33443519 0.33443519\n",
    "  0.25434658 0.         0.25434658 0.         0.76303975]\n",
    " [0.         0.46735098 0.         0.         0.46735098 0.\n",
    "  0.         0.46735098 0.35543247 0.         0.        ]\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7828250",
   "metadata": {},
   "source": [
    "11\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:10 - 04:14\n",
    "\n",
    "That's enough theory for now. Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5f7a8",
   "metadata": {},
   "source": [
    "tf-idf weight of commonly occurring words\n",
    "=========================================\n",
    "\n",
    "The word `bottle` occurs 5 times in a particular document `D` and also occurs in every document of the corpus. What is the tf-idf weight of `bottle` in `D`?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[x] -   0\n",
    "\n",
    "    PRESS1\n",
    "\n",
    "-   1\n",
    "\n",
    "    PRESS2\n",
    "\n",
    "-   Not defined\n",
    "\n",
    "    PRESS3\n",
    "\n",
    "-   5\n",
    "\n",
    "    PRESS4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42931799",
   "metadata": {},
   "source": [
    "tf-idf vectors for TED talks\n",
    "============================\n",
    "\n",
    "In this exercise, you have been given a corpus `ted` which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks.\n",
    "\n",
    "In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import `TfidfVectorizer` from `sklearn`.\n",
    "-   Create a `TfidfVectorizer` object. Name it `vectorizer`.\n",
    "-   Generate `tfidf_matrix` for `ted` using the `fit_transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer= TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423fd766",
   "metadata": {},
   "source": [
    "1\\. Cosine similarity\n",
    "---------------------\n",
    "\n",
    "00:00 - 00:25\n",
    "\n",
    "We now know how to compute vectors out of text documents. With this representation in mind, let us now explore techniques that will allow us to determine how similar two vectors and consequentially two documents, are to each other. More specifically, we will learn about the cosine similarity score which is one of the most popularly used similarity metrics in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae1defa",
   "metadata": {},
   "source": [
    "2\\. Mathematical formula\n",
    "------------------------\n",
    "\n",
    "00:25 - 00:45\n",
    "\n",
    "Very simply put, the cosine similarity score of two vectors is the cosine of the angle between the vectors. Mathematically, it is the ratio of the dot product of the vectors and the product of the magnitude of the two vectors. Let's walk through what this formula really means.\n",
    "\n",
    "\n",
    "```markdown\n",
    "## Cosine Similarity\n",
    "\n",
    "\\[\n",
    "sim(A, B) = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\|\\|B\\|}\n",
    "\\]\n",
    "\n",
    "```\n",
    "```plaintext\n",
    "          y\n",
    "          ^\n",
    "          |\n",
    "      10 -|        🐼 (A)\n",
    "          |      /\n",
    "          |     /\n",
    "       5 -|    /θ\n",
    "          |   /\n",
    "          |  /_______ 🐯 (B)\n",
    "          | /          \n",
    "          |/____________________> x\n",
    "            5    10    15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b005b",
   "metadata": {},
   "source": [
    "3\\. The dot product\n",
    "-------------------\n",
    "\n",
    "00:45 - 01:21\n",
    "\n",
    "The dot product is computed by summing the product of values across corresponding dimensions of the vectors. Let's say we have two n-dimensional vectors V and W as shown. Then, the dot product here would be v1 times w1 plus v2 times w2 and so on until vn times wn. As an example, consider two vectors A and B. By applying the formula above, we see that the dot product comes to 37.\n",
    "\n",
    "```markdown\n",
    "Consider two vectors,\n",
    "\n",
    "\\[\n",
    "V = (v_1, v_2, \\dots, v_n), W = (w_1, w_2, \\dots, w_n)\n",
    "\\]\n",
    "\n",
    "Then the dot product of \\( V \\) and \\( W \\) is,\n",
    "\n",
    "\\[\n",
    "V \\cdot W = (v_1 \\times w_1) + (v_2 \\times w_2) + \\dots + (v_n \\times w_n)\n",
    "\\]\n",
    "\n",
    "**Example:**\n",
    "\n",
    "\\[\n",
    "A = (4, 7, 1), B = (5, 2, 3)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "A \\cdot B = (4 \\times 5) + (7 \\times 2) + (1 \\times 3)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 20 + 14 + 3 = 37\n",
    "\\]\n",
    "```\n",
    "```plaintext\n",
    "          y\n",
    "          ^\n",
    "          |\n",
    "      10 -|        A (4, 7, 1)\n",
    "          |       /\n",
    "          |      /\n",
    "       7 -|     /\n",
    "          |    /\n",
    "       5 -|   /\n",
    "          |  /\n",
    "       3 -| /  \n",
    "          |/____________________> x\n",
    "             5   7   1   3\n",
    "          B (5, 2, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00224085",
   "metadata": {},
   "source": [
    "4\\. Magnitude of a vector\n",
    "-------------------------\n",
    "\n",
    "01:21 - 01:57\n",
    "\n",
    "The magnitude of a vector is essentially the length of the vector. Mathematically, it is defined as the square root of the sum of the squares of values across all the dimensions of a vector. Therefore, for an n-dimensional vector V, the magnitude,mod V, is computed as the square root of v1 square plus v2 square and so on until vn square. Consider the vector A from before. Using the above formula, we compute its magnitude to be root 66.\n",
    "\n",
    "```markdown\n",
    "For any vector,\n",
    "\n",
    "\\[\n",
    "V = (v_1, v_2, \\dots, v_n)\n",
    "\\]\n",
    "\n",
    "The magnitude is defined as,\n",
    "\n",
    "\\[\n",
    "\\|V\\| = \\sqrt{(v_1)^2 + (v_2)^2 + \\dots + (v_n)^2}\n",
    "\\]\n",
    "\n",
    "**Example:**\n",
    "\n",
    "\\[\n",
    "A = (4, 7, 1), B = (5, 2, 3)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\|A\\| = \\sqrt{(4)^2 + (7)^2 + (1)^2}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= \\sqrt{16 + 49 + 1} = \\sqrt{66}\n",
    "\\]\n",
    "```\n",
    "```plaintext\n",
    "                    A (4, 7, 1)\n",
    "                       |\n",
    "                       |\\\n",
    "                       | \\\n",
    "                       |  \\   Magnitude of A = √66\n",
    "                       |   \\\n",
    "                       |    \\\n",
    "                       |     \\\n",
    "                       |______\\__________________________> B (5, 2, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40ebeb",
   "metadata": {},
   "source": [
    "5\\. The cosine score\n",
    "--------------------\n",
    "\n",
    "01:57 - 02:23\n",
    "\n",
    "We are now in a position to compute the cosine similarity score of A and B. It is the dot product, which is 37, divided by the product of the magnitudes of A and B, which are root 66 and root 38 respectively. The value comes out to be approximately 0.738, which is the value of the cosine of the angle theta between the two vectors.\n",
    "\n",
    "```markdown\n",
    "For vectors,\n",
    "\n",
    "\\[\n",
    "A : (4, 7, 1), B : (5, 2, 3)\n",
    "\\]\n",
    "\n",
    "The cosine score,\n",
    "\n",
    "\\[\n",
    "\\cos(A, B) = \\frac{A \\cdot B}{|A| \\cdot |B|}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= \\frac{37}{\\sqrt{66} \\times \\sqrt{38}} = 0.7388\n",
    "\\]\n",
    "```\n",
    "```plaintext\n",
    "          y\n",
    "          ^\n",
    "          |\n",
    "      10 -|         B (5, 2, 3)\n",
    "          |        /\n",
    "          |       /\n",
    "       7 -|      /θ\n",
    "          |     /\n",
    "          |    /\n",
    "       5 -|   /\n",
    "          |  /\n",
    "       3 -| /  \n",
    "          |/____________________> x\n",
    "             5   7   1   3\n",
    "          A (4, 7, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4d0e1c",
   "metadata": {},
   "source": [
    "6\\. Cosine Score: points to remember\n",
    "------------------------------------\n",
    "\n",
    "02:23 - 03:03\n",
    "\n",
    "Since the cosine score is simply the cosine of the angle between two vectors, its value is bounded between -1 and 1. However, in NLP, document vectors almost always use non-negative weights. Therefore, cosine scores vary between 0 and 1 where 0 indicates no similarity and 1 indicates that the documents are identical. Finally, since the cosine score ignores the magnitude of the vectors, it is fairly robust to document length. This may be an advantage or a disadvantage depending on the use case.\n",
    "\n",
    "- Value between -1 and 1.\n",
    "- In NLP, value between 0 and 1.\n",
    "- Robust to document length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b011b79",
   "metadata": {},
   "source": [
    "7\\. Implementation using scikit-learn\n",
    "-------------------------------------\n",
    "\n",
    "03:03 - 03:42\n",
    "\n",
    "Scikit-learn offers a cosine_similarity function that outputs a similarity matrix containing the pairwise cosine scores for a set of vectors. You can import cosine_similarity from sklearn dot metrics dot pairwise. However, remember that cosine_similarity takes in 2-D arrays as arguments. Passing in 1-D arrays will throw an error. Let us compute the cosine similarity scores of vectors A and B from before. We see that we get the same answer of 0.738 from before.\n",
    "\n",
    "```python\n",
    "# Import the cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define two 3-dimensional vectors A and B\n",
    "A = (4, 7, 1)\n",
    "B = (5, 2, 3)\n",
    "\n",
    "# Compute the cosine score of A and B\n",
    "score = cosine_similarity([A], [B])\n",
    "\n",
    "# Print the cosine score\n",
    "print(score)\n",
    "```\n",
    "```plaintext\n",
    "array([[0.73881883]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53314397",
   "metadata": {},
   "source": [
    "8\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:42 - 03:46\n",
    "\n",
    "That's enough theory for now. Let's practice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
