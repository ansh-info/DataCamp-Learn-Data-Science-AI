{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Named Entity Recognition\n",
    "----------------------------\n",
    "\n",
    "00:00 - 00:05\n",
    "\n",
    "Welcome to our first video on named entity recognition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. What is Named Entity Recognition?\n",
    "-------------------------------------\n",
    "\n",
    "00:05 - 00:40\n",
    "\n",
    "Named Entity Recognition or NER for short is a natural language processing task used to identify important named entities in the text -- such as people, places and organizations -- they can even be dates, states, works of art and other categories depending on the libraries and notation you use. NER can be used alongside topic identification, or on its own to determine important items in a text or answer basic natural language understanding questions such as who? what? when and where?\n",
    "\n",
    "- NLP task to identify important named entities in the text\n",
    "  - People, places, organizations\n",
    "  - Dates, states, works of art\n",
    "  - ... and other categories!\n",
    "- Can be used alongside topic identification\n",
    "  - ... or on its own!\n",
    "- Who? What? When? Where?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Example of NER\n",
    "------------------\n",
    "\n",
    "00:40 - 01:30\n",
    "\n",
    "For example, take this piece of text which is from the English Wikipedia article on Albert Einstein. The text has been highlighted for different types of named entities that were found using the Stanford NER library. You can see the dates, locations, persons and organizations found and extract infomation on the text based on these named entities. You can use NER to solve problems like fact extraction as well as which entities are related using computational language models. For example, in this text we can see that Einstein has something to do with the United States, Adolf Hitler and Germany. We can also see by token proximity that Betrand Russel and Einstein created the Russel-Einstein manifesto -- all from simple entity highlighting.\n",
    "\n",
    "In **1917**, **Einstein** applied the general theory of relativity to model the large-scale structure of the universe. He was visiting the **United States** when **Adolf Hitler** came to power in **1933** and did not go back to **Germany**, where he had been a professor at the **Berlin Academy of Sciences**. He settled in the **U.S.**, becoming an American citizen in **1940**. On the eve of World War II, he endorsed a letter to President **Franklin D. Roosevelt** alerting him to the potential development of \"extremely powerful bombs of a new type\" and recommending that the **U.S.** begin similar research. This eventually led to what would become the **Manhattan Project**. Einstein supported defending the Allied forces, but largely denounced using the new discovery of nuclear fission as a weapon. Later, with the British philosopher **Bertrand Russell**, **Einstein** signed the **Russell-Einstein Manifesto**, which highlighted the danger of nuclear weapons. Einstein was affiliated with the **Institute for Advanced Study** in **Princeton**, **New Jersey**, until his death in **1955**.\n",
    "\n",
    "Tag colours:\n",
    "- LOCATION\n",
    "- TIME \n",
    "- PERSON\n",
    "- ORGANIZATION\n",
    "- MONEY\n",
    "- PERCENT\n",
    "- DATE\n",
    "\n",
    "(Source: Europeana Newspapers (http://www.europeana-newspapers.eu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. nltk and the Stanford CoreNLP Library\n",
    "-----------------------------------------\n",
    "\n",
    "01:30 - 02:20\n",
    "\n",
    "NLTK allows you to interact with named entity recognition via it's own model, but also the aforementioned Stanford library. The Stanford library integration requires you to perform a few steps before you can use it, including installing the required Java files and setting system environment variables. You can also use the standford library on its own without integrating it with NLTK or operate it as an API server. The stanford CoreNLP library has great support for named entity recognition as well as some related nlp tasks such as coreference (or linking pronouns and entities together) and dependency trees to help with parsing meaning and relationships amongst words or phrases in a sentence.\n",
    "\n",
    "- The Stanford CoreNLP library:\n",
    "  - Integrated into Python via `nltk`\n",
    "  - Java based\n",
    "  - Support for NER as well as coreference and dependency trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Using nltk for Named Entity Recognition\n",
    "-------------------------------------------\n",
    "\n",
    "02:20 - 02:49\n",
    "\n",
    "For our simple use case, we will use the built-in named entity recognition with NLTK. To do so, we take a normal sentence, and preprocess it via tokenization. Then, we can tag the sentence for parts of speech. This will add tags for proper nouns, pronouns, adjective, verbs and other part of speech that NLTK uses based on an english grammar. When we take a look at the tags, we see New and York are tagged NNP which is the tag for a proper noun, singular.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "sentence = '''In New York, I like to ride the Metro to \n",
    "             visit MOMA and some restaurants rated \n",
    "             well by Ruth Reichl.'''\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent) \n",
    "tagged_sent[:3]\n",
    "\n",
    "[('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. nltk's ne_chunk()\n",
    "---------------------\n",
    "\n",
    "02:49 - 03:31\n",
    "\n",
    "Then we pass this tagged sentence into the ne_chunk function, or named entity chunk, which will return the sentence as a tree. NLTK Tree's might look a bit different than trees you might use in other libraries, but they do have leaves and subtrees representing more complex grammar. This tree shows the named entities tagged as their own chunks such as GPE or geopolitical entity for New York, or MOMA and Metro as organizations. It also identifies Ruth Reichl as a person. It does so without consulting a knowledge base, like wikipedia, but instead uses trained statistical and grammatical parsers.\n",
    "\n",
    "```python\n",
    "print(nltk.ne_chunk(tagged_sent))\n",
    "\n",
    "(S\n",
    "  In/IN\n",
    "  (GPE New/NNP York/NNP)\n",
    "  ,/,\n",
    "  I/PRP\n",
    "  like/VBP\n",
    "  to/TO\n",
    "  ride/VB\n",
    "  the/DT\n",
    "  (ORGANIZATION Metro/NNP)\n",
    "  to/TO\n",
    "  visit/VB\n",
    "  (ORGANIZATION MOMA/NNP)\n",
    "  and/CC\n",
    "  some/DT\n",
    "  restaurants/NNS\n",
    "  rated/VBN\n",
    "  well/RB\n",
    "  by/IN\n",
    "  (PERSON Ruth/NNP Reichl/NNP)\n",
    "  ./.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:31 - 03:37\n",
    "\n",
    "Now it's your turn to practice some named entity recognition using nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER with NLTK\n",
    "=============\n",
    "\n",
    "You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use `nltk` to find the named entities in this article. \n",
    "\n",
    "What might the article be about, given the names you found?\n",
    "\n",
    "Along with `nltk`, `sent_tokenize` and `word_tokenize` from `nltk.tokenize` have been pre-imported.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Tokenize `article` into sentences.\n",
    "-   Tokenize each sentence in `sentences` into words using a list comprehension.\n",
    "-   Inside a list comprehension, tag each tokenized sentence into parts of speech using `nltk.pos_tag()`.\n",
    "-   Chunk each tagged sentence into named-entity chunks using `nltk.ne_chunk_sents()`. Along with `pos_sentences`, specify the additional keyword argument `binary=True`.\n",
    "-   Loop over each sentence and each chunk, and test whether it is a named-entity chunk by testing if it has the attribute `label`, and if the `chunk.label()` is equal to `\"NE\"`. If so, print that chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charting practice\n",
    "=================\n",
    "\n",
    "In this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n",
    "\n",
    "You'll use a `defaultdict` called `ner_categories`, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called `chunked_sentences` similar to the last exercise, but this time with non-binary category names.\n",
    "\n",
    "You can use `hasattr()` to determine if each chunk has a `'label'` and then simply use the chunk's `.label()` method as the dictionary key.\n",
    "\n",
    "Instructions 1/3\n",
    "----------------\n",
    "\n",
    "-   Create a `defaultdict` called `ner_categories`, with the default type set to `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/3\n",
    "----------------\n",
    "\n",
    "-   Fill up the dictionary with values for each of the keys. Remember, the keys will represent the `label()`.\n",
    "    -   In the outer `for` loop, iterate over `chunked_sentences`, using `sent` as your iterator variable.\n",
    "    -   In the inner `for` loop, iterate over `sent`. If the condition is true, increment the value of each key by 1. \n",
    "    -   *Remember to use the chunk's `.label()`method as the key!*\n",
    "-   For the pie chart labels, create a list called `labels` from the keys of `ner_categories`, which can be accessed using `.keys()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "----------------\n",
    "\n",
    "-   Use a list comprehension to create a list called `values`, using the `.get()` method on `ner_categories` to compute the values of each label `v`.\n",
    "-   Use `plt.pie()` to create a pie chart for each of the NER categories. Along with `values` and `labels=labels`, pass the extra keyword arguments `autopct='%1.1f%%'` and `startangle=140`to add percentages to the chart and rotate the initial start angle. \n",
    "    -   *This step has been done for you.*\n",
    "-   Display your pie chart. Was the distribution what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford library with NLTK\n",
    "==========================\n",
    "\n",
    "When using the Stanford library with NLTK, what is needed to get started?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   A normal installation of NLTK.\n",
    "\n",
    "-   An installation of the Stanford Java Library.\n",
    "\n",
    "-   Both NLTK and an installation of the Stanford Java Library.\n",
    "\n",
    "-   NLTK, the Stanford Java Libraries and some environment variables to help with integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Introduction to SpaCy\n",
    "-------------------------\n",
    "\n",
    "00:00 - 00:07\n",
    "\n",
    "In this video, we'll take a look at SpaCy, another great library for natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. What is SpaCy?\n",
    "------------------\n",
    "\n",
    "00:07 - 00:31\n",
    "\n",
    "SpaCy is a NLP library similar to Gensim, but with different implementations, including a particular focus on creating NLP pipelines to generate models and corpora. SpaCy is open-source and has several extra libraries and tools built by the same team, including Displacy - a visualization tool for viewing parse trees which uses Node-js to create interactive text.\n",
    "\n",
    "- NLP library similar to `gensim`, with different implementations\n",
    "- Focus on creating NLP pipelines to generate models and corpora  \n",
    "- Open-source, with extra libraries and tools\n",
    "  - Displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Displacy entity recognition visualizer\n",
    "------------------------------------------\n",
    "\n",
    "00:31 - 00:55\n",
    "\n",
    "For example, if we use the displacy entity recognition visualizer which has a live demo online, we can enter the sentence used in the last video. Here, we can see the SpaCy has identified three named entities and tagged them with the appropriate entity label -- such as location or person. SpaCy also has tools to build word and document vectors from text.\n",
    "\n",
    "In `New York`<sub>GPE</sub>, I like to ride the Metro to visit `MOMA`<sub>ORG</sub> and some restaurants rated well by `Ruth Reichl`<sub>PERSON</sub>\n",
    "\n",
    "(source: https://demos.explosion.ai/displacy-ent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. SpaCy NER\n",
    "-------------\n",
    "\n",
    "00:55 - 02:19\n",
    "\n",
    "To start using spacy for Named entity recognition, we must first install it and download all the appropriate pre-trained word vectors. You can also train vectors yourself and load them; but the pretrained ones let us get started immediately. We can load those into an object, NLP, which functions similarly to our Gensim dictionary and corpus. It has several linked objects, including entity which is an Entity Recognizer object from the pipeline module. This is what is used to find entities in the text. Then we load a new document by passing a string into the NLP variable. When the document is loaded, the named entities are stored as a document attribute called ents. We see Spacy properly tagged and identified the three main entities in the sentence. We can also investigate the labels of each entity by using indexing to pick out the first entity and the label_ attribute to see the label for that particular entity. Here we see the label for Berlin is GPE or Geopolitical entity. Spacy has several other language models available, including advanced German and Chinese implementations. It's a great tool especially if you want to build your own extraction and natural language processing pipeline quickly and iteratively.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.entity\n",
    "\n",
    "<spacy.pipeline.EntityRecognizer at 0x7f76b75e68b8>\n",
    "\n",
    "doc = nlp(\"\"\"Berlin is the capital of Germany;\n",
    "            and the residence of Chancellor Angela Merkel.\"\"\")\n",
    "doc.ents\n",
    "\n",
    "(Berlin, Germany, Angela Merkel)\n",
    "\n",
    "print(doc.ents[0], doc.ents[0].label_)\n",
    "\n",
    "Berlin GPE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Why use SpaCy for NER?\n",
    "--------------------------\n",
    "\n",
    "02:19 - 02:49\n",
    "\n",
    "Why use Spacy for NER? Outside of being able to integrate with the other great Spacy features like easy pipeline creation, it has a different set of entity types and often labels entities differently than nltk. In addition, Spacy comes with informal language corpora, allowing you to more easily find entities in documents like Tweets and chat messages. It's a quickly growing library, so it might even have more languages supported by the time you are watching this video!\n",
    "\n",
    "- Easy pipeline creation\n",
    "- Different entity types compared to `nltk`\n",
    "- Informal language corpora\n",
    "  - Easily find entities in Tweets and chat messages \n",
    "- Quickly growing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "02:49 - 02:55\n",
    "\n",
    "For now, however, you can get started using Spacy for named entity recognition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing NLTK with spaCy NER\n",
    "=============================\n",
    "\n",
    "Using the same text you used in the first exercise of this chapter, you'll now see the results using spaCy's NER annotator. How will they compare?\n",
    "\n",
    "The article has been pre-loaded as `article`. To minimize execution times, you'll be asked to specify the keyword argument `disable=['tagger', 'parser', 'matcher']`when loading the spaCy model, because you only care about the `entity` in this exercise.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import `spacy`.\n",
    "-   Load the `'en_core_web_sm'` model using `spacy.load()`. Specify the additional keyword arguments `disable=['tagger', 'parser', 'matcher']`.\n",
    "-   Create a `spacy` document object by passing `article` into `nlp()`.\n",
    "-   Using `ent` as your iterator variable, iterate over the entities of `doc` and print out the labels (`ent.label_`) and text (`ent.text`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'matcher'])\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy NER Categories\n",
    "====================\n",
    "\n",
    "Which are the *extra* categories that `spacy` uses compared to `nltk` in its named-entity recognition?\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "GPE, PERSON, MONEY\n",
    "\n",
    "ORGANIZATION, WORK*OF*ART\n",
    "\n",
    "[/] NORP, CARDINAL, MONEY, WORK*OF*ART, LANGUAGE, EVENT\n",
    "\n",
    "EVENT_LOCATION, FIGURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Multilingual NER with polyglot\n",
    "----------------------------------\n",
    "\n",
    "00:00 - 00:08\n",
    "\n",
    "In this video, we'll review multilingual named entity recognition using a new library Polyglot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. What is polyglot?\n",
    "---------------------\n",
    "\n",
    "00:08 - 01:04\n",
    "\n",
    "Polyglot is yet another natural language processing library which uses word vectors to perform simple tasks such as entity recognition. You might be wondering: why do I need to learn another library which uses word vectors? Don't I already have Gensim and Spacy? And you would be correct. The main benefit and difference of using Polyglot, however, is the wide variety of languages it supports. Polyglot has word embeddings for more than 130 languages! For this reason, you can even use it for tasks like transliteration, as shown here translating some english text into arabic. Transliteration is the ability to translate text by swapping characters from one language to another. Of course, any user of Google translate or its competitors has seen issues in translation created by word vectors, but Polyglot is a pretty neat open-source tool to have for so many languages.\n",
    "\n",
    "- NLP library which uses word vectors\n",
    "- Why `polyglot`?\n",
    "  - Vectors for many different languages\n",
    "  - More than 130!\n",
    "\n",
    "which     وکٹه بند یا\n",
    "India     بیٹ\n",
    "beat      بیرمودا\n",
    "Bermuda   ین\n",
    "in        پورٹ\n",
    "Port      وف\n",
    "of        سپا ین\n",
    "Spain     ین\n",
    "in        2007\n",
    "2007      ویکه\n",
    "which     واس  \n",
    "was       یکا لیند\n",
    "equalled  فیفی\n",
    "five      دا یس\n",
    "days      اغو\n",
    "ago       بی\n",
    "by        سووت\n",
    "South     ا فریکا\n",
    "Africa    ین\n",
    "in        ٹہیر\n",
    "their     فکٹوری\n",
    "victory   وفیر\n",
    "over      ویسٹ\n",
    "West      اند یس\n",
    "Indies    ین\n",
    "in        سید نی\n",
    "Sydney    .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Spanish NER with polyglot\n",
    "-----------------------------\n",
    "\n",
    "01:04 - 02:29\n",
    "\n",
    "Instead of transliteration, we are going to use Polyglot to perform named entity recognition for some new languages. Similar to SpaCy, you need to have the proper vectors downloaded and installed before you begin. Once you do, Polyglot does not need to be told which language you are using. It uses the language detection model to do so when the Text object is initialized by passing in the document string. Here is a recent headline from the newspapers in Madrid about the promotion of Madrid by another Spanish politician. If you know Spanish, (or even if you don't and you take a look at the capitalized words), you can see quite a few titles, locations and people. When we call the entities attribute of the text object, we can see a list of entity chunks found by Polyglot while parsing the text. Each chunk has a label, represented by the symbols starting with I-, such as I-ORG representing an organization, I-LOC representing a location and I-PER representing a person. You may notice some possible duplication in the first two entities found, separating Generalitat de and Catalunya. This makes some sense because the phrase represents both a location Catalunya and a organization the Generalitat. That said, you may need to clean up returned entities when they don't match your expected labels or have substrings you would rather not track.\n",
    "\n",
    "```python\n",
    "from polyglot.text import Text\n",
    "\n",
    "text = \"\"\"El presidente de la Generalitat de Cataluña, \n",
    "        Carles Puigdemont, ha afirmado hoy a la alcaldesa \n",
    "        de Madrid, Manuela Carmena, que en su etapa de\n",
    "        alcalde de Girona (de julio de 2011 a enero de 2016)\n",
    "        hizo una gran promoción de Madrid.\"\"\"\n",
    "\n",
    "ptext = Text(text)\n",
    "ptext.entities\n",
    "\n",
    "[I-ORG(['Generalitat', 'de']),\n",
    " I-LOC(['Generalitat', 'de', 'Cataluña']), \n",
    " I-PER(['Carles', 'Puigdemont']),\n",
    " I-LOC(['Madrid']),\n",
    " I-PER(['Manuela', 'Carmena']),\n",
    " I-LOC(['Girona']),\n",
    " I-LOC(['Madrid'])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "02:29 - 02:34\n",
    "\n",
    "Now it's your turn to use NER with Polyglot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French NER with polyglot I\n",
    "==========================\n",
    "\n",
    "In this exercise and the next, you'll use the `polyglot` library to identify French entities. The library functions slightly differently than `spacy`, so you'll use a few of the new things you learned in the last video to display the named entity text and category.\n",
    "\n",
    "You have access to the full article string in `article`. Additionally, the `Text` class of `polyglot` has been imported from `polyglot.text`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Using the article string in `article`, create a new `Text` object called `txt`.\n",
    "-   Iterate over `txt.entities` and print each entity, `ent`.\n",
    "-   Print the `type()` of `ent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of each entity\n",
    "print(type(ent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French NER with polyglot II\n",
    "===========================\n",
    "\n",
    "Here, you'll complete the work you began in the previous exercise.\n",
    "\n",
    "Your task is to use a list comprehension to create a list of tuples, in which the first element is the entity tag, and the second element is the full string of the entity text.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Use a list comprehension to create a list of tuples called `entities`. \n",
    "-   The output expression of your list comprehension should be a tuple.\n",
    "    -   The first element of each tuple is the entity tag, which you can access using its `.tag` attribute.\n",
    "    -   The second element is the full string of the entity text, which you can access using `.join(ent)`.\n",
    "-   Your iterator variable should be `ent`, and you should iterate over all of the entities of the `polyglot` `Text` object, `txt`.\n",
    "-   Print `entities` to see what you've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print the entities\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spanish NER with polyglot\n",
    "=========================\n",
    "\n",
    "You'll continue your exploration of `polyglot`now with some Spanish annotation. This article is not written by a newspaper, so it is your first example of a more blog-like text. How do you think that might compare when finding entities?\n",
    "\n",
    "The `Text` object has been created as `txt`, and each entity has been printed, as you can see in the IPython Shell. \n",
    "\n",
    "Your specific task is to determine how many of the entities contain the words `\"Márquez\"` or `\"Gabo\"` - these refer to the same person in different ways!\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Iterate over all of the entities of `txt`, using `ent` as your iterator variable.\n",
    "-   Check whether the entity contains `\"Márquez\"` or `\"Gabo\"`. If it does, increment `count`. *Don't forget to include the accented `á` in `\"Márquez\"`!*\n",
    "-   Hit 'Submit Answer' to see what percentage of entities refer to Gabriel García Márquez (aka Gabo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities:\n",
    "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
    "    if \"Márquez\" in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "        count += 1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count * 1.0 / len(txt.entities)\n",
    "print(percentage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
