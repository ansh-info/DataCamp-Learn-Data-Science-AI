{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Word counts with bag-of-words\n",
    "---------------------------------\n",
    "\n",
    "00:00 - 00:08\n",
    "\n",
    "Welcome to chapter two! We'll begin with using word counts with a bag of words approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Bag-of-words\n",
    "----------------\n",
    "\n",
    "00:08 - 00:35\n",
    "\n",
    "Bag of words is a very simple and basic method to finding topics in a text. For bag of words, you need to first create tokens using tokenization, and then count up all the tokens you have. The theory is that the more frequent a word or token is, the more central or important it might be to the text. Bag of words can be a great way to determine the significant words in a text based on the number of times they are used.\n",
    "\n",
    "* Basic method for finding topics in a text\n",
    "* Need to first create tokens using tokenization\n",
    "* ... and then count up all the tokens\n",
    "* The more frequent a word, the more important it might be\n",
    "* Can be a great way to determine the significant words in a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Bag-of-words example\n",
    "------------------------\n",
    "\n",
    "00:35 - 01:17\n",
    "\n",
    "Here we see an example series of sentences, mainly about a cat and a box. If we just us a simple bag of words model with tokenization like we learned in chapter one and remove the punctuation, we can see the example result. Box, cat, The and the are some of the most important words because they are the most frequent. Notice that the word THE appears twice in the bag of words, once with uppercase and once lowercase. If we added a preprocessing step to handle this issue, we could lowercase all of the words in the text so each word is counted only once.\n",
    "\n",
    "* Text: \"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
    "* Bag of words (stripped punctuation):\n",
    "  * \"The\": 3, \"box\": 3\n",
    "  * \"cat\": 3\n",
    "  * \"is\": 2\n",
    "  * \"in\": 1, \"likes\": 1, \"over\": 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Bag-of-words in Python\n",
    "--------------------------\n",
    "\n",
    "01:17 - 02:18\n",
    "\n",
    "We can use the NLP fundamentals we already know, such as tokenization with NLTK to create a list of tokens. We will use a new class called Counter which we import from the standard library module collections. The list of tokens generated using word_tokenize can be passed as the initialization argument for the Counter class. The result is a counter object which has similar structure to a dictionary and allows us to see each token and the frequency of the token. Counter objects also have a method called `most_common`, which takes an integer argument, such as 2 and would then return the top 2 tokens in terms of frequency. The return object is a series of tuples inside a list. For each tuple, the first element holds the token and the second element represents the frequency. Note: other than ordering by token frequency, the most_common method does not sort the tokens it returns or tell us there are more tokens with that same frequency.\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box.\n",
    "The box is over the cat.\"\"\"))\n",
    "\n",
    "Counter({'.': 3,\n",
    "        'The': 3, \n",
    "        'cat': 3,\n",
    "        'in': 1,\n",
    "        ...\n",
    "        'the': 3})\n",
    "\n",
    "counter.most_common(2)\n",
    "\n",
    "[('The', 3), ('box', 3)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "02:18 - 02:25\n",
    "\n",
    "Now you know a bit about bag of words and can get started building your own using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words picker\n",
    "===================\n",
    "\n",
    "It's time for a quick check on your understanding of bag-of-words. Which of the below options, with basic `nltk` tokenization, map the bag-of-words for the following text?\n",
    "\n",
    "\"The cat is in the box. The cat box.\"\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "50 XP\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "('the', 3), ('box.', 2), ('cat', 2), ('is', 1)\n",
    "\n",
    "('The', 3), ('box', 2), ('cat', 2), ('is', 1), ('in', 1), ('.', 1)\n",
    "\n",
    "('the', 3), ('cat box', 1), ('cat', 1), ('box', 1), ('is', 1), ('in', 1)\n",
    "\n",
    "[/] ('The', 2), ('box', 2), ('.', 2), ('cat', 2), ('is', 1), ('in', 1), ('the', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Counter with bag-of-words\n",
    "====================================\n",
    "\n",
    "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as `article`. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as `article_title`. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.\n",
    "\n",
    "`word_tokenize` has been imported for you.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "100 XP\n",
    "\n",
    "-   Import `Counter` from `collections`.\n",
    "-   Use `word_tokenize()` to split the article into tokens.\n",
    "-   Use a list comprehension with `t` as the iterator variable to convert all the tokens into lowercase. The `.lower()` method converts text into lowercase.\n",
    "-   Create a bag-of-words counter called `bow_simple` by using `Counter()` with `lower_tokens` as the argument.\n",
    "-   Use the `.most_common()` method of `bow_simple` to print the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Simple text preprocessing\n",
    "-----------------------------\n",
    "\n",
    "00:00 - 00:06\n",
    "\n",
    "In this video, we will cover some simple text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Why preprocess?\n",
    "-------------------\n",
    "\n",
    "00:06 - 01:06\n",
    "\n",
    "Text processing helps make for better input data when performing machine learning or other statistical methods. For example, in the last few exercises you have applied small bits of preprocessing (like tokenization) to create a bag of words. You also noticed that applying simple techniques like lowercasing all of the tokens, can lead to slightly better results for a bag-of-words model. Preprocessing steps like tokenization or lowercasing words are commonly used in NLP. Other common techniques are things like lemmatization or stemming, where you shorten the words to their root stems, or techniques like removing stop words, which are common words in a language that don't carry a lot of meaning -- such as and or the, or removing punctuation or unwanted tokens. Of course, each model and process will have different results -- so it's good to try a few different approaches to preprocessing and see which works best for your task and goal.\n",
    "\n",
    "* Helps make for better input data\n",
    "  * When performing machine learning or other statistical methods\n",
    "\n",
    "* Examples:\n",
    "  * Tokenization to create a bag of words\n",
    "  * Lowercasing words\n",
    "\n",
    "* Lemmatization/Stemming\n",
    "  * Shorten words to their root stems\n",
    "\n",
    "* Removing stop words, punctuation, or unwanted tokens\n",
    "\n",
    "* Good to experiment with different approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Preprocessing example\n",
    "-------------------------\n",
    "\n",
    "01:06 - 01:31\n",
    "\n",
    "We have here some example input and output text we might expect from preprocessing. First we have a simple two sentence string about pets. Then we have some example output tokens we want. You can see that the text has been tokenized and that everything is lowercase. We also notice that stopwords have been removed and the plural nouns have been made singular.\n",
    "\n",
    "* Input text: Cats, dogs and birds are common pets. So are fish.\n",
    "* Output tokens: cat, dog, bird, common, pet, fish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Text preprocessing with Python\n",
    "----------------------------------\n",
    "\n",
    "01:31 - 02:53\n",
    "\n",
    "We can perform text preprocessing using many of the tools we already know and have learned. In this code, we are using the same text as from our previous video, a few sentences about a cat with a box. We can use list comprehensions to tokenize the sentences which we first make lowercase using the string lower method. The string is_alpha method will return True if the string has only alphabetical characters. We use the is_alpha method along with an if statement iterating over our tokenized result to only return only alphabetic strings (this will effectively strip tokens with numbers or punctuation). To read out the process in both code and English we say we take each token from the word_tokenize output of the lowercase text if it contains only alphabetical characters. In the next line, we use another list comprehension to remove words that are in the stopwords list. This stopwords list for english comes built in with the NLTK library. Finally, we can create a counter and check the two most common words, which are now cat and box (unlike the and box which were the two tokens returned in our first result). Preprocessing has already improved our bag of words and made it more useful by removing the stopwords and non-alphabetic words.\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "text = \"\"\"The cat is in the box. The cat likes the box.\n",
    "The box is over the cat.\"\"\"\n",
    "tokens = [w for w in word_tokenize(text.lower())\n",
    "         if w.isalpha()]\n",
    "no_stops = [t for t in tokens\n",
    "           if t not in stopwords.words('english')]\n",
    "Counter(no_stops).most_common(2)\n",
    "\n",
    "[('cat', 3), ('box', 3)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "02:53 - 02:59\n",
    "\n",
    "You can now get started by preprocessing your own text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing steps\n",
    "========================\n",
    "\n",
    "Which of the following are useful text preprocessing steps?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "50XP\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   Stems, spelling corrections, lowercase.\n",
    "\n",
    "-   Lemmatization, lowercasing, removing unwanted tokens.\n",
    "\n",
    "-   Removing stop words, leaving in capital words.\n",
    "\n",
    "-   Strip stop words, word endings and digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing practice\n",
    "===========================\n",
    "\n",
    "Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n",
    "\n",
    "You start with the same tokens you created in the last exercise: `lower_tokens`. You also have the `Counter` class imported.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "100 XP\n",
    "\n",
    "-   Import the `WordNetLemmatizer` class from `nltk.stem`. \n",
    "-   Create a list `alpha_only` that contains **only** alphabetical characters. You can use the `.isalpha()` method to check for this.\n",
    "-   Create another list called `no_stops` consisting of words from `alpha_only` that **are not** contained in `english_stops`.\n",
    "-   Initialize a `WordNetLemmatizer` object called `wordnet_lemmatizer` and use its `.lemmatize()`method on the tokens in `no_stops` to create a new list called `lemmatized`.\n",
    "-   Create a new `Counter` called `bow` with the lemmatized words.\n",
    "-   Lastly, print the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Introduction to gensim\n",
    "--------------------------\n",
    "\n",
    "00:00 - 00:07\n",
    "\n",
    "In this video, we will get started using a new tool called Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. What is gensim?\n",
    "-------------------\n",
    "\n",
    "00:07 - 00:25\n",
    "\n",
    "**Gensim** is a popular open-source natural language processing library. It uses top academic models to perform complex tasks like building document or word vectors, corpora and performing topic identification and document comparisons.\n",
    "\n",
    "* Popular open source NLP library\n",
    "\n",
    "* Uses top academic models to perform complex tasks\n",
    "  * Building document or word vectors\n",
    "  * Performing topic identification and document comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. What is a word vector?\n",
    "--------------------------\n",
    "\n",
    "00:25 - 01:19\n",
    "\n",
    "You might be wondering what a word or document vector is? Here are some examples here in visual form. A word embedding or vector is trained from a larger corpus and is a multi-dimensional representation of a word or document. You can think of it as a multi-dimensional array normally with sparse features (lots of zeros and some ones). With these vectors, we can then see relationships among the words or documents based on how near or far they are and also what similar comparisons we find. For example, in this graphic we can see that the vector operation king minus queen is approximately equal to man minus woman. Or that Spain is to Madrid as Italy is to Rome. The deep learning algorithm used to create word vectors has been able to distill this meaning based on how those words are used throughout the text.\n",
    "\n",
    "```text\n",
    "Male-Female                Verb tense               Country-Capital\n",
    "\n",
    "man    ------>  woman     walked                   Spain ------> Madrid\n",
    "king   ------>  queen     walking    swam          Italy ------> Rome\n",
    "                          swimming                  Germany ----> Berlin\n",
    "                                                   Turkey ------> Ankara\n",
    "                                                   Russia ------> Moscow\n",
    "                                                   Canada ------> Ottawa \n",
    "                                                   Japan -------> Tokyo\n",
    "                                                   Vietnam ----> Hanoi\n",
    "                                                   China -------> Beijing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Gensim example\n",
    "------------------\n",
    "\n",
    "01:19 - 01:46\n",
    "\n",
    "The graphic we have here is an example of LDA visualization. LDA stands for latent dirichlet allocation, and it is a statistical model we can apply to text using Gensim for topic analysis and modelling. This graph is just a portion of a blog post written in 2015 using Gensim to analyze US presidential addresses. The article is really neat and you can find the link here.\n",
    "\n",
    "```text\n",
    "Top-30 Most Relevant Terms for Topic 6 (6.2% of tokens)\n",
    "\n",
    "soviet           ■■■■ \n",
    "nuclear          ■■■\n",
    "world            ■■■■■■■■■■■■■■■■■■■■■\n",
    "nations          ■■■■■■■■■■■\n",
    "berlin           ■■\n",
    "weapons          ■■■\n",
    "missiles         ■\n",
    "arms             ■■■\n",
    "soviets          ■\n",
    "communist        ■\n",
    "strategic        ■\n",
    "europe           ■■■■\n",
    "peace            ■■■■■■■■■■■■■\n",
    "\n",
    "0               2,000             4,000            6,000            8,000           10,000\n",
    "\n",
    "(Source: http://tlfvincent.github.io/2015/10/23/presidential-speech-topics)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Creating a gensim dictionary\n",
    "--------------------------------\n",
    "\n",
    "01:46 - 02:51\n",
    "\n",
    "Gensim allows you to build corpora and dictionaries using simple classes and functions. A corpus (or if plural, corpora) is a set of texts used to help perform natural language processing tasks. Here, our documents are a list of strings that look like movie reviews about space or sci-fi films. First we need to do some basic preprocessing. For brevity, we will only tokenize and lowercase. For better results, we would want to apply more of the preprocessing we have learned in this chapter, such as removing punctuation and stop words. Then we can pass the tokenized documents to the Gensim Dictionary class. This will create a mapping with an id for each token. This is the beginning of our corpus. We now can represent whole documents using just a list of their token ids and how often those tokens appear in each document. We can take a look at the tokens and their ids by looking at the token2id attribute, which is a dictionary of all of our tokens and their respective ids in our new dictionary.\n",
    "\n",
    "```python\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "                'I really liked the movie!',\n",
    "                'Awesome action scenes, but boring characters.',\n",
    "                'The movie was awful! I hate alien films.',\n",
    "                'Space is cool! I liked the movie.',\n",
    "                'More space films, please!']\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc.lower()) \n",
    "                 for doc in my_documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary.token2id\n",
    "\n",
    "{'!': 11,\n",
    " ',': 17,\n",
    " '.': 7,\n",
    " 'a': 2,\n",
    " 'about': 4,\n",
    " ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Creating a gensim corpus\n",
    "----------------------------\n",
    "\n",
    "02:51 - 03:59\n",
    "\n",
    "Using the dictionary we built in the last slide, we can then create a Gensim corpus. This is a bit different than a normal corpus -- which is just a collection of documents. Gensim uses a simple bag-of-words model which transforms each document into a bag of words using the token ids and the frequency of each token in the document. Here, we can see that the Gensim corpus is a list of lists, each list item representing one document. Each document a series of tuples, the first item representing the tokenid from the dictionary and the second item representing the token frequency in the document. In only a few lines, we have a new bag-of-words model and corpus thanks to Gensim. And unlike our previous Counter-based bag of words, this Gensim model can be easily saved, updated and reused thanks to the extra tools we have available in Gensim. Our dictionary can also be updated with new texts and extract only words that meet particular thresholds. We are building a more advanced and feature-rich bag-of-words model which can then be used for future exercises.\n",
    "\n",
    "```python\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "corpus\n",
    "\n",
    "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
    " [(0, 1), (1, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
    " ...]\n",
    "\n",
    "* gensim models can be easily saved, updated, and reused\n",
    "* Our dictionary can also be updated\n",
    "* This more advanced and feature rich bag-of-words can be used in future exercises\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:59 - 04:04\n",
    "\n",
    "Now you can get started building your own dictionary with Gensim!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are word vectors?\n",
    "======================\n",
    "\n",
    "What are word vectors and how do they help with NLP?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   They are similar to bags of words, just with numbers. You use them to count how many tokens there are.\n",
    "\n",
    "-   Word vectors are sparse arrays representing bigrams in the corpora. You can use them to compare two sets of words to one another.\n",
    "\n",
    "-   Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus.\n",
    "\n",
    "-   Word vectors don't actually help NLP and are just hype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and querying a corpus with gensim\n",
    "==========================================\n",
    "\n",
    "It's time to apply the methods you learned in the previous video to create your first `gensim` dictionary and corpus! \n",
    "\n",
    "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called `articles`. You'll need to do some light preprocessing and then generate the `gensim` dictionary and corpus.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import `Dictionary` from `gensim.corpora.dictionary`.\n",
    "-   Initialize a `gensim` `Dictionary` with the tokens in `articles`.\n",
    "-   Obtain the id for `\"computer\"` from `dictionary`. To do this, use its `.token2id` method which returns ids from text, and then chain `.get()` which returns tokens from ids. Pass in `\"computer\"` as an argument to `.get()`.\n",
    "-   Use a list comprehension in which you iterate over `articles` to create a `gensim` `MmCorpus` from `dictionary`.\n",
    "    -   In the output expression, use the `.doc2bow()` method on `dictionary` with `article` as the argument.\n",
    "-   Print the first 10 word ids with their frequency counts from the fifth document. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim bag-of-words\n",
    "===================\n",
    "\n",
    "Now, you'll use your new `gensim` corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell! \n",
    "\n",
    "You have access to the `dictionary` and `corpus` objects you created in the previous exercise, as well as the Python `defaultdict`and `itertools` to help with the creation of intermediate data structures for analysis. \n",
    "\n",
    "-   `defaultdict` allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument `int`, we are able to ensure that any non-existent keys are automatically assigned a default value of `0`. This makes it ideal for storing the counts of words in this exercise.\n",
    "\n",
    "-   `itertools.chain.from_iterable()` allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our `corpus` object (which is a list of lists).\n",
    "\n",
    "The fifth document from `corpus` is stored in the variable `doc`, which has been sorted in descending order.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Using the first `for` loop, print the top five words of `bow_doc` using each `word_id`with the `dictionary` alongside `word_count`. \n",
    "\n",
    "    -   The `word_id` can be accessed using the `.get()` method of `dictionary`.\n",
    "-   Create a `defaultdict` called `total_word_count` in which the keys are all the token ids (`word_id`) and the values are the sum of their occurrence across all documents (`word_count`). \n",
    "\n",
    "    -   Remember to specify `int` when creating the `defaultdict`, and inside the second `for` loop, increment each `word_id` of `total_word_count` by `word_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count \n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Create a sorted list from the `defaultdict`, using words across the entire corpus. To achieve this, use the `.items()` method on `total_word_count` inside `sorted()`.\n",
    "-   Similar to how you printed the top five words of `bow_doc` earlier, print the top five words of `sorted_word_count` as well as the number of occurrences of each word across all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Assuming dictionary and corpus are already created as in the previous exercise\n",
    "\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Tf-idf with gensim\n",
    "----------------------\n",
    "\n",
    "00:00 - 00:06\n",
    "\n",
    "In this video, we will learn how to use a TFIDF model with Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. What is tf-idf?\n",
    "-------------------\n",
    "\n",
    "00:06 - 01:00\n",
    "\n",
    "**Tf-idf** stands for term-frequncy - inverse document frequency. It is a commonly used natural language processing model that helps you determine the most important words in each document in the corpus. The idea behind tf-idf is that each corpus might have more shared words than just stopwords. These common words are like stopwords and should be removed or at least down-weighted in importance. For example, if I am an astronomer, sky might be used often but is not important, so I want to downweight that word. TF-Idf does precisely that. It will take texts that share common language and ensure the most common words across the entire corpus don't show up as keywords. Tf-idf helps keep the document-specific frequent words weighted high and the common words across the entire corpus weighted low.\n",
    "\n",
    "* Term frequency - inverse document frequency\n",
    "\n",
    "* Allows you to determine the most important words in each document\n",
    "\n",
    "* Each corpus may have shared words beyond just stopwords \n",
    "\n",
    "* These words should be down-weighted in importance\n",
    "\n",
    "* Example from astronomy: \"Sky\"\n",
    "\n",
    "* Ensures most common words don't show up as key words\n",
    "\n",
    "* Keeps document specific frequent words weighted high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Tf-idf formula\n",
    "------------------\n",
    "\n",
    "01:00 - 02:05\n",
    "\n",
    "The equation to calculate the weights can be outlined like so: The weight of token i in document j is calculated by taking the term frequency (or how many times the token appears in the document) multiplied by the log of the total number of documents divided by the number of documents that contain the same term. Let's unpack this a bit. First, the weight will be low if the term doesnt appear often in the document because the tf variable will then be low. However, the weight will also be a low if the logarithm is close to zero, meaning the internal equation is low. Here we can see if the total number of documents divded by the number of documents that have the term is close to one, then our logarithm will be close to zero. So words that occur across many or all documents will have a very low tf-idf weight. On the contrary, if the word only occurs in a few documents, that logarithm will return a higher number.\n",
    "\n",
    "```text\n",
    "w(i,j) = tf(i,j) * log(N/df(i))\n",
    "\n",
    "w(i,j) = tf-idf weight for token i in document j\n",
    "tf(i,j) = number of occurences of token i in document j\n",
    "df(i) = number of documents that contain token i\n",
    "N = total number of documents\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Tf-idf with gensim\n",
    "----------------------\n",
    "\n",
    "02:05 - 02:56\n",
    "\n",
    "You can build a Tfidf model using Gensim and the corpus you developed previously. Taking a look at the corpus we used in the last video, around movie reviews, we can use the Bag of Words corpus to translate it into a TF-idf model by simply passing it in initialization. We can then reference each document by using it like a dictionary key with our new tfidf model. For the second document in our corpora, we see the token weights along with the token ids. Notice there are some large differences! Token id 10 has a weight of 0.77 whereas tokens 0 and 1 have weights below 0.18. These weights can help you determine good topics and keywords for a corpus with shared vocabulary.\n",
    "\n",
    "```python\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[1]]\n",
    "\n",
    "[(0, 0.17462982767351674),\n",
    " (1, 0.17462982767351674),\n",
    " (9, 0.29853166221463673),\n",
    " (10, 0.77169315210279081),\n",
    " ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "02:56 - 03:02\n",
    "\n",
    "Now you can build a tfidf model using Gensim to explore topics in the Wikipedia article list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is tf-idf?\n",
    "===============\n",
    "\n",
    "You want to calculate the tf-idf weight for the word `\"computer\"`, which appears five times in a document containing 100 words. Given a corpus containing 200 documents, with 20 documents mentioning the word `\"computer\"`, tf-idf can be calculated by multiplying term frequency with inverse document frequency.\n",
    "\n",
    "Term frequency = percentage share of the word compared to all tokens in the document Inverse document frequency = logarithm of the total number of documents in a corpora divided by the number of documents containing the term\n",
    "\n",
    "Which of the below options is correct?\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "50 XP\n",
    "\n",
    "### Possible answers\n",
    "\n",
    "[/] (5 / 100) * log(200 / 20)\n",
    "\n",
    "(5 * 100) / log(200 * 20)\n",
    "\n",
    "(20 / 5) * log(200 / 20)\n",
    "\n",
    "(200 * 5) * log(400 / 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf with Wikipedia\n",
    "=====================\n",
    "\n",
    "Now it's your turn to determine new significant terms for your corpus by applying `gensim`'s tf-idf. You will again have access to the same corpus and dictionary objects you created in the previous exercises - `dictionary`, `corpus`, and `doc`. Will tf-idf make for more interesting results on the document level?\n",
    "\n",
    "`TfidfModel` has been imported for you from `gensim.models.tfidfmodel`.\n",
    "\n",
    "Instructions 1/2\n",
    "----------------\n",
    "\n",
    "-   Initialize a new `TfidfModel` called `tfidf`using `corpus`.\n",
    "-   Use `doc` to calculate the weights. You can do this by passing `[doc]` to `tfidf`.\n",
    "-   Print the first five term ids with weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 2/2\n",
    "----------------\n",
    "\n",
    "-   Sort the term ids and weights in a new list from highest to lowest weight. *This has been done for you.*\n",
    "-   Using your pre-existing `dictionary`, print the top five weighted words (`term_id`) from `sorted_tfidf_weights`, along with their weighted score (`weight`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Assuming dictionary and corpus are already created as in the previous exercises\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
