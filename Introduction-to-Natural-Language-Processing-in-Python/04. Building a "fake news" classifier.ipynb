{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Classifying fake news using supervised learning with NLP\n",
    "------------------------------------------------------------\n",
    "\n",
    "00:00Â -Â 00:13\n",
    "\n",
    "In this video, we'll be learning about supervised machine learning with NLP. Throughout this chapter you will be using the skills and ideas applied to classifying fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. What is supervised learning?\n",
    "--------------------------------\n",
    "\n",
    "00:13Â -Â 00:59\n",
    "\n",
    "Supervised learning is a form of machine learning where you are given or create training data. This data has a label or outcome which you want the model or algorithm to learn. One common problem used as a good example of introductory machine learning is the Fischer's iris data; we have a few example rows of it here. The data has several features: Sepal Length and width and Petal length and width. The label we want to learn and predict is the species. This is a classification problem, so you want to be able to classify or categorize some data based on what you already know or have learned. Our goal is to use the dataset to make intelligent hypotheses about the species based on the geometric features.\n",
    "\n",
    "* Form of machine learning\n",
    "  * Problem has predefined training data\n",
    "  * This data has a label (or outcome) you want the model to learn\n",
    "  * Classification problem  \n",
    "  * Goal: Make good hypotheses about the species based on geometric features\n",
    "\n",
    "| Sepal length | Sepal width | Petal length | Petal width | Species |\n",
    "|--------------|-------------|--------------|-------------|----------|\n",
    "| 5.1 | 3.5 | 1.4 | 0.2 | I. setosa |\n",
    "| 7.0 | 3.2 | 4.77 | 1.4 | I.versicolor |  \n",
    "| 6.3 | 3.3 | 6.0 | 2.5 | I.virginica |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Supervised learning with NLP\n",
    "--------------------------------\n",
    "\n",
    "00:59Â -Â 01:20\n",
    "\n",
    "But instead of using geometric features like the Iris dataset, we need to use language. To help create features and train a model, we will use Scikit learn, a powerful open-source library. One of the ways you can create supervised learning data from text is by using bag of words models or TFIDF as features.\n",
    "\n",
    "* Need to use language instead of geometric features\n",
    "* `scikit-learn`: Powerful open-source library \n",
    "* How to create supervised learning data from text?\n",
    "  * Use bag-of-words models or tf-idf as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. IMDB Movie Dataset\n",
    "----------------------\n",
    "\n",
    "01:20Â -Â 02:09\n",
    "\n",
    "Let's say I have a dataset full of movie plots and genres from the IMDB database, as shown in this chart. I've separated the action and sci-fi movies, removing any movies labeled both action and scifi. I want to predict whether a movie is action or sci-fi based on the plot summary. The dataset we've extracted has categorical features generated using some preprocessing. We can see the plot summary, and the sci-fi and action columns. You can also see the Sci-Fi column, which is 1 for movies that are scifi and 0 for movies that are action. The Action column is the inverse of the Sci-Fi column.\n",
    "\n",
    "| Plot | Sci-Fi | Action |\n",
    "|------|--------|---------|\n",
    "| In a post-apocalyptic world in human decay, a ... | 1 | 0 |\n",
    "| Mohei is a wandering swordsman. He arrives in ... | 0 | 1 |\n",
    "| #137 is a SCI/FI thriller about a girl, Marla,... | 1 | 0 |\n",
    "\n",
    "* Goal: Predict movie genre based on plot summary\n",
    "* Categorical features generated using preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Supervised learning steps\n",
    "-----------------------------\n",
    "\n",
    "02:09Â -Â 03:08\n",
    "\n",
    "In the next video, we'll use scikit-learn to predict a movie's genre from its plot. But first, let's review the supervised learning process as a whole. To begin, we collect and preprocess our data. Then, we determine a label - this is what we want the model to learn, in our case, the genre of the movie. We can split our data into training and testing datasets, keeping them separate so we can build our model using only the training data. The test data remains unseen so we can test how well our model performs after it is trained. This is an essential part of Supervised Learning! We also need to extract features from the text to predict the label. We will use a bagof words vectorizer built into scikit-learn to do so. After the model is trained, we can then test it using the test dataset. There are also other methods to evaluate model performance, such as k-fold cross validation and you can check out DataCamp's Machine Learning curriculum to learn that and more!\n",
    "\n",
    "* Collect and preprocess our data\n",
    "* Determine a label (Example: Movie genre)\n",
    "* Split data into training and test sets \n",
    "* Extract features from the text to help predict the label\n",
    "  * Bag-of-words vector built into `scikit-learn`\n",
    "* Evaluate trained model using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:08Â -Â 03:16\n",
    "\n",
    "Let's review some of the supervised learning steps, like splitting testing and training data before applying it to our movie plot data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which possible features?\n",
    "========================\n",
    "\n",
    "Which of the following are possible features for a text classification problem?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "50XP\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   Number of words in a document.\n",
    "\n",
    "-   Specific named entities.\n",
    "\n",
    "-   Language.\n",
    "\n",
    "-   All of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing\n",
    "====================\n",
    "\n",
    "What datasets are needed for supervised learning?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "50XP\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   Training data.\n",
    "\n",
    "-   Testing data.\n",
    "\n",
    "-   Both training and testing data.\n",
    "\n",
    "-   A label or outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Building word count vectors with scikit-learn\n",
    "-------------------------------------------------\n",
    "\n",
    "00:00Â -Â 00:07\n",
    "\n",
    "In this video, we'll build our first scikit learn vectors from the movie plot and genre dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Predicting movie genre\n",
    "--------------------------\n",
    "\n",
    "00:07Â -Â 00:22\n",
    "\n",
    "We have a dataset full of movie plots and what genre the movie is -- either action or sci-fi. We want to create bag of words vectors for these movie plots to see if we can predict the genre based on the words used in the plot summary.\n",
    "\n",
    "* Dataset consisting of movie plots and corresponding genre\n",
    "* Goal: Create bag-of-word vectors for the movie plots\n",
    "  * Can we predict genre based on the words used in the plot summary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Count Vectorizer with Python\n",
    "--------------------------------\n",
    "\n",
    "00:22Â -Â 03:28\n",
    "\n",
    "To do so, we first need to import some necessary tools from Sci-kit learn. Once the data is loaded, we can create y which traditionally refers to the labels or outcome you want the model to learn. We can use the Sci-Fi column which has 1 if the movie is Sci-Fi and 0 if it is Action. Then, scikit learn's train_test_split function can be used to split the dataframe into training and testing data. This method will split the features which is the plot summary or column PLOT and the labels (y) based on a given test_size such as 0.33, representing 33 percent. I have also set random state so we have a repeatable result, it operates similar to setting a random seed and ensures I get the same results when I run the code again. The function will take 33% of rows to be marked as test data, and remove them from the training data. The test data is later used to see what my model has learned. The resulting data from train_test_split are training data (as X_train) and training labels (as y_train) and testing data as X_test and testing labels as y_test. Next, we create a countvectorizer which turns my text into bag of words vectors similar to a Gensim corpus, it will also remove English stop words from the movie plot summaries as a preprocessing step. Each token now acts as a feature for the machine learning classification problem, just like the flower measurements in the iris data set. We can then call fit_transform on the training data to create the bag-of-words vectors. Fit_transform is a handy shortcut which will call the model's fit and then transform methods; which here generates a mapping of words with IDs and vectors representing how many times each word appears in the plot. Fit_transform operates differently for each model, but generally fit will find parameters or norms in the data and transform will apply the model's underlying algorithm or approximation -- similar to preprocessing but with a specific use case in mind. For the CountVectorizer class, fit_transform will create the bagofwords dictionary and vectors for each document using the training data. After calling fit_transform on the training data, we call transform on the test data to create bag of words vectors using the same dictionary. The training and test vectors need to use a consistent set of words, so the trained model can understand the test input. If we don't have much data, there can be an issue with words in the test set which don't appear in the training data. This will throw an error, so you will need to either add more training data or remove the unknown words from the test dataset. In only a few lines of Python, we have transformed text into bagofwords vectors and generated test and training datasets. Scikitlearn is a great aid in helping make NLP machine learning simple and accessible.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = ... # Load data into DataFrame\n",
    "y = df['Sci-Fi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    df['plot'], y,\n",
    "                    test_size=0.33,\n",
    "                    random_state=53)\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "count_test = count_vectorizer.transform(X_test.values) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "03:28Â -Â 03:34\n",
    "\n",
    "Now it's your turn to create some scikitlearn vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer for text classification\n",
    "=======================================\n",
    "\n",
    "It's time to begin building your text classifier! TheÂ [data](https://s3.amazonaws.com/assets.datacamp.com/production/course_3629/fake_or_real_news.csv)Â has been loaded into a DataFrame calledÂ `df`. Explore it in the IPython Shell to investigate what columns you can use. TheÂ `.head()`Â method is particularly informative.\n",
    "\n",
    "In this exercise, you'll useÂ `pandas`Â alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you'll set up aÂ `CountVectorizer`Â and investigate some of its features.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   ImportÂ `CountVectorizer`Â fromÂ `sklearn.feature_extraction.text`Â andÂ `train_test_split`Â fromÂ `sklearn.model_selection`.\n",
    "-   Create a SeriesÂ `y`Â to use for the labels by assigning theÂ `.label`Â attribute ofÂ `df`Â toÂ `y`.\n",
    "-   UsingÂ `df[\"text\"]`Â (features) andÂ `y`(labels), create training and test sets usingÂ `train_test_split()`. Use aÂ `test_size`Â ofÂ `0.33`Â and aÂ `random_state`Â ofÂ `53`.\n",
    "-   Create aÂ `CountVectorizer`Â object calledÂ `count_vectorizer`. Ensure you specify the keyword argumentÂ `stop_words=\"english\"`so that stop words are removed.\n",
    "-   Fit and transform the training dataÂ `X_train`Â using theÂ `.fit_transform()`method of yourÂ `CountVectorizer`Â object. Do the same with the test dataÂ `X_test`, except using theÂ `.transform()`Â method.\n",
    "-   Print the first 10 features of theÂ `count_vectorizer`Â using itsÂ `.get_feature_names()`Â method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer for text classification\n",
    "=======================================\n",
    "\n",
    "Similar to the sparseÂ `CountVectorizer`created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up aÂ `TfidfVectorizer`Â and investigate some of its features.\n",
    "\n",
    "In this exercise, you'll useÂ `pandas`Â andÂ `sklearn`Â along with the sameÂ `X_train`,Â `y_train`Â andÂ `X_test`,Â `y_test`Â DataFrames and Series you created in the last exercise.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   ImportÂ `TfidfVectorizer`Â fromÂ `sklearn.feature_extraction.text`.\n",
    "-   Create aÂ `TfidfVectorizer`Â object calledÂ `tfidf_vectorizer`. When doing so, specify the keyword argumentsÂ `stop_words=\"english\"`Â andÂ `max_df=0.7`.\n",
    "-   Fit and transform the training data.Â \n",
    "-   Transform the test data.\n",
    "-   Print the first 10 features ofÂ `tfidf_vectorizer`.\n",
    "-   Print the first 5 vectors of the tfidf training data using slicing on theÂ `.A`Â (or array)Â ***attribute***Â ofÂ `tfidf_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the vectors\n",
    "======================\n",
    "\n",
    "To get a better idea of how the vectors work, you'll investigate them by converting them intoÂ `pandas`Â DataFrames.\n",
    "\n",
    "Here, you'll use the same data structures you created in the previous two exercises (`count_train`,Â `count_vectorizer`,Â `tfidf_train`,Â `tfidf_vectorizer`) as well asÂ `pandas`, which is imported asÂ `pd`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create the DataFramesÂ `count_df`Â andÂ `tfidf_df`Â by usingÂ `pd.DataFrame()`Â and specifying the values as the first argument and the columns (or features) as the second argument.\n",
    "    -   The values can be accessed by using theÂ `.A`Â attribute of, respectively,Â `count_train`Â andÂ `tfidf_train`.\n",
    "    -   The columns can be accessed using theÂ `.get_feature_names()`Â methods ofÂ `count_vectorizer`Â andÂ `tfidf_vectorizer`.\n",
    "-   Print the head of each DataFrame to investigate their structure.Â *This has been done for you.*\n",
    "-   Test if the column names are the same for each DataFrame by creating a new object calledÂ `difference`Â to see the difference between the columns thatÂ `count_df`Â has fromÂ `tfidf_df`. Columns can be accessed using theÂ `.columns`Â attribute of a DataFrame. Subtract the set ofÂ `tfidf_df.columns`Â from the set ofÂ `count_df.columns`.\n",
    "-   Test if the two DataFrames are equivalent by using theÂ `.equals()`Â method onÂ `count_df`Â withÂ `tfidf_df`Â as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Training and testing a classification model with scikit-learn\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "00:00Â -Â 00:07\n",
    "\n",
    "In this video, we'll use the features we have extracted to train and test a supervised classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Naive Bayes classifier\n",
    "--------------------------\n",
    "\n",
    "00:07Â -Â 01:12\n",
    "\n",
    "A Naive Bayes model is commonly used for testing NLP classification problems because of its basis in probability. Naive bayes algorithm uses probability, attempting to answer the question if given a particular piece of data, how likely is a particular outcome? For example, thinking back to our movie genres dataset -- If the plot has a spaceship, how likely is it that the movie is Sci-Fi? And given a Spaceship and an alien how likely NOW is it a sci-fi movie? Each word acts as a feature from our CountVectorizer helping classify our text using probability. Naive bayes has been used for text classification problems since the 1960s and continues to be used today despite the growth of many other models, algorithms and neural network architectures. That said, it is not always the best tool for the job, but it is a simple and effective one you will use to build a fake news classifier.\n",
    "\n",
    "* Naive Bayes Model\n",
    "  * Commonly used for testing NLP classification problems\n",
    "  * Basis in probability\n",
    "* Given a particular piece of data, how likely is a particular outcome?\n",
    "* Examples:\n",
    "  * If the plot has a spaceship, how likely is it to be sci-fi?\n",
    "  * Given a spaceship and an alien, how likely now is it sci-fi?\n",
    "* Each word from `CountVectorizer` acts as a feature\n",
    "* Naive Bayes: Simple and effective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Naive Bayes with scikit-learn\n",
    "---------------------------------\n",
    "\n",
    "01:12Â -Â 03:04\n",
    "\n",
    "We'll use scikit learn's naive bayes to take a look at our scifi versus action plot classification problem. Recall the data we're using is simply IMDB plot summaries, and whether the movie is science fiction or action. First, we import the naive bayes model class, multinomial naive bayes, which works well with count vectorizers as it expects integer inputs. MultinomialNB is also used for multiple label classification. This model may not work as well with floats, such as tfidf weighted inputs. Instead, use support vector machines or even linear models; although I recommend trying Naive Bayes first to determine if it can also work well. We use the metrics module to evaluate model performance. We initialize our class and call fit with the training data. If you recall from the previous video, this will determine the internal parameters based on the dataset. We pass the training count vectorizer first and the training labels second. After fitting the model, we call predict with the count vectorizer test data. Predict will use the trained model to predict the label based on the test data vectors. We save the predicted labels in variable pred to test the accuracy. Finally, we test accuracy using accuracy_score from the metrics module and passing the predicted and test labels. Accuracy for our model means the percentage of correct genre guesses out of total guesses. Our model has about 86% accuracy -- which is pretty good for a first try! You'll be applying the Multinomial Naive Bayes classifier to the fake news dataset in the following exercises.\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "0.8584184938982042\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Confusion matrix\n",
    "--------------------\n",
    "\n",
    "03:04Â -Â 04:16\n",
    "\n",
    "To further evaluate our model, we can also check the confusion matrix which shows correct and incorrect labels. The confusion_matrix function from the metrics module takes the test labels, the predictions and a list of labels. If the label list is not passed, scikit learn will order them using Python ordering. The confusion matrix is a bit easier to read when we transform it into a table. The first value and last value of the matrix (or the main diagonal of the matrix) show true scores, meaning, true classification of both action and scifi films based on the plot bag of words vectors. In a confusion matrix, the predicted labels are shown across the top and the true labels are shown down the side. This confusion matrix shows 864 Sci-Fi movies incorrectly labeled as Action and 563 Action movies incorrectly labeled as Sci-Fi. We can see from the distribution of true positives and negatives that our dataset is a bit skewed, we have many more action films than sci-fi. This could be one reason that our action movies are predicted more accurately.\n",
    "\n",
    "```python\n",
    "metrics.confusion_matrix(y_test, pred, labels=[0,1])\n",
    "\n",
    "array([[6410,  563],\n",
    "       [ 864, 2242]])\n",
    "\n",
    "# Table format:\n",
    "#           Action  Sci-Fi\n",
    "# Action    6410    563\n",
    "# Sci-Fi    864     2242\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "04:16Â -Â 04:22\n",
    "\n",
    "Now it's your turn to train and test a naive bayes model for the fake news problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification models\n",
    "==========================\n",
    "\n",
    "Which of the below is the most reasonable model to use when training a new supervised model using text vector data?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "50XP\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   Random Forests\n",
    "\n",
    "-   Naive Bayes\n",
    "\n",
    "-   Linear Regression\n",
    "\n",
    "-   Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing the \"fake news\" model with CountVectorizer\n",
    "===============================================================\n",
    "\n",
    "Now it's your turn to train the \"fake news\" model using the features you identified and extracted. In this first exercise you'll train and test a Naive Bayes model using theÂ `CountVectorizer`Â data.\n",
    "\n",
    "The training and test sets have been created, andÂ `count_vectorizer`,Â `count_train`, andÂ `count_test`Â have been computed.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Import theÂ `metrics`Â module fromÂ `sklearn`andÂ `MultinomialNB`Â fromÂ `sklearn.naive_bayes`.\n",
    "-   Instantiate aÂ `MultinomialNB`Â classifier calledÂ `nb_classifier`.\n",
    "-   Fit the classifier to the training data.\n",
    "-   Compute the predicted tags for the test data.\n",
    "-   Calculate and print the accuracy score of the classifier.\n",
    "-   Compute the confusion matrix. To make it easier to read, specify the keyword argumentÂ `labels=['FAKE', 'REAL']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing the \"fake news\" model with TfidfVectorizer\n",
    "===============================================================\n",
    "\n",
    "Now that you have evaluated the model using theÂ `CountVectorizer`, you'll do the same using theÂ `TfidfVectorizer`Â with a Naive Bayes model.\n",
    "\n",
    "The training and test sets have been created, andÂ `tfidf_vectorizer`,Â `tfidf_train`, andÂ `tfidf_test`Â have been computed. Additionally,Â `MultinomialNB`Â andÂ `metrics`have been imported from, respectively,Â `sklearn.naive_bayes`Â andÂ `sklearn`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Instantiate aÂ `MultinomialNB`Â classifier calledÂ `nb_classifier`.\n",
    "-   Fit the classifier to the training data.\n",
    "-   Compute the predicted tags for the test data.\n",
    "-   Calculate and print the accuracy score of the classifier.\n",
    "-   Compute the confusion matrix. As in the previous exercise, specify the keyword argumentÂ `labels=['FAKE', 'REAL']`Â so that the resulting confusion matrix is easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Simple NLP, complex problems\n",
    "--------------------------------\n",
    "\n",
    "00:00Â -Â 00:27\n",
    "\n",
    "You've learned so much about Natural Language Processing fundamentals in this course, congratulations! In this video we'll talk more about how complex these problems can be and how to use the skills you have learned to start a longer exploration of working with language in Python. In the exercises, you will apply some extra investigation into your fake news classification model to see if it has really learned what you wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Translation\n",
    "---------------\n",
    "\n",
    "00:27Â -Â 01:12\n",
    "\n",
    "Translation, although it might work well for some languages, still has a long way to go. This tweet by Lupin attempting to translate some legal or bureaucratic text about economics and industry is a pretty funny and also sadly accurate example of when using word vectors between two languages can end poorly. The German text uses words like NationalÃ¶konomie and Wirtschaftswissenschaft but the English text simply says the economics of economics (including economics, economics and so forth) is part of economics. The German text has many different words related to economics and they are all simply closest to the english vector for economics, leading to a hilarious but woefully inaccurate translation.\n",
    "\n",
    "> **Lupin** @Lupintweets Â· Follow\n",
    "> \n",
    "> god bless the german language\n",
    "> \n",
    "> [*Translation interface showing:*]\n",
    ">\n",
    "> **German (detected):**  \n",
    "> Die Volkswirtschaftslehre (auch NationalÃ¶konomie, Wirtschaftliche Staatswissenschaften oder SozialÃ¶konomie, kurz VWL), ist ein Teilgebiet der Wirtschaftswissenschaft\n",
    ">\n",
    "> **English translation:**  \n",
    "> The economics of economics (including economics, economics, economics, economics, economics, economics) is a part of economics\n",
    ">\n",
    "> ğŸ”„ 9,595 Retweets   â¤ï¸ 16,327 Likes\n",
    "\n",
    "[Source](https://twitter.com/Lupintweets/status/865533182455685121)\n",
    "\n",
    "The image shows a humorous translation example highlighting how German compounds words in unique ways. In the image, a German text about economics (\"Volkswirtschaftslehre\") is translated to English, where \"economics\" gets repeated multiple times, making it comically redundant: \"The economics of economics (including economics, economics, economics, economics, economics, economics) is a part of economics.\"\n",
    "\n",
    "The tweet caption \"god bless the german language\" humorously acknowledges German's tendency to create precise, often lengthy compound words that can translate awkwardly into English. The tweet received significant engagement with 9,595 retweets and 16,327 likes.\n",
    "\n",
    "The humor comes from how the German language can express complex concepts in single compound words, while the English translation becomes a repetitive string of the same word \"economics\" to try to capture the same meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Sentiment analysis\n",
    "----------------------\n",
    "\n",
    "01:12Â -Â 02:04\n",
    "\n",
    "Sentiment analysis is far from a solved problem. Complex issues like snark or sarcasm, and difficult problems with negation (for example: I liked it BUT it could have been better) make it an open field of research. There is also active research regarding how separate communities use the same words differently. This is a graphic from a project called Social Sent created by a group of researchers at Stanford. The project compares sentiment changes in words over time and from different communities. Here, the authors compare sentiment in word usage between two different reddit communities, 2X which is a woman-centered reddit and sports. The graphic illustrates the SAME word can be used with very different sentiments depending on the communal understanding of the word.\n",
    "\n",
    "```markdown\n",
    "\"big men are very _soft_\"\n",
    "\"freakin raging _animal_\"\n",
    "\"went from the _ladies_ tees\"\n",
    "\"two _dogs_ fighting\"\n",
    "\"being able to _hit_\"\n",
    "\"insanely _difficult_ saves\"\n",
    "\"amazing _shot_\"\n",
    "\"he is still _crazy_ good\"\n",
    "\"his stats are _insane_\"\n",
    "\n",
    "Ex. contexts in r/sports\n",
    "\n",
    "[Bar chart showing sentiment scores ranging from -10 to +10]\n",
    "\n",
    "\"some _soft_ pajamas\"\n",
    "\"stuffed _animal_\"\n",
    "\"lovely _ladies_\"\n",
    "\"hiking with the _dogs_\"\n",
    "\"it didn't really _hit_ me\"\n",
    "\"a _difficult_ time\"\n",
    "\"totally _shot_ me down\"\n",
    "\"overreacting _crazy_ woman\"\n",
    "\"people are just _insane_\"\n",
    "\n",
    "Ex. contexts in r/TwoX\n",
    "\n",
    "(source: https://nlp.stanford.edu/projects/socialsent/)\n",
    "```\n",
    "\n",
    "```markdown\n",
    "| \"big men are very _soft_\"             |               soft â– â– â– â– â– â– â– â– â– â”€         |     \"some _soft_ pajamas\"           |\n",
    "| \"freakin raging _animal_\"             |             animal â– â– â– â– â– â– â– â– â”€â”€         |     \"stuffed _animal_\"              |\n",
    "| \"went from the _ladies_ tees\"         |             ladies â– â– â– â– â– â”€â”€â”€           |     \"lovely _ladies_\"               |\n",
    "| \"two _dogs_ fighting\"                 |               dogs â– â– â– â– â”€â”€â”€â”€           |     \"hiking with the _dogs_\"        |\n",
    "| \"being able to _hit_\"                 |                hit â– â”€â”€â”€â”€â”€             |     \"it didn't really _hit_ me\"     |\n",
    "| \"insanely _difficult_ saves\"          |           difficult â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              |     \"a _difficult_ time\"            |\n",
    "| \"amazing _shot_\"                      |               shot â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             |     \"totally _shot_ me down\"        |\n",
    "| \"his stats are _insane_\"             |             insane â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           |     \"people are just _insane_\"      |\n",
    "| \"he is still _crazy_ good\"           |              crazy â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            |     \"overreacting _crazy_ woman\"    |\n",
    "\n",
    "**Ex. contexts in r/sports**                                                    **Ex. contexts in r/TwoX**\n",
    "\n",
    "                               â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’\n",
    "                               -10              0               +10\n",
    "                     more positive in r/sports,     more positive in r/TwoX,\n",
    "                     more negative in r/TwoX        more negative in r/sports\n",
    "\n",
    "_(source: https://nlp.stanford.edu/projects/socialsent/)_\n",
    "```\n",
    "\n",
    "Note: I used â–  for red/brown bars (positive sentiment) and â–ˆ for blue bars (negative sentiment) to approximate the visualization. The actual bars are colored differently in the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Language biases\n",
    "-------------------\n",
    "\n",
    "02:04Â -Â 02:46\n",
    "\n",
    "Finally, we must remember language can contain its own prejudices and unfair treatment towards groups. When we then train word vectors on these prejudiced texts, our word vectors will likely reflect those problems. Here, we take a gendered language, like english and translate it to Turkish, a language with no gendered pronouns. When we click to translate it back, we see the genders have switched. This phenomena was studied in a recent article by a Princeton researcher Aylin Caliskan alongside several ethical machine learning researchers. She also gave a talk at the 33rd annual Chaos Computer Club conference in Hamburg, which I can definitely recommend viewing.\n",
    "\n",
    "```markdown\n",
    "## Google Ãœbersetzer\n",
    "[Englisch] [RumÃ¤nisch] [TÃ¼rkisch] [Sprache erkennen â–¼] [ğŸ”„] [TÃ¼rkisch] [Englisch] [Deutsch â–¼] [Ãœbersetzen]\n",
    "\n",
    "She's a professor. He's a babysitter.\n",
    "â†”\n",
    "O bir profesÃ¶r. O bir bebek bakÄ±cÄ±sÄ±.\n",
    "\n",
    "---\n",
    "\n",
    "## Google Ãœbersetzer  \n",
    "[Englisch] [RumÃ¤nisch] [TÃ¼rkisch] [Sprache erkennen â–¼] [ğŸ”„] [TÃ¼rkisch] [Englisch] [Deutsch â–¼] [Ãœbersetzen]\n",
    "\n",
    "O bir profesÃ¶r. O bir bebek bakÄ±cÄ±sÄ±.\n",
    "â†” \n",
    "He's a professor. She's a babysitter.\n",
    "\n",
    "_(related talk: https://www.youtube.com/watch?v=j7FwpZB1hWc)_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Let's practice!\n",
    "-------------------\n",
    "\n",
    "02:46Â -Â 03:07\n",
    "\n",
    "As we have seen in just a few examples, the field of natural language processing still has plenty of unsolved problems. In fact, our fake news detector is likely one of them! Let's do some investigation into what it has learned to determine if the model will be widely applicable or if the problem is likely more complex than simple word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving the model\n",
    "===================\n",
    "\n",
    "What are possible next steps you could take to improve the model?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "50XP\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "[/] -   Tweaking alpha levels.\n",
    "\n",
    "-   Trying a new classification model.\n",
    "\n",
    "-   Training on a larger dataset.\n",
    "\n",
    "-   Improving text preprocessing.\n",
    "\n",
    "-   All of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving your model\n",
    "====================\n",
    "\n",
    "Your job in this exercise is to test a few different alpha levels using theÂ `Tfidf`Â vectors to determine if there is a better performing combination.\n",
    "\n",
    "The training and test sets have been created, andÂ `tfidf_vectorizer`,Â `tfidf_train`, andÂ `tfidf_test`Â have been computed.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Create a list of alphas to try usingÂ `np.arange()`. Values should range fromÂ `0`toÂ `1`Â with steps ofÂ `0.1`.\n",
    "-   Create a functionÂ `train_and_predict()`that takes in one argument:Â `alpha`. The function should:\n",
    "    -   Instantiate aÂ `MultinomialNB`Â classifier withÂ `alpha=alpha`.\n",
    "    -   Fit it to the training data.\n",
    "    -   Compute predictions on the test data.\n",
    "    -   Compute and return the accuracy score.\n",
    "-   Using aÂ `for`Â loop, print theÂ `alpha`,Â `score`and a newline in between. Use yourÂ `train_and_predict()`Â function to compute theÂ `score`. Does the score change along with the alpha? What is the best alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0, 1, .1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting your model\n",
    "=====================\n",
    "\n",
    "Now that you have built a \"fake news\" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques.\n",
    "\n",
    "You have your well performing tfidf Naive Bayes classifier available asÂ `nb_classifier`, and the vectors asÂ `tfidf_vectorizer`.\n",
    "\n",
    "Instructions\n",
    "------------\n",
    "\n",
    "-   Save the class labels asÂ `class_labels`Â by accessing theÂ `.classes_`Â attribute ofÂ `nb_classifier`.\n",
    "-   Extract the features using theÂ `.get_feature_names()`Â method ofÂ `tfidf_vectorizer`.\n",
    "-   Create a zipped array of the classifier coefficients with the feature names and sort them by the coefficients. To do this, first useÂ `zip()`Â with the argumentsÂ `nb_classifier.coef_[0]`Â andÂ `feature_names`. Then, useÂ `sorted()`Â on this.\n",
    "-   Print theÂ *top*Â 20 weighted features for the first label ofÂ `class_labels`Â and print the bottom 20 weighted features for the second label ofÂ `class_labels`.Â *This has been done for you.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
