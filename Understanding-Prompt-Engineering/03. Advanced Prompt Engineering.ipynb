{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99bd681e",
   "metadata": {},
   "source": [
    "1\\. Training techniques\n",
    "-----------------------\n",
    "\n",
    "00:00 - 00:03\n",
    "\n",
    "Welcome back to our journey with ChatGPT!\n",
    "\n",
    "2\\. Introduction to training techniques\n",
    "---------------------------------------\n",
    "\n",
    "00:03 - 00:26\n",
    "\n",
    "At the core of your interactions lie training techniques. These determine the way the model generates answers. It's crucial to understand the spectrum of zero-shot, one-shot, and few-shot learning. These aren't just cool names. They represent the degree of examples or context we provide ChatGPT before asking our main question. Let's explore these methods.\n",
    "\n",
    "3\\. Zero-shot learning\n",
    "----------------------\n",
    "\n",
    "00:26 - 00:46\n",
    "\n",
    "Zero-shot learning is when we throw a question or task at ChatGPT without providing any prior examples. It's like asking someone to dive into the deep end without any practice. But sometimes, diving right in works! For example, let's ask ChatGPT to write a poem about the tranquility of mountains.\n",
    "\n",
    "4\\. Zero-shot learning\n",
    "----------------------\n",
    "\n",
    "00:46 - 01:04\n",
    "\n",
    "In this scenario, the model relies on its extensive pre-training, leveraging the multitude of patterns it has learned to generate a response that fits the prompt. Zero-shot learning showcases the power of language models to respond in novel situations without the need for prior examples.\n",
    "\n",
    "5\\. One-shot learning\n",
    "---------------------\n",
    "\n",
    "01:04 - 01:22\n",
    "\n",
    "One-shot is the middle ground. Think of it as showing someone how to do a task once and then expecting them to replicate it. We give ChatGPT one example to guide its response, showing that London is the capital of the UK. Then we ask what is the capital of Japan.\n",
    "\n",
    "6\\. One-shot learning\n",
    "---------------------\n",
    "\n",
    "01:22 - 01:31\n",
    "\n",
    "One-shot learning is an echo of human learning, where one example can serve as a powerful template for understanding and action.\n",
    "\n",
    "7\\. Few-shot learning\n",
    "---------------------\n",
    "\n",
    "01:31 - 01:47\n",
    "\n",
    "Few-shot learning is where we arm ChatGPT with multiple examples before posing our main query. Here, we ask for the capital of Australia, whilst learning the formatting of placing the country's flag after the capital city.\n",
    "\n",
    "8\\. Few-shot learning\n",
    "---------------------\n",
    "\n",
    "01:47 - 01:59\n",
    "\n",
    "Each example serves as a building block, creating a more nuanced understanding within ChatGPT. We're essentially training the model on the fly, giving it a richer context to grasp the essence of our queries.\n",
    "\n",
    "9\\. Pattern matching and recognition\n",
    "------------------------------------\n",
    "\n",
    "01:59 - 02:25\n",
    "\n",
    "One fascinating aspect of few-shot learning is that ChatGPT becomes more than just an autocomplete tool. It turns into a pattern-matching and pattern-generation engine. ChatGPT understands the structure of the information and generates new content based on recognized patterns you provide. Number one, it starts by analyzing the examples. It then mirrors the underlying patterns. Then finally it creates new ideas.\n",
    "\n",
    "10\\. Pattern matching and recognition\n",
    "-------------------------------------\n",
    "\n",
    "02:25 - 02:53\n",
    "\n",
    "The possibilities for few-shot learning are endless. You can feed ChatGPT examples of: your writing style to help construct email replies; also, formatting preferences for reports to ensure consistency across documents; and finally, decision-making frameworks to generate new approaches to problems. Few-shot learning provides the opportunity for ChatGPT to extend beyond a mere respondent and become an extension of our mind.\n",
    "\n",
    "11\\. Chain of thought (COT) prompting\n",
    "-------------------------------------\n",
    "\n",
    "02:53 - 03:11\n",
    "\n",
    "Chain of Thought Prompting (COT) is an advanced technique that takes training a step further. Here, we're not just giving examples but a roadmap of how to arrive at the answer. It mirrors the way we are, as humans, approach problem-solving: by breaking down complex tasks into manageable steps.\n",
    "\n",
    "12\\. Zero-shot COT\n",
    "------------------\n",
    "\n",
    "03:11 - 03:20\n",
    "\n",
    "With zero-shot COT, we provide a situation: travelling to space and encountering aliens. We ask ChatGPT to \"think step by step.\"\n",
    "\n",
    "13\\. Zero-shot COT\n",
    "------------------\n",
    "\n",
    "03:20 - 03:37\n",
    "\n",
    "We get a thoughtful breakdown where ChatGPT reasons through the encounters. By prompting ChatGPT to reveal its reasoning, we gain insights into the model's thought process, allowing us to better understand, verify and trust its conclusions.\n",
    "\n",
    "14\\. One-shot COT\n",
    "-----------------\n",
    "\n",
    "03:37 - 03:48\n",
    "\n",
    "But when we provide an example using one-shot training, we can teach the model how to approach a particular type of problem. It learns the steps and considerations necessary to reach a conclusion.\n",
    "\n",
    "15\\. One-shot COT\n",
    "-----------------\n",
    "\n",
    "03:48 - 04:01\n",
    "\n",
    "In this case, ChatGPT only acknowledges the number of astronauts you had a direct interaction with. This can help prevent errors that might occur if it were to jump directly to the end.\n",
    "\n",
    "16\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "04:01 - 04:30\n",
    "\n",
    "Training techniques aren't just about getting an answer; they're about shaping that answer. Whether you're using Zero-shot for a quick reply, One-shot for guided responses, Few-shot for pattern-driven answers, or COT for methodical solutions, you're in the director's chair, shaping the narrative of ChatGPT's responses. Dive into the exercises and see how different training techniques can dramatically alter the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f77741",
   "metadata": {},
   "source": [
    "#### Insightful shot\n",
    "\n",
    "Utilizing **examples and patterns** allow LLMs like ChatGPT to create **connections** and generate **more meaningful responses**.\n",
    "\n",
    "Compare the impact of examples and patterns by prompting ChatGPT with and without example response(s) to observe the dramatic impact examples have on responses. \n",
    "\n",
    "Which statement **describes** how **one-shot prompting differs from few-shot prompting**? \n",
    "\n",
    "##### Instructions\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "-   Few-shot prompting is less accurate than one-shot prompting because the AI gets confused with more examples.\n",
    "\n",
    "-   One-shot and few-shot prompting are just different terms for the same process, there is no difference in their applications or results.\n",
    "\n",
    "[x] -   One-shot prompting uses a single example to guide the AI model, while few-shot prompting uses multiple examples for more detailed context and guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39225b",
   "metadata": {},
   "source": [
    "Thinking deep\n",
    "=============\n",
    "\n",
    "**Chain of Thought (COT)** prompts direct ChatGPT to explain the responses **step by step** creating dramatically different responses than traditional prompting.\n",
    "\n",
    "What are the benefits of **Chain of Thought (COT)** prompts?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "50XP\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "-   They make the AI's responses shorter and more concise.\n",
    "\n",
    "[x] -   They help in breaking down complex problems into simpler, understandable steps, leading to more accurate and detailed responses.\n",
    "\n",
    "-   They increase the creative capabilities of the AI in generating responses.\n",
    "\n",
    "-   They make the AI's responses more entertaining and engaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4210e",
   "metadata": {},
   "source": [
    "1\\. Mitigating model limitations\n",
    "--------------------------------\n",
    "\n",
    "00:00 - 00:01\n",
    "\n",
    "Hello again!\n",
    "\n",
    "2\\. Introduction to model limitations\n",
    "-------------------------------------\n",
    "\n",
    "00:01 - 00:16\n",
    "\n",
    "Every model, regardless of its sophistication, has its set of limitations. These limitations stem from the data it's trained on. By recognizing these, we can craft prompts that navigate around these pitfalls and evaluate outputs with a critical eye.\n",
    "\n",
    "3\\. The reversal curse\n",
    "----------------------\n",
    "\n",
    "00:16 - 00:39\n",
    "\n",
    "ChatGPT's knowledge base is imperfect and often quite strange. This is perfectly demonstrated with a recent viral example, highlighting the reversal curse. Let's ask GPT-4, the best language model currently available, \"Who is Tom Cruise's mother?\" It will respond with \"Mary Lee Pfeiffer\", which is correct.\n",
    "\n",
    "4\\. The reversal curse\n",
    "----------------------\n",
    "\n",
    "00:39 - 01:04\n",
    "\n",
    "But if you ask \"Who is Mary Lee Pfeiffer's son?\", ChatGPT doesn't know. As Andrej Karpathy recently highlighted, it shows that ChatGPT's knowledge is almost one-dimensional. You have to ask questions from a certain direction to get the answer. Unlike understanding a car and its respective components, we still don't know exactly how language models work.\n",
    "\n",
    "5\\. Biases - a mirror of Society\n",
    "--------------------------------\n",
    "\n",
    "01:04 - 01:43\n",
    "\n",
    "What we do know is that language models learn from vast amounts of data sourced from the internet. Consequently, they can inherit and sometimes amplify biases present in the data. Recognizing biases in outputs is crucial to avoid perpetuating stereotypes or misinformation. If you ask ChatGPT, \"Who typically cooks in a household?\" and it responds with a gendered answer, it showcases the bias it might have absorbed from historical or cultural data. A more neutral answer would acknowledge that anyone, regardless of gender, can cook in a household.\n",
    "\n",
    "6\\. Hallucinations - when the model imagines\n",
    "--------------------------------------------\n",
    "\n",
    "01:43 - 02:08\n",
    "\n",
    "Hallucinations in the context of LLMs refer to instances when the model confidently provides information that isn't accurate. It might \"imagine\" details or facts, leading to incorrect outputs. As models like ChatGPT are improving over time, it can be challenging to show concrete examples. Nonetheless, let's take a closer look at what a hallucination could look like.\n",
    "\n",
    "7\\. Hallucinations - when the model imagines\n",
    "--------------------------------------------\n",
    "\n",
    "02:08 - 02:28\n",
    "\n",
    "If you ask: \"Who was the only survivor of the sinking of the Titanic?\" ChatGPT responds: \"The only survivor of the sinking of the Titanic who is widely recognized as such was Violet Jessop.\" This is a hallucination. Asking ChatGPT to provide sources of information often leads to correcting itself.\n",
    "\n",
    "8\\. Hallucinations - when the model imagines\n",
    "--------------------------------------------\n",
    "\n",
    "02:28 - 02:44\n",
    "\n",
    "We can ask the follow-up question: \"Can you provide sources? Really think about it.\" ChatGPT apologizes for the confusion and highlights the misconception. Recognizing and cross-referencing ChatGPT's answers is crucial to ensure factual correctness.\n",
    "\n",
    "9\\. Overfitting - echoing the data\n",
    "----------------------------------\n",
    "\n",
    "02:44 - 03:00\n",
    "\n",
    "Overfitting occurs when a model is too closely tailored to its training data, making it less effective at generalizing to new, unseen data. In essence, the model might echo what it has seen in the past, rather than providing a balanced response.\n",
    "\n",
    "10\\. Overfitting - echoing the data\n",
    "-----------------------------------\n",
    "\n",
    "03:00 - 03:23\n",
    "\n",
    "An example of overfitting is humor. Comedy challenges large language models like ChatGPT. In June 2023, researchers found that if you ask ChatGPT to tell you a joke, 90% of 1,008 generations were the same 25 jokes. These responses were likely learned during ChatGPT's training.\n",
    "\n",
    "11\\. Overfitting - echoing the data\n",
    "-----------------------------------\n",
    "\n",
    "03:23 - 03:54\n",
    "\n",
    "The model currently sucks at discontinuous tasks that require a creative leap in progress. Examples include writing jokes, developing scientific hypotheses or creating new writing styles. My advice is to stick to incremental tasks. These are solved sequentially, adding more information in a gradual setting. Examples include writing summaries, answering questions or imitating your writing style through techniques such as one-shot or few-shot learning.\n",
    "\n",
    "12\\. Let's practice!\n",
    "--------------------\n",
    "\n",
    "03:54 - 04:12\n",
    "\n",
    "Dive into the exercises designed to challenge your understanding of biases, hallucinations, and overfitting. By understanding its limitations, we become better users, ensuring that our interactions with ChatGPT are informed, critical, and productive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e997ba0",
   "metadata": {},
   "source": [
    "Getting the right fit\n",
    "=====================\n",
    "\n",
    "You are using ChatGPT to assist in writing a script for a comedy show. \n",
    "\n",
    "You notice that the jokes generated by ChatGPT are often **repetitive** and **similar** to popular jokes. Based on your understanding of **overfitting**, how would you address this issue to **enhance the originality** of the jokes?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "-   Keep prompting ChatGPT with the same joke format, but ask for variations in the punchline each time.\n",
    "\n",
    "-   Introduce a new dataset of modern and diverse humor styles to retrain ChatGPT, thereby expanding its joke repertoire.\n",
    "\n",
    "[x] -   Prompt ChatGPT with discontinuous tasks, such as creating jokes in novel styles or blending humor with unexpected topics, to encourage creative leaps beyond its training data.\n",
    "\n",
    "-   Limit the length of the jokes requested from ChatGPT, as shorter jokes are less likely to be affected by overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9e688",
   "metadata": {},
   "source": [
    "Beating bias\n",
    "============\n",
    "\n",
    "You are using **ChatGPT** to assist in writing a script for a comedy show. \n",
    "\n",
    "You've observed that some jokes generated by ChatGPT reflect **societal biases** present in its training data, potentially **perpetuating stereotypes**. \n",
    "\n",
    "How would you **address** this concern to ensure the humor is **free of bias**?\n",
    "\n",
    "##### Answer the question\n",
    "\n",
    "#### Possible Answers\n",
    "\n",
    "Select one answer\n",
    "\n",
    "-   Use keyword filters to automatically remove any content that might be biased or stereotypical.\n",
    "\n",
    "[x] -   Actively prompt ChatGPT to avoid stereotypes and include diverse perspectives in the jokes it generates.\n",
    "\n",
    "-   Ask ChatGPT to self-correct any biases it might have in its responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIAgents4Pharma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
